{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:07.212007Z",
     "start_time": "2020-06-06T00:36:05.366067Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# import gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:07.224834Z",
     "start_time": "2020-06-06T00:36:07.214227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('superpixels.zip'):\n",
    "    print('downloading..')\n",
    "    !curl https://www.dropbox.com/s/y2qwa77a0fxem47/superpixels.zip?dl=1 -o superpixels.zip -J -L -k\n",
    "    !unzip superpixels.zip -d ../\n",
    "    # !tar -xvf superpixels.zip -C ../\n",
    "else:\n",
    "    print('File already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:07.235814Z",
     "start_time": "2020-06-06T00:36:07.229374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ziangeng/Desktop/Research\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('../../') # go to root folder of the project\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:08.946179Z",
     "start_time": "2020-06-06T00:36:07.240350Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from superpixels import SuperPixDatasetDGL \n",
    "\n",
    "from data import LoadData\n",
    "from torch.utils.data import DataLoader\n",
    "from superpixels import SuperPixDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.155827Z",
     "start_time": "2020-06-06T00:36:08.948164Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adj matrix defined from super-pixel locations (only...)\n",
      "test [1, 3, 0, 4]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/superpixels/mnist_75sp_test.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8dcebe502c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mftlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSuperPixDatasetDGL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mway\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/superpixels/MNIST.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Research/Data/superpixels/superpixels.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, testset, trainset, it, way, task, shot, num_val)\u001b[0m\n\u001b[1;32m    278\u001b[0m                             \u001b[0mways\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mways\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                             \u001b[0muse_mean_px\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_mean_px\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                             use_coord=use_coord)\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         self.train_ = SuperPixDGL(\"./data/superpixels\", dataset=self.name, split='train',select=trainset,it=it,way=way,\n",
      "\u001b[0;32m~/Desktop/Research/Data/superpixels/superpixels.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, dataset, split, select, it, way, task, shot, ways, use_mean_px, use_coord)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MNIST'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mnist_75sp_%s.pkl'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;31m#                 self.labels, self.sp_data = pickle.load(f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mtlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/superpixels/mnist_75sp_test.pkl'"
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# DATASET_NAME = 'MNIST'\n",
    "# trainset=[0,1,2,3,4,5]\n",
    "# testset=[6,7,8,9]\n",
    "# way=4\n",
    "# shot=8\n",
    "# task=2\n",
    "# it=0\n",
    "# datasetlist=[]\n",
    "# ftlist=[]\n",
    "# for it in range (task):\n",
    "#     dataset = SuperPixDatasetDGL(DATASET_NAME,trainset,trainset,it,way,task,shot) \n",
    "#     with open('data/superpixels/MNIST.pkl','wb') as f:\n",
    "#         pickle.dump([dataset.train,dataset.val,dataset.test],f)\n",
    "#     dataset = LoadData(DATASET_NAME) # 54s\n",
    "#     print('train',dataset.train.graph_labels)\n",
    "#     datasetlist.append(dataset)\n",
    "# for it in range (task):\n",
    "#     dataset = SuperPixDatasetDGL(DATASET_NAME,testset,testset,it,way,task,shot) \n",
    "#     with open('data/superpixels/MNIST.pkl','wb') as f:\n",
    "#         pickle.dump([dataset.train,dataset.val,dataset.test],f)\n",
    "#     dataset = LoadData(DATASET_NAME) # 54s\n",
    "#     print('test',dataset.train.graph_labels)\n",
    "#     ftlist.append(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:45:26.594646Z",
     "start_time": "2020-06-06T00:40:42.318258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ls7-www.cs.uni-dortmund.de/cvpr_geometric_dl/mnist_superpixels.tar.gz\n",
      "Extracting data/superpixels/raw/mnist_superpixels.tar.gz\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "import random\n",
    "from torch.utils.data import random_split,Subset\n",
    "\n",
    "# trainset=[0,1,2,3,4,5]\n",
    "# testset=[6,7,8,9]\n",
    "trainset=[0,1,2,3,4,5,6,7,8,9]\n",
    "testset=[0,1,2,3,4,5,6,7,8,9]\n",
    "way=10\n",
    "shot=10\n",
    "task=4\n",
    "SAG = MNISTSuperpixels('data/superpixels/')\n",
    "# Creates a list containing 5 lists, each of 8 items, all set to 0\n",
    "indexlist = [[] for y in range(10)] \n",
    "# if(SAG[0].y==5):\n",
    "#     print('hello')\n",
    "for ind in range (len(SAG)):\n",
    "    for sa in range(10):\n",
    "        if(SAG[ind].y==sa):\n",
    "            indexlist[sa].append(ind)\n",
    "\n",
    "\n",
    "SAGtrainlist=[]\n",
    "SAGftlist=[]\n",
    "\n",
    "for n in range(task):\n",
    "    train_choice=random.sample(trainset,way)\n",
    "    ft_choice=random.sample(testset,way)\n",
    "    tr_indlist=[]\n",
    "    te_indlist=[]\n",
    "    ft_indlist=[]\n",
    "    ftte_indlist=[]\n",
    "    for t in train_choice:\n",
    "        tr_indlist=tr_indlist+indexlist[t][n*shot:(n+1)*shot]\n",
    "        te_indlist=te_indlist+indexlist[t][(len(indexlist[t])-(n+1)*shot):(len(indexlist[t])-n*shot)]\n",
    "    for f in ft_choice:\n",
    "#         ft_indlist=ft_indlist+indexlist[f][n*shot:(n+1)*shot]\n",
    "#         ftte_indlist= ftte_indlist+indexlist[f][(len(indexlist[f])-(n+1)*shot):(len(indexlist[f])-n*shot)]\n",
    "        ft_indlist=ft_indlist+indexlist[f][(task+n)*shot:(task+n+1)*shot]\n",
    "        ftte_indlist= ftte_indlist+indexlist[f][(len(indexlist[f])-(task+n+1)*shot):(len(indexlist[f])-(task+n)*shot)]\n",
    "    sup_qur_tr=[]\n",
    "    sup_qur_ft=[]\n",
    "    sup_qur_tr.append(Subset(SAG,tr_indlist))\n",
    "    sup_qur_tr.append(Subset(SAG,te_indlist))\n",
    "    sup_qur_ft.append(Subset(SAG,ft_indlist))\n",
    "    sup_qur_ft.append(Subset(SAG,ftte_indlist))\n",
    "    SAGtrainlist.append(sup_qur_tr)\n",
    "    SAGftlist.append(sup_qur_ft)\n",
    "\n",
    "print('finish')   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-06T21:16:42.158Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:2.3021867275238037\n",
      "Training loss:2.3017938137054443\n",
      "Training loss:2.3066868782043457\n",
      "Training loss:2.303407669067383\n",
      "Training loss:2.3023195266723633\n",
      "Training loss:2.3019628524780273\n",
      "Training loss:2.3024301528930664\n",
      "Training loss:2.3004305362701416\n",
      "Training loss:2.3023433685302734\n",
      "Training loss:2.3002536296844482\n",
      "Training loss:2.307572841644287\n",
      "Training loss:2.300997495651245\n",
      "Training loss:2.306103229522705\n",
      "Training loss:2.3049018383026123\n",
      "Training loss:2.2990689277648926\n",
      "Training loss:2.3033831119537354\n",
      "Training loss:2.301781177520752\n",
      "Training loss:2.302147626876831\n",
      "Training loss:2.3015506267547607\n",
      "Training loss:2.3045132160186768\n",
      "Training loss:2.3016531467437744\n",
      "Training loss:2.3003597259521484\n",
      "Training loss:2.303877830505371\n",
      "Training loss:2.3005361557006836\n",
      "Training loss:2.306321620941162\n",
      "Training loss:2.3039169311523438\n",
      "Training loss:2.299220323562622\n",
      "Training loss:2.301729679107666\n",
      "Training loss:2.300781726837158\n",
      "Training loss:2.3049464225769043\n",
      "Training loss:2.3036463260650635\n",
      "Training loss:2.3065152168273926\n",
      "Training loss:2.300734043121338\n",
      "Training loss:2.302224636077881\n",
      "Training loss:2.2980222702026367\n",
      "Training loss:2.299365520477295\n",
      "Training loss:2.30474853515625\n",
      "Training loss:2.3063104152679443\n",
      "Training loss:2.3027350902557373\n",
      "Training loss:2.2998783588409424\n",
      "Training loss:2.3009068965911865\n",
      "Training loss:2.3022189140319824\n",
      "Training loss:2.3024823665618896\n",
      "Training loss:2.3016104698181152\n",
      "Training loss:2.3019068241119385\n",
      "Training loss:2.297929286956787\n",
      "Training loss:2.2987797260284424\n",
      "Training loss:2.30177640914917\n",
      "Training loss:2.3032732009887695\n",
      "Training loss:2.299384355545044\n",
      "Training loss:2.299232244491577\n",
      "Training loss:2.2979722023010254\n",
      "Training loss:2.3004322052001953\n",
      "Training loss:2.3015971183776855\n",
      "Training loss:2.302337884902954\n",
      "Training loss:2.297867774963379\n",
      "Training loss:2.3023054599761963\n",
      "Training loss:2.3001785278320312\n",
      "Training loss:2.3015480041503906\n",
      "Training loss:2.3030359745025635\n",
      "Training loss:2.3025014400482178\n",
      "Training loss:2.301751136779785\n",
      "Training loss:2.3043956756591797\n",
      "Training loss:2.2986106872558594\n",
      "Training loss:2.3022053241729736\n",
      "Training loss:2.3027031421661377\n",
      "Training loss:2.3026623725891113\n",
      "Training loss:2.3006463050842285\n",
      "Training loss:2.3006606101989746\n",
      "Training loss:2.30178165435791\n",
      "Training loss:2.300131320953369\n",
      "Training loss:2.300482988357544\n",
      "Training loss:2.3028225898742676\n",
      "Training loss:2.2999377250671387\n",
      "Training loss:2.300957202911377\n",
      "Training loss:2.3010668754577637\n",
      "Training loss:2.301490545272827\n",
      "Training loss:2.2985317707061768\n",
      "Training loss:2.301793336868286\n",
      "Training loss:2.302219867706299\n",
      "Training loss:2.300779342651367\n",
      "Training loss:2.2995123863220215\n",
      "Training loss:2.2985305786132812\n",
      "Training loss:2.300039291381836\n",
      "Training loss:2.2987663745880127\n",
      "Training loss:2.3024513721466064\n",
      "Training loss:2.3034865856170654\n",
      "Training loss:2.3023924827575684\n",
      "Training loss:2.2988665103912354\n",
      "Training loss:2.2960939407348633\n",
      "Training loss:2.301795244216919\n",
      "Training loss:2.302851676940918\n",
      "Training loss:2.2990763187408447\n",
      "Training loss:2.2942264080047607\n",
      "Epoch: 0001 loss_train: 216.346356 acc_train: 0.106521 loss_val: 27.602128 acc_val: 0.118333 time: 2569.700636s\n",
      "Training loss:2.300102949142456\n",
      "Training loss:2.3004987239837646\n",
      "Training loss:2.299116849899292\n",
      "Training loss:2.3011276721954346\n",
      "Training loss:2.2951056957244873\n",
      "Training loss:2.299598217010498\n",
      "Training loss:2.298618793487549\n",
      "Training loss:2.3012423515319824\n",
      "Training loss:2.2966558933258057\n",
      "Training loss:2.3023362159729004\n",
      "Training loss:2.3025808334350586\n",
      "Training loss:2.2991578578948975\n",
      "Training loss:2.306360960006714\n",
      "Training loss:2.303334951400757\n",
      "Training loss:2.299685478210449\n",
      "Training loss:2.3024985790252686\n",
      "Training loss:2.299964666366577\n",
      "Training loss:2.295297861099243\n",
      "Training loss:2.305738925933838\n",
      "Training loss:2.301107406616211\n",
      "Training loss:2.301313638687134\n",
      "Training loss:2.30122447013855\n",
      "Training loss:2.300527334213257\n",
      "Training loss:2.3029003143310547\n",
      "Training loss:2.3005430698394775\n",
      "Training loss:2.299149990081787\n",
      "Training loss:2.2950587272644043\n",
      "Training loss:2.301537275314331\n",
      "Training loss:2.297036647796631\n",
      "Training loss:2.3029232025146484\n",
      "Training loss:2.3019959926605225\n",
      "Training loss:2.3018665313720703\n",
      "Training loss:2.2998435497283936\n",
      "Training loss:2.2992138862609863\n",
      "Training loss:2.300651788711548\n",
      "Training loss:2.2999532222747803\n",
      "Training loss:2.3035507202148438\n",
      "Training loss:2.300873279571533\n",
      "Training loss:2.299739360809326\n",
      "Training loss:2.299865245819092\n",
      "Training loss:2.3008861541748047\n",
      "Training loss:2.299211025238037\n",
      "Training loss:2.3017585277557373\n",
      "Training loss:2.3006486892700195\n",
      "Training loss:2.2984983921051025\n",
      "Training loss:2.298625946044922\n",
      "Training loss:2.2976651191711426\n",
      "Training loss:2.2982053756713867\n",
      "Training loss:2.300410509109497\n",
      "Training loss:2.295344829559326\n",
      "Training loss:2.2974650859832764\n",
      "Training loss:2.299063205718994\n",
      "Training loss:2.3012490272521973\n",
      "Training loss:2.300140857696533\n",
      "Training loss:2.300553560256958\n",
      "Training loss:2.296917676925659\n",
      "Training loss:2.298593521118164\n",
      "Training loss:2.2956440448760986\n",
      "Training loss:2.298325777053833\n",
      "Training loss:2.2997515201568604\n",
      "Training loss:2.293632984161377\n",
      "Training loss:2.2975542545318604\n",
      "Training loss:2.2922170162200928\n",
      "Training loss:2.2956550121307373\n",
      "Training loss:2.2974634170532227\n",
      "Training loss:2.2996039390563965\n",
      "Training loss:2.297931432723999\n",
      "Training loss:2.298076629638672\n",
      "Training loss:2.298724412918091\n",
      "Training loss:2.3006319999694824\n",
      "Training loss:2.302377939224243\n",
      "Training loss:2.293630361557007\n",
      "Training loss:2.2962048053741455\n",
      "Training loss:2.2966079711914062\n",
      "Training loss:2.295259714126587\n",
      "Training loss:2.2916955947875977\n",
      "Training loss:2.293179988861084\n",
      "Training loss:2.2950000762939453\n",
      "Training loss:2.290050745010376\n",
      "Training loss:2.293135404586792\n",
      "Training loss:2.2869765758514404\n",
      "Training loss:2.293452501296997\n",
      "Training loss:2.288431406021118\n",
      "Training loss:2.290318489074707\n",
      "Training loss:2.2788565158843994\n",
      "Training loss:2.2895352840423584\n",
      "Training loss:2.2860472202301025\n",
      "Training loss:2.290555238723755\n",
      "Training loss:2.2943429946899414\n",
      "Training loss:2.2869226932525635\n",
      "Training loss:2.292898178100586\n",
      "Training loss:2.280007839202881\n",
      "Training loss:2.283146619796753\n",
      "Training loss:2.283020257949829\n",
      "Epoch: 0002 loss_train: 215.942003 acc_train: 0.118938 loss_val: 27.401644 acc_val: 0.136500 time: 5032.254671s\n",
      "Training loss:2.284876823425293\n",
      "Training loss:2.279332399368286\n",
      "Training loss:2.290266275405884\n",
      "Training loss:2.2869441509246826\n",
      "Training loss:2.2726733684539795\n",
      "Training loss:2.2767131328582764\n",
      "Training loss:2.2739109992980957\n",
      "Training loss:2.2789807319641113\n",
      "Training loss:2.2667179107666016\n",
      "Training loss:2.2912511825561523\n",
      "Training loss:2.2776906490325928\n",
      "Training loss:2.258080244064331\n",
      "Training loss:2.2763755321502686\n",
      "Training loss:2.258021593093872\n",
      "Training loss:2.2671632766723633\n",
      "Training loss:2.277282238006592\n",
      "Training loss:2.255948305130005\n",
      "Training loss:2.259791135787964\n",
      "Training loss:2.2581138610839844\n",
      "Training loss:2.259256362915039\n",
      "Training loss:2.2625041007995605\n",
      "Training loss:2.2600724697113037\n",
      "Training loss:2.262721538543701\n",
      "Training loss:2.2524678707122803\n",
      "Training loss:2.2393839359283447\n",
      "Training loss:2.276801586151123\n",
      "Training loss:2.255476713180542\n",
      "Training loss:2.2438907623291016\n",
      "Training loss:2.2524378299713135\n",
      "Training loss:2.2385969161987305\n",
      "Training loss:2.2696902751922607\n",
      "Training loss:2.2368698120117188\n",
      "Training loss:2.253657341003418\n",
      "Training loss:2.2684037685394287\n",
      "Training loss:2.2355401515960693\n",
      "Training loss:2.2337021827697754\n",
      "Training loss:2.2473297119140625\n",
      "Training loss:2.209343433380127\n",
      "Training loss:2.2500648498535156\n",
      "Training loss:2.2281174659729004\n",
      "Training loss:2.2235515117645264\n",
      "Training loss:2.2300806045532227\n",
      "Training loss:2.2092339992523193\n",
      "Training loss:2.259861707687378\n",
      "Training loss:2.2402024269104004\n",
      "Training loss:2.1797873973846436\n",
      "Training loss:2.219480514526367\n",
      "Training loss:2.2235636711120605\n",
      "Training loss:2.2101027965545654\n",
      "Training loss:2.2077090740203857\n",
      "Training loss:2.234677791595459\n",
      "Training loss:2.1764779090881348\n",
      "Training loss:2.21903395652771\n",
      "Training loss:2.2046539783477783\n",
      "Training loss:2.1949214935302734\n",
      "Training loss:2.2073636054992676\n",
      "Training loss:2.2101895809173584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:2.1854746341705322\n",
      "Training loss:2.186081647872925\n",
      "Training loss:2.2365329265594482\n",
      "Training loss:2.161325216293335\n",
      "Training loss:2.160799026489258\n",
      "Training loss:2.1688904762268066\n",
      "Training loss:2.158125638961792\n",
      "Training loss:2.1556918621063232\n",
      "Training loss:2.154301881790161\n",
      "Training loss:2.165303945541382\n",
      "Training loss:2.185283899307251\n",
      "Training loss:2.126926898956299\n",
      "Training loss:2.1199874877929688\n",
      "Training loss:2.1434197425842285\n",
      "Training loss:2.152299642562866\n",
      "Training loss:2.173352003097534\n",
      "Training loss:2.1235735416412354\n",
      "Training loss:2.1708157062530518\n",
      "Training loss:2.152787923812866\n",
      "Training loss:2.1321985721588135\n",
      "Training loss:2.0999221801757812\n",
      "Training loss:2.1273622512817383\n",
      "Training loss:2.1356523036956787\n",
      "Training loss:2.136162042617798\n",
      "Training loss:2.13671612739563\n",
      "Training loss:2.1489923000335693\n",
      "Training loss:2.101346015930176\n",
      "Training loss:2.150085210800171\n",
      "Training loss:2.1181623935699463\n",
      "Training loss:2.146397590637207\n",
      "Training loss:2.120704174041748\n",
      "Training loss:2.1607041358947754\n",
      "Training loss:2.123619318008423\n",
      "Training loss:2.0790159702301025\n",
      "Training loss:2.1327357292175293\n",
      "Training loss:2.087411403656006\n",
      "Training loss:2.0988197326660156\n",
      "Epoch: 0003 loss_train: 207.394336 acc_train: 0.179125 loss_val: 25.440674 acc_val: 0.230833 time: 7429.610152s\n",
      "Training loss:2.14276385307312\n",
      "Training loss:2.132896900177002\n",
      "Training loss:2.1753435134887695\n",
      "Training loss:2.06437087059021\n",
      "Training loss:2.085329532623291\n",
      "Training loss:2.1325793266296387\n",
      "Training loss:2.0576655864715576\n",
      "Training loss:2.1084418296813965\n",
      "Training loss:2.0868592262268066\n",
      "Training loss:2.136089324951172\n",
      "Training loss:2.0316319465637207\n",
      "Training loss:2.1187167167663574\n",
      "Training loss:2.124124050140381\n",
      "Training loss:2.084981918334961\n",
      "Training loss:2.0901877880096436\n",
      "Training loss:2.1207163333892822\n",
      "Training loss:2.090998411178589\n",
      "Training loss:2.0627384185791016\n",
      "Training loss:2.066743850708008\n",
      "Training loss:2.0641677379608154\n",
      "Training loss:2.097993850708008\n",
      "Training loss:2.0539495944976807\n",
      "Training loss:2.054490327835083\n",
      "Training loss:2.042403221130371\n",
      "Training loss:2.0691757202148438\n",
      "Training loss:2.0677802562713623\n",
      "Training loss:2.1441593170166016\n",
      "Training loss:2.090332269668579\n",
      "Training loss:2.1003384590148926\n",
      "Training loss:2.079871654510498\n",
      "Training loss:2.0406363010406494\n",
      "Training loss:2.113828420639038\n",
      "Training loss:2.128431797027588\n",
      "Training loss:2.0886664390563965\n",
      "Training loss:2.045450210571289\n",
      "Training loss:2.072650909423828\n",
      "Training loss:2.043069839477539\n",
      "Training loss:2.054330825805664\n",
      "Training loss:2.0964128971099854\n",
      "Training loss:2.1172432899475098\n",
      "Training loss:2.0028741359710693\n",
      "Training loss:2.053706407546997\n",
      "Training loss:2.0443170070648193\n",
      "Training loss:2.0801398754119873\n",
      "Training loss:2.06349515914917\n",
      "Training loss:2.0736913681030273\n",
      "Training loss:2.0106139183044434\n",
      "Training loss:2.0259530544281006\n",
      "Training loss:2.080350160598755\n",
      "Training loss:2.0575668811798096\n",
      "Training loss:2.0313544273376465\n",
      "Training loss:2.039268970489502\n",
      "Training loss:2.022611141204834\n",
      "Training loss:2.0068752765655518\n",
      "Training loss:2.0267200469970703\n",
      "Training loss:2.045759916305542\n",
      "Training loss:2.030600070953369\n",
      "Training loss:1.9848403930664062\n",
      "Training loss:1.972751498222351\n",
      "Training loss:2.068779468536377\n",
      "Training loss:1.9975941181182861\n",
      "Training loss:2.073803663253784\n",
      "Training loss:2.003061056137085\n",
      "Training loss:2.006054162979126\n",
      "Training loss:2.0306763648986816\n",
      "Training loss:1.9439455270767212\n",
      "Training loss:1.9505784511566162\n",
      "Training loss:2.0039987564086914\n",
      "Training loss:1.978615641593933\n",
      "Training loss:1.939326524734497\n",
      "Training loss:1.96885085105896\n",
      "Training loss:1.9541658163070679\n",
      "Training loss:1.9294685125350952\n",
      "Training loss:2.019822597503662\n",
      "Training loss:1.9580262899398804\n",
      "Training loss:1.9455525875091553\n",
      "Training loss:1.9301908016204834\n",
      "Training loss:1.983431339263916\n",
      "Training loss:2.0177204608917236\n",
      "Training loss:1.9503214359283447\n",
      "Training loss:2.013706922531128\n",
      "Training loss:2.0187830924987793\n",
      "Training loss:1.961466670036316\n",
      "Training loss:1.9782991409301758\n",
      "Training loss:1.9057786464691162\n",
      "Training loss:2.028364658355713\n",
      "Training loss:1.9949126243591309\n",
      "Training loss:2.0213301181793213\n",
      "Training loss:2.0259432792663574\n",
      "Training loss:2.0090465545654297\n",
      "Training loss:1.945070743560791\n",
      "Training loss:2.0145771503448486\n",
      "Training loss:1.958622694015503\n",
      "Training loss:1.9716070890426636\n",
      "Epoch: 0004 loss_train: 191.733546 acc_train: 0.242625 loss_val: 23.197355 acc_val: 0.281333 time: 9793.148606s\n",
      "Training loss:1.9538202285766602\n",
      "Training loss:2.0664477348327637\n",
      "Training loss:1.9567700624465942\n",
      "Training loss:2.0053906440734863\n",
      "Training loss:1.9830478429794312\n",
      "Training loss:1.9226242303848267\n",
      "Training loss:2.02677059173584\n",
      "Training loss:1.9438161849975586\n",
      "Training loss:1.9718297719955444\n",
      "Training loss:2.032684087753296\n",
      "Training loss:1.9777270555496216\n",
      "Training loss:1.9767534732818604\n",
      "Training loss:1.946136713027954\n",
      "Training loss:1.9817168712615967\n",
      "Training loss:2.023266553878784\n",
      "Training loss:1.9659907817840576\n",
      "Training loss:1.9260802268981934\n",
      "Training loss:1.9646589756011963\n",
      "Training loss:1.9486989974975586\n",
      "Training loss:1.9291424751281738\n",
      "Training loss:1.8999145030975342\n",
      "Training loss:1.967362880706787\n",
      "Training loss:1.936650276184082\n",
      "Training loss:1.9748533964157104\n",
      "Training loss:1.9121129512786865\n",
      "Training loss:1.9470925331115723\n",
      "Training loss:1.9576539993286133\n",
      "Training loss:1.9033427238464355\n",
      "Training loss:1.9692569971084595\n",
      "Training loss:1.9323638677597046\n",
      "Training loss:1.9308831691741943\n",
      "Training loss:1.9903197288513184\n",
      "Training loss:1.9306665658950806\n",
      "Training loss:1.9569776058197021\n",
      "Training loss:1.9279110431671143\n",
      "Training loss:1.9226181507110596\n",
      "Training loss:1.9338757991790771\n",
      "Training loss:1.9052696228027344\n",
      "Training loss:1.9488043785095215\n",
      "Training loss:1.915642261505127\n",
      "Training loss:2.0075368881225586\n",
      "Training loss:1.917574167251587\n",
      "Training loss:1.9437110424041748\n",
      "Training loss:1.9677716493606567\n",
      "Training loss:1.9703048467636108\n",
      "Training loss:1.920479416847229\n",
      "Training loss:1.940050482749939\n",
      "Training loss:1.9560201168060303\n",
      "Training loss:1.9298467636108398\n",
      "Training loss:1.9749705791473389\n",
      "Training loss:1.920768141746521\n",
      "Training loss:1.8621259927749634\n",
      "Training loss:1.9452502727508545\n",
      "Training loss:1.90061354637146\n",
      "Training loss:1.9347953796386719\n",
      "Training loss:1.8892165422439575\n",
      "Training loss:1.9406238794326782\n",
      "Training loss:1.8984572887420654\n",
      "Training loss:1.8776421546936035\n",
      "Training loss:2.01448130607605\n",
      "Training loss:1.8840254545211792\n",
      "Training loss:1.877830147743225\n",
      "Training loss:1.919669508934021\n",
      "Training loss:1.9287470579147339\n",
      "Training loss:1.9155703783035278\n",
      "Training loss:1.8757520914077759\n",
      "Training loss:1.8851580619812012\n",
      "Training loss:1.9475106000900269\n",
      "Training loss:1.9229940176010132\n",
      "Training loss:1.929999589920044\n",
      "Training loss:1.9333025217056274\n",
      "Training loss:1.9110524654388428\n",
      "Training loss:2.0072097778320312\n",
      "Training loss:1.8919850587844849\n",
      "Training loss:1.8195242881774902\n",
      "Training loss:1.9438523054122925\n",
      "Training loss:1.8680311441421509\n",
      "Training loss:1.8894649744033813\n",
      "Training loss:1.9215706586837769\n",
      "Training loss:1.854562520980835\n",
      "Training loss:1.896409273147583\n",
      "Training loss:1.9701216220855713\n",
      "Training loss:1.8934215307235718\n",
      "Training loss:1.9176509380340576\n",
      "Training loss:1.9600856304168701\n",
      "Training loss:1.961800217628479\n",
      "Training loss:1.8933641910552979\n",
      "Training loss:1.9099289178848267\n",
      "Training loss:2.0000174045562744\n",
      "Training loss:1.8216116428375244\n",
      "Training loss:1.8857121467590332\n",
      "Training loss:1.8735612630844116\n",
      "Training loss:1.9038435220718384\n",
      "Training loss:1.8219698667526245\n",
      "Epoch: 0005 loss_train: 181.816497 acc_train: 0.272167 loss_val: 22.720675 acc_val: 0.289000 time: 12154.556813s\n",
      "Training loss:1.900827407836914\n",
      "Training loss:1.8820042610168457\n",
      "Training loss:1.964392900466919\n",
      "Training loss:1.8720506429672241\n",
      "Training loss:1.8765625953674316\n",
      "Training loss:1.947000503540039\n",
      "Training loss:1.885602355003357\n",
      "Training loss:1.9437111616134644\n",
      "Training loss:1.9512557983398438\n",
      "Training loss:1.9152507781982422\n",
      "Training loss:1.8944282531738281\n",
      "Training loss:1.8492680788040161\n",
      "Training loss:1.8641395568847656\n",
      "Training loss:1.9595210552215576\n",
      "Training loss:1.8421036005020142\n",
      "Training loss:1.8948410749435425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.8633173704147339\n",
      "Training loss:1.9101756811141968\n",
      "Training loss:1.921073079109192\n",
      "Training loss:1.8189913034439087\n",
      "Training loss:1.8920515775680542\n",
      "Training loss:1.8728994131088257\n",
      "Training loss:1.87646484375\n",
      "Training loss:1.9495209455490112\n",
      "Training loss:1.84376859664917\n",
      "Training loss:1.8627896308898926\n",
      "Training loss:1.867034673690796\n",
      "Training loss:1.9477884769439697\n",
      "Training loss:1.8626195192337036\n",
      "Training loss:1.9644521474838257\n",
      "Training loss:1.9461445808410645\n",
      "Training loss:1.9236520528793335\n",
      "Training loss:1.8806403875350952\n",
      "Training loss:1.926361322402954\n",
      "Training loss:1.9741120338439941\n",
      "Training loss:1.9525765180587769\n",
      "Training loss:1.9610539674758911\n",
      "Training loss:1.9107122421264648\n",
      "Training loss:1.9128422737121582\n",
      "Training loss:1.9284448623657227\n",
      "Training loss:1.9504600763320923\n",
      "Training loss:1.9099682569503784\n",
      "Training loss:1.8586679697036743\n",
      "Training loss:1.8569766283035278\n",
      "Training loss:1.8788655996322632\n",
      "Training loss:1.907950520515442\n",
      "Training loss:1.8624202013015747\n",
      "Training loss:1.9056916236877441\n",
      "Training loss:1.9022190570831299\n",
      "Training loss:1.9493869543075562\n",
      "Training loss:1.95613431930542\n",
      "Training loss:1.8295360803604126\n",
      "Training loss:1.9371672868728638\n",
      "Training loss:1.939711332321167\n",
      "Training loss:1.9209325313568115\n",
      "Training loss:1.9598513841629028\n",
      "Training loss:1.9064024686813354\n",
      "Training loss:1.8977810144424438\n",
      "Training loss:1.8411532640457153\n",
      "Training loss:1.9087672233581543\n",
      "Training loss:1.851783037185669\n",
      "Training loss:1.8918490409851074\n",
      "Training loss:1.9343503713607788\n",
      "Training loss:1.8878988027572632\n",
      "Training loss:2.016425371170044\n",
      "Training loss:1.9269529581069946\n",
      "Training loss:1.8956868648529053\n",
      "Training loss:1.8775715827941895\n",
      "Training loss:1.9509687423706055\n",
      "Training loss:1.875954270362854\n",
      "Training loss:1.9130650758743286\n",
      "Training loss:1.9130910634994507\n",
      "Training loss:1.8724069595336914\n",
      "Training loss:1.8854095935821533\n",
      "Training loss:1.8979698419570923\n",
      "Training loss:1.8846269845962524\n",
      "Training loss:1.9020826816558838\n",
      "Training loss:1.9349457025527954\n",
      "Training loss:1.8739358186721802\n",
      "Training loss:1.9408131837844849\n",
      "Training loss:1.8734573125839233\n",
      "Training loss:1.9341638088226318\n",
      "Training loss:1.9045382738113403\n",
      "Training loss:1.885448694229126\n",
      "Training loss:1.8616623878479004\n",
      "Training loss:1.8831334114074707\n",
      "Training loss:1.941117286682129\n",
      "Training loss:1.9496567249298096\n",
      "Training loss:1.9064414501190186\n",
      "Training loss:1.8948606252670288\n",
      "Training loss:1.9370241165161133\n",
      "Training loss:1.9221174716949463\n",
      "Training loss:1.8778005838394165\n",
      "Training loss:1.8814805746078491\n",
      "Epoch: 0006 loss_train: 179.073182 acc_train: 0.280479 loss_val: 22.548495 acc_val: 0.294833 time: 14527.442522s\n",
      "Training loss:1.9421534538269043\n",
      "Training loss:1.8540552854537964\n",
      "Training loss:1.7990676164627075\n",
      "Training loss:1.9088703393936157\n",
      "Training loss:1.914189338684082\n",
      "Training loss:1.8764903545379639\n",
      "Training loss:1.9427416324615479\n",
      "Training loss:1.887291669845581\n",
      "Training loss:1.9234039783477783\n",
      "Training loss:1.906426191329956\n",
      "Training loss:1.9290732145309448\n",
      "Training loss:1.933903694152832\n",
      "Training loss:1.945409893989563\n",
      "Training loss:1.89200758934021\n",
      "Training loss:1.9177141189575195\n",
      "Training loss:1.8677875995635986\n",
      "Training loss:1.9647456407546997\n",
      "Training loss:1.900978922843933\n",
      "Training loss:1.9341102838516235\n",
      "Training loss:1.9322035312652588\n",
      "Training loss:1.9650448560714722\n",
      "Training loss:1.9378471374511719\n",
      "Training loss:1.914814829826355\n",
      "Training loss:1.9379785060882568\n",
      "Training loss:1.8957102298736572\n",
      "Training loss:1.8790112733840942\n",
      "Training loss:1.9234719276428223\n",
      "Training loss:1.8235024213790894\n",
      "Training loss:1.9309977293014526\n",
      "Training loss:1.8313679695129395\n",
      "Training loss:1.861711859703064\n",
      "Training loss:1.9189462661743164\n",
      "Training loss:1.8920543193817139\n",
      "Training loss:1.9136435985565186\n",
      "Training loss:1.9061700105667114\n",
      "Training loss:1.8530265092849731\n",
      "Training loss:1.8528895378112793\n",
      "Training loss:1.9385597705841064\n",
      "Training loss:1.8602497577667236\n",
      "Training loss:1.8779902458190918\n",
      "Training loss:1.87189781665802\n",
      "Training loss:1.9134811162948608\n",
      "Training loss:1.900177001953125\n",
      "Training loss:1.9503957033157349\n",
      "Training loss:1.9307700395584106\n",
      "Training loss:1.9883357286453247\n",
      "Training loss:1.9155166149139404\n",
      "Training loss:1.8531075716018677\n",
      "Training loss:1.922444224357605\n",
      "Training loss:1.9231185913085938\n",
      "Training loss:1.8404074907302856\n",
      "Training loss:1.888140082359314\n",
      "Training loss:1.925104022026062\n",
      "Training loss:1.882053017616272\n",
      "Training loss:1.8948391675949097\n",
      "Training loss:1.9645119905471802\n",
      "Training loss:1.8069074153900146\n",
      "Training loss:1.8203473091125488\n",
      "Training loss:1.8054949045181274\n",
      "Training loss:1.8178900480270386\n",
      "Training loss:1.8327995538711548\n",
      "Training loss:1.8945248126983643\n",
      "Training loss:1.9119470119476318\n",
      "Training loss:1.9129420518875122\n",
      "Training loss:1.9014047384262085\n",
      "Training loss:1.8537039756774902\n",
      "Training loss:1.8510218858718872\n",
      "Training loss:1.8760201930999756\n",
      "Training loss:1.9118626117706299\n",
      "Training loss:1.9398621320724487\n",
      "Training loss:1.8808902502059937\n",
      "Training loss:1.8815268278121948\n",
      "Training loss:1.8378479480743408\n",
      "Training loss:1.8573050498962402\n",
      "Training loss:1.9513109922409058\n",
      "Training loss:1.8700534105300903\n",
      "Training loss:1.8913626670837402\n",
      "Training loss:1.8859363794326782\n",
      "Training loss:1.9123798608779907\n",
      "Training loss:1.8386096954345703\n",
      "Training loss:1.9549124240875244\n",
      "Training loss:1.8737707138061523\n",
      "Training loss:1.9761927127838135\n",
      "Training loss:1.9074559211730957\n",
      "Training loss:1.8425763845443726\n",
      "Training loss:1.8494970798492432\n",
      "Training loss:1.8493698835372925\n",
      "Training loss:1.8586714267730713\n",
      "Training loss:1.9072414636611938\n",
      "Training loss:1.8424369096755981\n",
      "Training loss:1.8994196653366089\n",
      "Training loss:1.857545018196106\n",
      "Training loss:1.9391485452651978\n",
      "Training loss:1.8825922012329102\n",
      "Epoch: 0007 loss_train: 178.036695 acc_train: 0.285562 loss_val: 22.518560 acc_val: 0.303667 time: 16965.434043s\n",
      "Training loss:1.869864583015442\n",
      "Training loss:1.886582612991333\n",
      "Training loss:1.8759149312973022\n",
      "Training loss:1.8989355564117432\n",
      "Training loss:1.8362905979156494\n",
      "Training loss:1.9413999319076538\n",
      "Training loss:1.8139718770980835\n",
      "Training loss:1.8763347864151\n",
      "Training loss:1.865822672843933\n",
      "Training loss:1.8309553861618042\n",
      "Training loss:1.9084789752960205\n",
      "Training loss:1.9176310300827026\n",
      "Training loss:1.9929168224334717\n",
      "Training loss:1.9906655550003052\n",
      "Training loss:1.8552534580230713\n",
      "Training loss:1.987491488456726\n",
      "Training loss:1.9214444160461426\n",
      "Training loss:1.858713984489441\n",
      "Training loss:1.8478137254714966\n",
      "Training loss:1.8669143915176392\n",
      "Training loss:1.926342487335205\n",
      "Training loss:1.8206760883331299\n",
      "Training loss:1.8435341119766235\n",
      "Training loss:1.855725646018982\n",
      "Training loss:1.8163596391677856\n",
      "Training loss:1.8798246383666992\n",
      "Training loss:1.9020557403564453\n",
      "Training loss:1.8896433115005493\n",
      "Training loss:1.876200556755066\n",
      "Training loss:1.846580982208252\n",
      "Training loss:1.9422715902328491\n",
      "Training loss:1.9153825044631958\n",
      "Training loss:1.900647521018982\n",
      "Training loss:1.8789790868759155\n",
      "Training loss:1.8778185844421387\n",
      "Training loss:1.9573960304260254\n",
      "Training loss:1.9699571132659912\n",
      "Training loss:1.8805158138275146\n",
      "Training loss:1.9133223295211792\n",
      "Training loss:1.8620736598968506\n",
      "Training loss:1.8957302570343018\n",
      "Training loss:1.9294440746307373\n",
      "Training loss:1.9027637243270874\n",
      "Training loss:1.9446645975112915\n",
      "Training loss:1.9252957105636597\n",
      "Training loss:1.8838695287704468\n",
      "Training loss:1.8784737586975098\n",
      "Training loss:1.8255460262298584\n",
      "Training loss:1.9888923168182373\n",
      "Training loss:1.9263858795166016\n",
      "Training loss:1.8236387968063354\n",
      "Training loss:1.9626555442810059\n",
      "Training loss:1.8956471681594849\n",
      "Training loss:1.8767985105514526\n",
      "Training loss:1.909091830253601\n",
      "Training loss:1.8706663846969604\n",
      "Training loss:1.9320265054702759\n",
      "Training loss:1.8341366052627563\n",
      "Training loss:1.916848063468933\n",
      "Training loss:1.8107529878616333\n",
      "Training loss:1.8525229692459106\n",
      "Training loss:1.908808946609497\n",
      "Training loss:1.9303172826766968\n",
      "Training loss:1.8650593757629395\n",
      "Training loss:1.9045895338058472\n",
      "Training loss:1.8178253173828125\n",
      "Training loss:1.9383118152618408\n",
      "Training loss:1.8797268867492676\n",
      "Training loss:1.8518483638763428\n",
      "Training loss:1.9569272994995117\n",
      "Training loss:1.8431099653244019\n",
      "Training loss:1.9031329154968262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.806599497795105\n",
      "Training loss:1.8284589052200317\n",
      "Training loss:1.9241132736206055\n",
      "Training loss:1.8259180784225464\n",
      "Training loss:1.9059008359909058\n",
      "Training loss:1.863379955291748\n",
      "Training loss:1.9067541360855103\n",
      "Training loss:1.8712126016616821\n",
      "Training loss:1.8652104139328003\n",
      "Training loss:1.8826614618301392\n",
      "Training loss:1.7674132585525513\n",
      "Training loss:1.8499655723571777\n",
      "Training loss:1.8715715408325195\n",
      "Training loss:1.8870970010757446\n",
      "Training loss:1.8733628988265991\n",
      "Training loss:1.8605899810791016\n",
      "Training loss:1.8656587600708008\n",
      "Training loss:1.865763545036316\n",
      "Training loss:1.8581674098968506\n",
      "Training loss:1.9129626750946045\n",
      "Training loss:1.8241358995437622\n",
      "Training loss:1.9077739715576172\n",
      "Epoch: 0008 loss_train: 177.208885 acc_train: 0.293812 loss_val: 22.245569 acc_val: 0.309000 time: 19325.054084s\n",
      "Training loss:1.842180609703064\n",
      "Training loss:1.8788315057754517\n",
      "Training loss:1.83364999294281\n",
      "Training loss:1.8529938459396362\n",
      "Training loss:1.933984398841858\n",
      "Training loss:1.950806975364685\n",
      "Training loss:1.8767099380493164\n",
      "Training loss:1.8579410314559937\n",
      "Training loss:1.8691080808639526\n",
      "Training loss:1.892563819885254\n",
      "Training loss:1.8193811178207397\n",
      "Training loss:1.881555438041687\n",
      "Training loss:1.7988998889923096\n",
      "Training loss:1.8386070728302002\n",
      "Training loss:1.935048222541809\n",
      "Training loss:1.8423757553100586\n",
      "Training loss:1.8547039031982422\n",
      "Training loss:1.8667175769805908\n",
      "Training loss:1.7996164560317993\n",
      "Training loss:1.9202327728271484\n",
      "Training loss:1.7966560125350952\n",
      "Training loss:1.8282403945922852\n",
      "Training loss:1.8896350860595703\n",
      "Training loss:1.8132760524749756\n",
      "Training loss:1.8832517862319946\n",
      "Training loss:1.889072299003601\n",
      "Training loss:1.8724699020385742\n",
      "Training loss:1.8733540773391724\n",
      "Training loss:1.8954421281814575\n",
      "Training loss:1.8620258569717407\n",
      "Training loss:1.9063397645950317\n",
      "Training loss:1.767272710800171\n",
      "Training loss:1.8438845872879028\n",
      "Training loss:1.8530044555664062\n",
      "Training loss:1.8791643381118774\n",
      "Training loss:1.7894760370254517\n",
      "Training loss:1.8309035301208496\n",
      "Training loss:1.8291809558868408\n",
      "Training loss:1.9166486263275146\n",
      "Training loss:1.859267234802246\n",
      "Training loss:1.8711121082305908\n",
      "Training loss:1.9034026861190796\n",
      "Training loss:1.8679014444351196\n",
      "Training loss:1.8550606966018677\n",
      "Training loss:1.8708711862564087\n",
      "Training loss:1.9573791027069092\n",
      "Training loss:1.8488380908966064\n",
      "Training loss:1.8865782022476196\n",
      "Training loss:1.8716456890106201\n",
      "Training loss:1.915035605430603\n",
      "Training loss:1.8528107404708862\n",
      "Training loss:1.8392767906188965\n",
      "Training loss:1.9594794511795044\n",
      "Training loss:1.8935768604278564\n",
      "Training loss:1.965763807296753\n",
      "Training loss:1.851805567741394\n",
      "Training loss:1.9020705223083496\n",
      "Training loss:1.9244798421859741\n",
      "Training loss:1.8803800344467163\n",
      "Training loss:1.976311206817627\n",
      "Training loss:1.892553687095642\n",
      "Training loss:1.8616739511489868\n",
      "Training loss:1.8760231733322144\n",
      "Training loss:1.8192124366760254\n",
      "Training loss:1.901243805885315\n",
      "Training loss:1.9403727054595947\n",
      "Training loss:1.8517903089523315\n",
      "Training loss:1.7971466779708862\n",
      "Training loss:1.861015796661377\n",
      "Training loss:1.9035634994506836\n",
      "Training loss:1.8552907705307007\n",
      "Training loss:1.859908938407898\n",
      "Training loss:1.8707184791564941\n",
      "Training loss:1.835354208946228\n",
      "Training loss:1.894613265991211\n",
      "Training loss:1.8767176866531372\n",
      "Training loss:1.8845497369766235\n",
      "Training loss:1.8685740232467651\n",
      "Training loss:1.8098853826522827\n",
      "Training loss:1.872216820716858\n",
      "Training loss:1.8670234680175781\n",
      "Training loss:1.8664616346359253\n",
      "Training loss:1.9045517444610596\n",
      "Training loss:1.8303155899047852\n",
      "Training loss:1.8721593618392944\n",
      "Training loss:1.8861509561538696\n",
      "Training loss:1.9118443727493286\n",
      "Training loss:1.8688664436340332\n",
      "Training loss:1.8689062595367432\n",
      "Training loss:1.7711539268493652\n",
      "Training loss:1.8390506505966187\n",
      "Training loss:1.8949915170669556\n",
      "Training loss:1.8654255867004395\n",
      "Training loss:1.8906184434890747\n",
      "Epoch: 0009 loss_train: 175.788199 acc_train: 0.298458 loss_val: 22.089956 acc_val: 0.313833 time: 21709.803015s\n",
      "Training loss:1.900862693786621\n",
      "Training loss:1.839915156364441\n",
      "Training loss:1.8566257953643799\n",
      "Training loss:1.7875306606292725\n",
      "Training loss:1.8464257717132568\n",
      "Training loss:1.828876256942749\n",
      "Training loss:1.7923976182937622\n",
      "Training loss:1.855543851852417\n",
      "Training loss:1.8386142253875732\n",
      "Training loss:1.8354424238204956\n",
      "Training loss:1.8500226736068726\n",
      "Training loss:1.8505046367645264\n",
      "Training loss:1.922626256942749\n",
      "Training loss:1.839559555053711\n",
      "Training loss:1.8600326776504517\n",
      "Training loss:1.8478333950042725\n",
      "Training loss:1.920466661453247\n",
      "Training loss:1.8709681034088135\n",
      "Training loss:1.8743788003921509\n",
      "Training loss:1.907421350479126\n",
      "Training loss:1.8895974159240723\n",
      "Training loss:1.8782405853271484\n",
      "Training loss:1.8243855237960815\n",
      "Training loss:1.88088059425354\n",
      "Training loss:1.873093605041504\n",
      "Training loss:1.8792012929916382\n",
      "Training loss:1.82306706905365\n",
      "Training loss:1.845798373222351\n",
      "Training loss:1.9259964227676392\n",
      "Training loss:1.8433787822723389\n",
      "Training loss:1.8309890031814575\n",
      "Training loss:1.8875929117202759\n",
      "Training loss:1.8901569843292236\n",
      "Training loss:1.8400299549102783\n",
      "Training loss:1.8287608623504639\n",
      "Training loss:1.818464994430542\n",
      "Training loss:1.7810579538345337\n",
      "Training loss:1.855791449546814\n",
      "Training loss:1.8226191997528076\n",
      "Training loss:1.8885318040847778\n",
      "Training loss:1.9007676839828491\n",
      "Training loss:1.8787387609481812\n",
      "Training loss:1.8077255487442017\n",
      "Training loss:1.8137000799179077\n",
      "Training loss:1.8702517747879028\n",
      "Training loss:1.8681399822235107\n",
      "Training loss:1.8385080099105835\n",
      "Training loss:1.772972822189331\n",
      "Training loss:1.8090696334838867\n",
      "Training loss:1.847680687904358\n",
      "Training loss:1.814326286315918\n",
      "Training loss:1.7619140148162842\n",
      "Training loss:1.876893162727356\n",
      "Training loss:1.8260141611099243\n",
      "Training loss:1.80556058883667\n",
      "Training loss:1.8365494012832642\n",
      "Training loss:1.808464765548706\n",
      "Training loss:1.7697410583496094\n",
      "Training loss:1.7921701669692993\n",
      "Training loss:1.9065748453140259\n",
      "Training loss:1.8058730363845825\n",
      "Training loss:1.8449329137802124\n",
      "Training loss:1.8333128690719604\n",
      "Training loss:1.8511217832565308\n",
      "Training loss:1.8880051374435425\n",
      "Training loss:1.8535517454147339\n",
      "Training loss:1.8855316638946533\n",
      "Training loss:1.85558021068573\n",
      "Training loss:1.8060277700424194\n",
      "Training loss:1.8238872289657593\n",
      "Training loss:1.8722927570343018\n",
      "Training loss:1.9041621685028076\n",
      "Training loss:1.9045599699020386\n",
      "Training loss:1.8254876136779785\n",
      "Training loss:1.821602702140808\n",
      "Training loss:1.7762203216552734\n",
      "Training loss:1.8594701290130615\n",
      "Training loss:1.8308765888214111\n",
      "Training loss:1.8582509756088257\n",
      "Training loss:1.880771517753601\n",
      "Training loss:1.8516671657562256\n",
      "Training loss:1.7959473133087158\n",
      "Training loss:1.9011192321777344\n",
      "Training loss:1.7782617807388306\n",
      "Training loss:1.8481876850128174\n",
      "Training loss:1.823492407798767\n",
      "Training loss:1.8707799911499023\n",
      "Training loss:1.8883098363876343\n",
      "Training loss:1.795937418937683\n",
      "Training loss:1.8119310140609741\n",
      "Training loss:1.8219795227050781\n",
      "Training loss:1.8687121868133545\n",
      "Training loss:1.8239314556121826\n",
      "Training loss:1.9283722639083862\n",
      "Epoch: 0010 loss_train: 173.629595 acc_train: 0.304708 loss_val: 21.954963 acc_val: 0.315167 time: 24068.898231s\n",
      "Training loss:1.8150513172149658\n",
      "Training loss:1.8735431432724\n",
      "Training loss:1.8841711282730103\n",
      "Training loss:1.8561121225357056\n",
      "Training loss:1.8138333559036255\n",
      "Training loss:1.8507161140441895\n",
      "Training loss:1.821008324623108\n",
      "Training loss:1.8575774431228638\n",
      "Training loss:1.8626710176467896\n",
      "Training loss:1.8722504377365112\n",
      "Training loss:1.836841106414795\n",
      "Training loss:1.801066517829895\n",
      "Training loss:1.8282976150512695\n",
      "Training loss:1.8022586107254028\n",
      "Training loss:1.8193423748016357\n",
      "Training loss:1.8291335105895996\n",
      "Training loss:1.7933145761489868\n",
      "Training loss:1.832940697669983\n",
      "Training loss:1.8069642782211304\n",
      "Training loss:1.8194990158081055\n",
      "Training loss:1.8091169595718384\n",
      "Training loss:1.8201593160629272\n",
      "Training loss:1.8857346773147583\n",
      "Training loss:1.8286793231964111\n",
      "Training loss:1.8362414836883545\n",
      "Training loss:1.8645154237747192\n",
      "Training loss:1.8466864824295044\n",
      "Training loss:1.7643275260925293\n",
      "Training loss:1.7726874351501465\n",
      "Training loss:1.906140685081482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.7846546173095703\n",
      "Training loss:1.8622360229492188\n",
      "Training loss:1.8396662473678589\n",
      "Training loss:1.8379873037338257\n",
      "Training loss:1.8705817461013794\n",
      "Training loss:1.87871515750885\n",
      "Training loss:1.86555016040802\n",
      "Training loss:1.7871395349502563\n",
      "Training loss:1.8583365678787231\n",
      "Training loss:1.8585383892059326\n",
      "Training loss:1.8124345541000366\n",
      "Training loss:1.8451811075210571\n",
      "Training loss:1.8571794033050537\n",
      "Training loss:1.905220866203308\n",
      "Training loss:1.8599371910095215\n",
      "Training loss:1.8968446254730225\n",
      "Training loss:1.8178530931472778\n",
      "Training loss:1.8992208242416382\n",
      "Training loss:1.8019962310791016\n",
      "Training loss:1.8689919710159302\n",
      "Training loss:1.8643858432769775\n",
      "Training loss:1.867207646369934\n",
      "Training loss:1.8330687284469604\n",
      "Training loss:1.7762150764465332\n",
      "Training loss:1.8757503032684326\n",
      "Training loss:1.8569601774215698\n",
      "Training loss:1.8658710718154907\n",
      "Training loss:1.8304435014724731\n",
      "Training loss:1.7480268478393555\n",
      "Training loss:1.8219997882843018\n",
      "Training loss:1.874943733215332\n",
      "Training loss:1.8510782718658447\n",
      "Training loss:1.8435605764389038\n",
      "Training loss:1.7926788330078125\n",
      "Training loss:1.9085942506790161\n",
      "Training loss:1.8912279605865479\n",
      "Training loss:1.8301411867141724\n",
      "Training loss:1.8498386144638062\n",
      "Training loss:1.8497328758239746\n",
      "Training loss:1.8422844409942627\n",
      "Training loss:1.8707016706466675\n",
      "Training loss:1.8353071212768555\n",
      "Training loss:1.8297255039215088\n",
      "Training loss:1.8247504234313965\n",
      "Training loss:1.8230366706848145\n",
      "Training loss:1.812529444694519\n",
      "Training loss:1.9281216859817505\n",
      "Training loss:1.8212467432022095\n",
      "Training loss:1.888723611831665\n",
      "Training loss:1.7797433137893677\n",
      "Training loss:1.8564454317092896\n",
      "Training loss:1.8491170406341553\n",
      "Training loss:1.8121132850646973\n",
      "Training loss:1.7908995151519775\n",
      "Training loss:1.7889621257781982\n",
      "Training loss:1.8026925325393677\n",
      "Training loss:1.8582355976104736\n",
      "Training loss:1.8432180881500244\n",
      "Training loss:1.8100066184997559\n",
      "Training loss:1.9191014766693115\n",
      "Training loss:1.8539570569992065\n",
      "Training loss:1.718024730682373\n",
      "Training loss:1.774656891822815\n",
      "Training loss:1.8266571760177612\n",
      "Epoch: 0011 loss_train: 172.811131 acc_train: 0.309583 loss_val: 21.805789 acc_val: 0.320500 time: 26488.549710s\n",
      "Training loss:1.8379919528961182\n",
      "Training loss:1.8173903226852417\n",
      "Training loss:1.773127555847168\n",
      "Training loss:1.8112542629241943\n",
      "Training loss:1.854128360748291\n",
      "Training loss:1.8339745998382568\n",
      "Training loss:1.8026690483093262\n",
      "Training loss:1.9069494009017944\n",
      "Training loss:1.8730266094207764\n",
      "Training loss:1.7957789897918701\n",
      "Training loss:1.7505269050598145\n",
      "Training loss:1.833358645439148\n",
      "Training loss:1.8323354721069336\n",
      "Training loss:1.7984678745269775\n",
      "Training loss:1.7900476455688477\n",
      "Training loss:1.8118817806243896\n",
      "Training loss:1.8581780195236206\n",
      "Training loss:1.7920141220092773\n",
      "Training loss:1.8242601156234741\n",
      "Training loss:1.7637861967086792\n",
      "Training loss:1.8335763216018677\n",
      "Training loss:1.8072444200515747\n",
      "Training loss:1.871094822883606\n",
      "Training loss:1.8663064241409302\n",
      "Training loss:1.785503625869751\n",
      "Training loss:1.8166714906692505\n",
      "Training loss:1.8163434267044067\n",
      "Training loss:1.799967646598816\n",
      "Training loss:1.851686716079712\n",
      "Training loss:1.8736754655838013\n",
      "Training loss:1.7406173944473267\n",
      "Training loss:1.845571756362915\n",
      "Training loss:1.8564640283584595\n",
      "Training loss:1.8122625350952148\n",
      "Training loss:1.853589415550232\n",
      "Training loss:1.815902829170227\n",
      "Training loss:1.875795602798462\n",
      "Training loss:1.7751061916351318\n",
      "Training loss:1.8232109546661377\n",
      "Training loss:1.8792651891708374\n",
      "Training loss:1.8318421840667725\n",
      "Training loss:1.8538460731506348\n",
      "Training loss:1.8111783266067505\n",
      "Training loss:1.8267964124679565\n",
      "Training loss:1.8135433197021484\n",
      "Training loss:1.8581926822662354\n",
      "Training loss:1.8526890277862549\n",
      "Training loss:1.7269307374954224\n",
      "Training loss:1.8269562721252441\n",
      "Training loss:1.801550269126892\n",
      "Training loss:1.7816557884216309\n",
      "Training loss:1.816507339477539\n",
      "Training loss:1.8926564455032349\n",
      "Training loss:1.797966480255127\n",
      "Training loss:1.8375053405761719\n",
      "Training loss:1.81155264377594\n",
      "Training loss:1.8157069683074951\n",
      "Training loss:1.838631510734558\n",
      "Training loss:1.8862216472625732\n",
      "Training loss:1.8713407516479492\n",
      "Training loss:1.855672836303711\n",
      "Training loss:1.8962268829345703\n",
      "Training loss:1.79035222530365\n",
      "Training loss:1.8050427436828613\n",
      "Training loss:1.819631814956665\n",
      "Training loss:1.7950602769851685\n",
      "Training loss:1.8022710084915161\n",
      "Training loss:1.7880280017852783\n",
      "Training loss:1.7959926128387451\n",
      "Training loss:1.7970223426818848\n",
      "Training loss:1.835697889328003\n",
      "Training loss:1.8061484098434448\n",
      "Training loss:1.852341890335083\n",
      "Training loss:1.8860195875167847\n",
      "Training loss:1.7510313987731934\n",
      "Training loss:1.7244817018508911\n",
      "Training loss:1.8626686334609985\n",
      "Training loss:1.758981466293335\n",
      "Training loss:1.823315143585205\n",
      "Training loss:1.7964756488800049\n",
      "Training loss:1.8067781925201416\n",
      "Training loss:1.8352755308151245\n",
      "Training loss:1.9228684902191162\n",
      "Training loss:1.876207709312439\n",
      "Training loss:1.8777955770492554\n",
      "Training loss:1.7752981185913086\n",
      "Training loss:1.868005633354187\n",
      "Training loss:1.8529688119888306\n",
      "Training loss:1.7770771980285645\n",
      "Training loss:1.8365130424499512\n",
      "Training loss:1.8215936422348022\n",
      "Training loss:1.868260145187378\n",
      "Training loss:1.8263976573944092\n",
      "Training loss:1.6981428861618042\n",
      "Epoch: 0012 loss_train: 171.399920 acc_train: 0.316396 loss_val: 21.637483 acc_val: 0.328333 time: 28892.431596s\n",
      "Training loss:1.8983641862869263\n",
      "Training loss:1.7761566638946533\n",
      "Training loss:1.7813177108764648\n",
      "Training loss:1.8613224029541016\n",
      "Training loss:1.838989019393921\n",
      "Training loss:1.7673105001449585\n",
      "Training loss:1.7872401475906372\n",
      "Training loss:1.7887334823608398\n",
      "Training loss:1.783531904220581\n",
      "Training loss:1.879786729812622\n",
      "Training loss:1.8368269205093384\n",
      "Training loss:1.8188878297805786\n",
      "Training loss:1.7644017934799194\n",
      "Training loss:1.8257498741149902\n",
      "Training loss:1.751287579536438\n",
      "Training loss:1.786718726158142\n",
      "Training loss:1.9018930196762085\n",
      "Training loss:1.7713713645935059\n",
      "Training loss:1.8243111371994019\n",
      "Training loss:1.7743219137191772\n",
      "Training loss:1.8552461862564087\n",
      "Training loss:1.7973523139953613\n",
      "Training loss:1.7619060277938843\n",
      "Training loss:1.847336769104004\n",
      "Training loss:1.7668818235397339\n",
      "Training loss:1.877400279045105\n",
      "Training loss:1.8216499090194702\n",
      "Training loss:1.7367267608642578\n",
      "Training loss:1.8259413242340088\n",
      "Training loss:1.7089344263076782\n",
      "Training loss:1.7915376424789429\n",
      "Training loss:1.8567532300949097\n",
      "Training loss:1.779038906097412\n",
      "Training loss:1.8252166509628296\n",
      "Training loss:1.8371766805648804\n",
      "Training loss:1.809765100479126\n",
      "Training loss:1.8136591911315918\n",
      "Training loss:1.7924703359603882\n",
      "Training loss:1.9041534662246704\n",
      "Training loss:1.8093820810317993\n",
      "Training loss:1.8407131433486938\n",
      "Training loss:1.7848589420318604\n",
      "Training loss:1.8439526557922363\n",
      "Training loss:1.7968895435333252\n",
      "Training loss:1.8079164028167725\n",
      "Training loss:1.7019834518432617\n",
      "Training loss:1.803120732307434\n",
      "Training loss:1.7488946914672852\n",
      "Training loss:1.8139302730560303\n",
      "Training loss:1.8228482007980347\n",
      "Training loss:1.8750125169754028\n",
      "Training loss:1.8525117635726929\n",
      "Training loss:1.7638622522354126\n",
      "Training loss:1.8354650735855103\n",
      "Training loss:1.7873399257659912\n",
      "Training loss:1.8628312349319458\n",
      "Training loss:1.8098785877227783\n",
      "Training loss:1.7791826725006104\n",
      "Training loss:1.8137532472610474\n",
      "Training loss:1.8728243112564087\n",
      "Training loss:1.827465534210205\n",
      "Training loss:1.7848306894302368\n",
      "Training loss:1.8551108837127686\n",
      "Training loss:1.8131707906723022\n",
      "Training loss:1.833951473236084\n",
      "Training loss:1.7974225282669067\n",
      "Training loss:1.8121023178100586\n",
      "Training loss:1.791864037513733\n",
      "Training loss:1.82132887840271\n",
      "Training loss:1.8354352712631226\n",
      "Training loss:1.866080641746521\n",
      "Training loss:1.7660105228424072\n",
      "Training loss:1.8484954833984375\n",
      "Training loss:1.8300998210906982\n",
      "Training loss:1.771571159362793\n",
      "Training loss:1.810789704322815\n",
      "Training loss:1.7782690525054932\n",
      "Training loss:1.8104664087295532\n",
      "Training loss:1.7772672176361084\n",
      "Training loss:1.7754539251327515\n",
      "Training loss:1.8376928567886353\n",
      "Training loss:1.7945998907089233\n",
      "Training loss:1.7429739236831665\n",
      "Training loss:1.784393072128296\n",
      "Training loss:1.8536062240600586\n",
      "Training loss:1.82477867603302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.8701164722442627\n",
      "Training loss:1.7654610872268677\n",
      "Training loss:1.8486087322235107\n",
      "Training loss:1.782970905303955\n",
      "Training loss:1.8078064918518066\n",
      "Training loss:1.8660067319869995\n",
      "Training loss:1.7847281694412231\n",
      "Training loss:1.865869402885437\n",
      "Epoch: 0013 loss_train: 170.291621 acc_train: 0.323188 loss_val: 21.544415 acc_val: 0.324667 time: 31268.414071s\n",
      "Training loss:1.723551869392395\n",
      "Training loss:1.869072675704956\n",
      "Training loss:1.7370591163635254\n",
      "Training loss:1.801883578300476\n",
      "Training loss:1.8527398109436035\n",
      "Training loss:1.7720208168029785\n",
      "Training loss:1.7736446857452393\n",
      "Training loss:1.785964846611023\n",
      "Training loss:1.806121826171875\n",
      "Training loss:1.723186731338501\n",
      "Training loss:1.7825429439544678\n",
      "Training loss:1.7341406345367432\n",
      "Training loss:1.9035613536834717\n",
      "Training loss:1.780648946762085\n",
      "Training loss:1.7794599533081055\n",
      "Training loss:1.7233951091766357\n",
      "Training loss:1.8559033870697021\n",
      "Training loss:1.7656279802322388\n",
      "Training loss:1.7151844501495361\n",
      "Training loss:1.8199357986450195\n",
      "Training loss:1.7315610647201538\n",
      "Training loss:1.80893874168396\n",
      "Training loss:1.7936450242996216\n",
      "Training loss:1.706838607788086\n",
      "Training loss:1.8097143173217773\n",
      "Training loss:1.7184669971466064\n",
      "Training loss:1.8522818088531494\n",
      "Training loss:1.7674232721328735\n",
      "Training loss:1.843605875968933\n",
      "Training loss:1.7820115089416504\n",
      "Training loss:1.8095169067382812\n",
      "Training loss:1.797555685043335\n",
      "Training loss:1.789458155632019\n",
      "Training loss:1.8234844207763672\n",
      "Training loss:1.8023643493652344\n",
      "Training loss:1.8131000995635986\n",
      "Training loss:1.8680219650268555\n",
      "Training loss:1.78214693069458\n",
      "Training loss:1.8131588697433472\n",
      "Training loss:1.7321492433547974\n",
      "Training loss:1.890470266342163\n",
      "Training loss:1.763469934463501\n",
      "Training loss:1.8102060556411743\n",
      "Training loss:1.7443468570709229\n",
      "Training loss:1.7472903728485107\n",
      "Training loss:1.8087207078933716\n",
      "Training loss:1.7358016967773438\n",
      "Training loss:1.7385215759277344\n",
      "Training loss:1.8198552131652832\n",
      "Training loss:1.8020055294036865\n",
      "Training loss:1.7912564277648926\n",
      "Training loss:1.8383132219314575\n",
      "Training loss:1.7793673276901245\n",
      "Training loss:1.7522186040878296\n",
      "Training loss:1.8166429996490479\n",
      "Training loss:1.8614685535430908\n",
      "Training loss:1.8217954635620117\n",
      "Training loss:1.73307204246521\n",
      "Training loss:1.7391798496246338\n",
      "Training loss:1.7961838245391846\n",
      "Training loss:1.7909367084503174\n",
      "Training loss:1.8560049533843994\n",
      "Training loss:1.857100009918213\n",
      "Training loss:1.7693307399749756\n",
      "Training loss:1.8521504402160645\n",
      "Training loss:1.8614915609359741\n",
      "Training loss:1.8301668167114258\n",
      "Training loss:1.7947211265563965\n",
      "Training loss:1.8413277864456177\n",
      "Training loss:1.7743749618530273\n",
      "Training loss:1.8270411491394043\n",
      "Training loss:1.8437927961349487\n",
      "Training loss:1.8365588188171387\n",
      "Training loss:1.7912575006484985\n",
      "Training loss:1.84256112575531\n",
      "Training loss:1.8594199419021606\n",
      "Training loss:1.8266398906707764\n",
      "Training loss:1.781412124633789\n",
      "Training loss:1.7247952222824097\n",
      "Training loss:1.7935950756072998\n",
      "Training loss:1.7914621829986572\n",
      "Training loss:1.8106096982955933\n",
      "Training loss:1.7939468622207642\n",
      "Training loss:1.8044923543930054\n",
      "Training loss:1.8347232341766357\n",
      "Training loss:1.784835696220398\n",
      "Training loss:1.8454724550247192\n",
      "Training loss:1.8306916952133179\n",
      "Training loss:1.8135892152786255\n",
      "Training loss:1.8265221118927002\n",
      "Training loss:1.7871452569961548\n",
      "Training loss:1.8810780048370361\n",
      "Training loss:1.810939073562622\n",
      "Training loss:1.7899781465530396\n",
      "Epoch: 0014 loss_train: 169.101442 acc_train: 0.327792 loss_val: 21.339586 acc_val: 0.344500 time: 33668.507736s\n",
      "Training loss:1.7591545581817627\n",
      "Training loss:1.785228967666626\n",
      "Training loss:1.7723381519317627\n",
      "Training loss:1.8137584924697876\n",
      "Training loss:1.8559744358062744\n",
      "Training loss:1.8650320768356323\n",
      "Training loss:1.7646006345748901\n",
      "Training loss:1.85960054397583\n",
      "Training loss:1.7922757863998413\n",
      "Training loss:1.8147610425949097\n",
      "Training loss:1.783661127090454\n",
      "Training loss:1.7469524145126343\n",
      "Training loss:1.7896215915679932\n",
      "Training loss:1.7294058799743652\n",
      "Training loss:1.7774324417114258\n",
      "Training loss:1.7454633712768555\n",
      "Training loss:1.869983434677124\n",
      "Training loss:1.721239686012268\n",
      "Training loss:1.8522197008132935\n",
      "Training loss:1.8196914196014404\n",
      "Training loss:1.8302329778671265\n",
      "Training loss:1.7745634317398071\n",
      "Training loss:1.8006523847579956\n",
      "Training loss:1.7672247886657715\n",
      "Training loss:1.7199699878692627\n",
      "Training loss:1.8105263710021973\n",
      "Training loss:1.7240893840789795\n",
      "Training loss:1.833368182182312\n",
      "Training loss:1.812127709388733\n",
      "Training loss:1.8812757730484009\n",
      "Training loss:1.8288452625274658\n",
      "Training loss:1.7459516525268555\n",
      "Training loss:1.7978594303131104\n",
      "Training loss:1.696224570274353\n",
      "Training loss:1.7520338296890259\n",
      "Training loss:1.7779357433319092\n",
      "Training loss:1.7724710702896118\n",
      "Training loss:1.7732230424880981\n",
      "Training loss:1.7961535453796387\n",
      "Training loss:1.805880069732666\n",
      "Training loss:1.8299881219863892\n",
      "Training loss:1.7790968418121338\n",
      "Training loss:1.825520634651184\n",
      "Training loss:1.7548061609268188\n",
      "Training loss:1.7984511852264404\n",
      "Training loss:1.7852020263671875\n",
      "Training loss:1.7360641956329346\n",
      "Training loss:1.8299202919006348\n",
      "Training loss:1.8350982666015625\n",
      "Training loss:1.8116455078125\n",
      "Training loss:1.818021535873413\n",
      "Training loss:1.758552074432373\n",
      "Training loss:1.8392168283462524\n",
      "Training loss:1.8444769382476807\n",
      "Training loss:1.7923284769058228\n",
      "Training loss:1.813466191291809\n",
      "Training loss:1.8150761127471924\n",
      "Training loss:1.818895697593689\n",
      "Training loss:1.8074663877487183\n",
      "Training loss:1.8153865337371826\n",
      "Training loss:1.7504206895828247\n",
      "Training loss:1.774798035621643\n",
      "Training loss:1.776248812675476\n",
      "Training loss:1.7565844058990479\n",
      "Training loss:1.7785042524337769\n",
      "Training loss:1.7952580451965332\n",
      "Training loss:1.890555739402771\n",
      "Training loss:1.8363443613052368\n",
      "Training loss:1.775195598602295\n",
      "Training loss:1.738925576210022\n",
      "Training loss:1.8433367013931274\n",
      "Training loss:1.8386938571929932\n",
      "Training loss:1.8385740518569946\n",
      "Training loss:1.7862651348114014\n",
      "Training loss:1.811624526977539\n",
      "Training loss:1.8307589292526245\n",
      "Training loss:1.8301019668579102\n",
      "Training loss:1.7849675416946411\n",
      "Training loss:1.831815242767334\n",
      "Training loss:1.7501474618911743\n",
      "Training loss:1.8566914796829224\n",
      "Training loss:1.8420724868774414\n",
      "Training loss:1.8508853912353516\n",
      "Training loss:1.7485300302505493\n",
      "Training loss:1.7606070041656494\n",
      "Training loss:1.7811087369918823\n",
      "Training loss:1.7591450214385986\n",
      "Training loss:1.777441143989563\n",
      "Training loss:1.7705398797988892\n",
      "Training loss:1.7536921501159668\n",
      "Training loss:1.7647186517715454\n",
      "Training loss:1.7900596857070923\n",
      "Training loss:1.7981303930282593\n",
      "Training loss:1.7399638891220093\n",
      "Epoch: 0015 loss_train: 168.740392 acc_train: 0.333042 loss_val: 21.272608 acc_val: 0.342667 time: 36064.971712s\n",
      "Training loss:1.7298667430877686\n",
      "Training loss:1.760614037513733\n",
      "Training loss:1.7698121070861816\n",
      "Training loss:1.7766162157058716\n",
      "Training loss:1.7761716842651367\n",
      "Training loss:1.7815535068511963\n",
      "Training loss:1.7691820859909058\n",
      "Training loss:1.8704794645309448\n",
      "Training loss:1.8385382890701294\n",
      "Training loss:1.8134206533432007\n",
      "Training loss:1.8353384733200073\n",
      "Training loss:1.811152696609497\n",
      "Training loss:1.7892930507659912\n",
      "Training loss:1.738747477531433\n",
      "Training loss:1.8020944595336914\n",
      "Training loss:1.7699599266052246\n",
      "Training loss:1.7604411840438843\n",
      "Training loss:1.7458232641220093\n",
      "Training loss:1.746026873588562\n",
      "Training loss:1.812564492225647\n",
      "Training loss:1.7772046327590942\n",
      "Training loss:1.7436730861663818\n",
      "Training loss:1.8060060739517212\n",
      "Training loss:1.7503689527511597\n",
      "Training loss:1.7760977745056152\n",
      "Training loss:1.8277496099472046\n",
      "Training loss:1.8940738439559937\n",
      "Training loss:1.7902237176895142\n",
      "Training loss:1.7023035287857056\n",
      "Training loss:1.7684814929962158\n",
      "Training loss:1.810172438621521\n",
      "Training loss:1.7557045221328735\n",
      "Training loss:1.7546409368515015\n",
      "Training loss:1.7833360433578491\n",
      "Training loss:1.7784488201141357\n",
      "Training loss:1.8536550998687744\n",
      "Training loss:1.8057230710983276\n",
      "Training loss:1.8072688579559326\n",
      "Training loss:1.9180991649627686\n",
      "Training loss:1.7496089935302734\n",
      "Training loss:1.7063493728637695\n",
      "Training loss:1.86111319065094\n",
      "Training loss:1.769183874130249\n",
      "Training loss:1.7690179347991943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.8212394714355469\n",
      "Training loss:1.8777834177017212\n",
      "Training loss:1.8936469554901123\n",
      "Training loss:1.8358477354049683\n",
      "Training loss:1.790665626525879\n",
      "Training loss:1.8030242919921875\n",
      "Training loss:1.8384300470352173\n",
      "Training loss:1.7609362602233887\n",
      "Training loss:1.851797103881836\n",
      "Training loss:1.707263469696045\n",
      "Training loss:1.7499881982803345\n",
      "Training loss:1.7320729494094849\n",
      "Training loss:1.771369218826294\n",
      "Training loss:1.733544111251831\n",
      "Training loss:1.8391317129135132\n",
      "Training loss:1.7275558710098267\n",
      "Training loss:1.8159595727920532\n",
      "Training loss:1.7748655080795288\n",
      "Training loss:1.771411657333374\n",
      "Training loss:1.7458983659744263\n",
      "Training loss:1.7340693473815918\n",
      "Training loss:1.7270889282226562\n",
      "Training loss:1.8293988704681396\n",
      "Training loss:1.8550007343292236\n",
      "Training loss:1.8085280656814575\n",
      "Training loss:1.8636469841003418\n",
      "Training loss:1.8162343502044678\n",
      "Training loss:1.7530014514923096\n",
      "Training loss:1.8104925155639648\n",
      "Training loss:1.848327398300171\n",
      "Training loss:1.8015373945236206\n",
      "Training loss:1.8206806182861328\n",
      "Training loss:1.8164929151535034\n",
      "Training loss:1.8454753160476685\n",
      "Training loss:1.8217076063156128\n",
      "Training loss:1.7630480527877808\n",
      "Training loss:1.8470429182052612\n",
      "Training loss:1.8066515922546387\n",
      "Training loss:1.7600305080413818\n",
      "Training loss:1.811044692993164\n",
      "Training loss:1.810972809791565\n",
      "Training loss:1.8469465970993042\n",
      "Training loss:1.801491141319275\n",
      "Training loss:1.846168875694275\n",
      "Training loss:1.7619887590408325\n",
      "Training loss:1.8199392557144165\n",
      "Training loss:1.7532105445861816\n",
      "Training loss:1.8051615953445435\n",
      "Training loss:1.8673818111419678\n",
      "Training loss:1.793102741241455\n",
      "Epoch: 0016 loss_train: 168.744498 acc_train: 0.331479 loss_val: 21.286581 acc_val: 0.340167 time: 38430.546782s\n",
      "Training loss:1.797073245048523\n",
      "Training loss:1.7961167097091675\n",
      "Training loss:1.7690980434417725\n",
      "Training loss:1.7769496440887451\n",
      "Training loss:1.8143519163131714\n",
      "Training loss:1.8195313215255737\n",
      "Training loss:1.8010426759719849\n",
      "Training loss:1.8157707452774048\n",
      "Training loss:1.7704582214355469\n",
      "Training loss:1.7655304670333862\n",
      "Training loss:1.7353110313415527\n",
      "Training loss:1.8191478252410889\n",
      "Training loss:1.8007465600967407\n",
      "Training loss:1.7641363143920898\n",
      "Training loss:1.6937484741210938\n",
      "Training loss:1.7417043447494507\n",
      "Training loss:1.786447525024414\n",
      "Training loss:1.7642136812210083\n",
      "Training loss:1.7374374866485596\n",
      "Training loss:1.7160674333572388\n",
      "Training loss:1.823677659034729\n",
      "Training loss:1.7905309200286865\n",
      "Training loss:1.7316471338272095\n",
      "Training loss:1.8116029500961304\n",
      "Training loss:1.7999266386032104\n",
      "Training loss:1.8609974384307861\n",
      "Training loss:1.7143572568893433\n",
      "Training loss:1.8196359872817993\n",
      "Training loss:1.7972640991210938\n",
      "Training loss:1.8355040550231934\n",
      "Training loss:1.8209478855133057\n",
      "Training loss:1.7194921970367432\n",
      "Training loss:1.7890541553497314\n",
      "Training loss:1.8229734897613525\n",
      "Training loss:1.716783046722412\n",
      "Training loss:1.82245934009552\n",
      "Training loss:1.746192455291748\n",
      "Training loss:1.812584638595581\n",
      "Training loss:1.8008345365524292\n",
      "Training loss:1.8270187377929688\n",
      "Training loss:1.7599190473556519\n",
      "Training loss:1.7363122701644897\n",
      "Training loss:1.7375479936599731\n",
      "Training loss:1.720988154411316\n",
      "Training loss:1.78485906124115\n",
      "Training loss:1.7669235467910767\n",
      "Training loss:1.7853403091430664\n",
      "Training loss:1.78148353099823\n",
      "Training loss:1.7691236734390259\n",
      "Training loss:1.7610872983932495\n",
      "Training loss:1.773219347000122\n",
      "Training loss:1.7779030799865723\n",
      "Training loss:1.7630090713500977\n",
      "Training loss:1.761782169342041\n",
      "Training loss:1.7541545629501343\n",
      "Training loss:1.7960054874420166\n",
      "Training loss:1.7939112186431885\n",
      "Training loss:1.7817662954330444\n",
      "Training loss:1.735873818397522\n",
      "Training loss:1.7378532886505127\n",
      "Training loss:1.7314674854278564\n",
      "Training loss:1.8092328310012817\n",
      "Training loss:1.8145991563796997\n",
      "Training loss:1.7803078889846802\n",
      "Training loss:1.7205779552459717\n",
      "Training loss:1.7897398471832275\n",
      "Training loss:1.7928524017333984\n",
      "Training loss:1.7701265811920166\n",
      "Training loss:1.766610026359558\n",
      "Training loss:1.8263684511184692\n",
      "Training loss:1.7317852973937988\n",
      "Training loss:1.7353595495224\n",
      "Training loss:1.8103644847869873\n",
      "Training loss:1.8180145025253296\n",
      "Training loss:1.7817580699920654\n",
      "Training loss:1.7549729347229004\n",
      "Training loss:1.7619545459747314\n",
      "Training loss:1.8218988180160522\n",
      "Training loss:1.7421329021453857\n",
      "Training loss:1.7959749698638916\n",
      "Training loss:1.7597476243972778\n",
      "Training loss:1.8317152261734009\n",
      "Training loss:1.7509446144104004\n",
      "Training loss:1.8355746269226074\n",
      "Training loss:1.7204898595809937\n",
      "Training loss:1.7173336744308472\n",
      "Training loss:1.8303759098052979\n",
      "Training loss:1.7618449926376343\n",
      "Training loss:1.7296147346496582\n",
      "Training loss:1.802052617073059\n",
      "Training loss:1.7312939167022705\n",
      "Training loss:1.8012499809265137\n",
      "Training loss:1.7883697748184204\n",
      "Training loss:1.7511667013168335\n",
      "Epoch: 0017 loss_train: 166.995304 acc_train: 0.338667 loss_val: 21.082108 acc_val: 0.345333 time: 40787.904509s\n",
      "Training loss:1.7513235807418823\n",
      "Training loss:1.8007919788360596\n",
      "Training loss:1.7644119262695312\n",
      "Training loss:1.789581060409546\n",
      "Training loss:1.8312064409255981\n",
      "Training loss:1.8058546781539917\n",
      "Training loss:1.7533644437789917\n",
      "Training loss:1.7794166803359985\n",
      "Training loss:1.7506109476089478\n",
      "Training loss:1.7654707431793213\n",
      "Training loss:1.7193430662155151\n",
      "Training loss:1.787421703338623\n",
      "Training loss:1.7603312730789185\n",
      "Training loss:1.7572965621948242\n",
      "Training loss:1.7285431623458862\n",
      "Training loss:1.9134330749511719\n",
      "Training loss:1.7175811529159546\n",
      "Training loss:1.767337679862976\n",
      "Training loss:1.7738347053527832\n",
      "Training loss:1.7028350830078125\n",
      "Training loss:1.690456748008728\n",
      "Training loss:1.7774630784988403\n",
      "Training loss:1.7961699962615967\n",
      "Training loss:1.7681218385696411\n",
      "Training loss:1.7438428401947021\n",
      "Training loss:1.7577227354049683\n",
      "Training loss:1.7706787586212158\n",
      "Training loss:1.738132119178772\n",
      "Training loss:1.830684781074524\n",
      "Training loss:1.7094982862472534\n",
      "Training loss:1.8202213048934937\n",
      "Training loss:1.7762571573257446\n",
      "Training loss:1.781195878982544\n",
      "Training loss:1.7318986654281616\n",
      "Training loss:1.6934667825698853\n",
      "Training loss:1.8285845518112183\n",
      "Training loss:1.6793566942214966\n",
      "Training loss:1.7882741689682007\n",
      "Training loss:1.7840851545333862\n",
      "Training loss:1.8098187446594238\n",
      "Training loss:1.7940500974655151\n",
      "Training loss:1.7842923402786255\n",
      "Training loss:1.8097726106643677\n",
      "Training loss:1.740828514099121\n",
      "Training loss:1.7501147985458374\n",
      "Training loss:1.7856301069259644\n",
      "Training loss:1.7984179258346558\n",
      "Training loss:1.7340116500854492\n",
      "Training loss:1.7816880941390991\n",
      "Training loss:1.7277134656906128\n",
      "Training loss:1.760725975036621\n",
      "Training loss:1.7775253057479858\n",
      "Training loss:1.7814655303955078\n",
      "Training loss:1.7806917428970337\n",
      "Training loss:1.7611124515533447\n",
      "Training loss:1.726473331451416\n",
      "Training loss:1.7754416465759277\n",
      "Training loss:1.7598373889923096\n",
      "Training loss:1.8365789651870728\n",
      "Training loss:1.7606292963027954\n",
      "Training loss:1.7729828357696533\n",
      "Training loss:1.749469518661499\n",
      "Training loss:1.8080581426620483\n",
      "Training loss:1.807368516921997\n",
      "Training loss:1.7285723686218262\n",
      "Training loss:1.7918318510055542\n",
      "Training loss:1.8123266696929932\n",
      "Training loss:1.7577831745147705\n",
      "Training loss:1.8014715909957886\n",
      "Training loss:1.77366304397583\n",
      "Training loss:1.7063530683517456\n",
      "Training loss:1.8006149530410767\n",
      "Training loss:1.737217664718628\n",
      "Training loss:1.7662235498428345\n",
      "Training loss:1.7417017221450806\n",
      "Training loss:1.8794031143188477\n",
      "Training loss:1.714273452758789\n",
      "Training loss:1.7790868282318115\n",
      "Training loss:1.792063593864441\n",
      "Training loss:1.7441027164459229\n",
      "Training loss:1.7543206214904785\n",
      "Training loss:1.8175336122512817\n",
      "Training loss:1.7420798540115356\n",
      "Training loss:1.7725521326065063\n",
      "Training loss:1.8268264532089233\n",
      "Training loss:1.8375592231750488\n",
      "Training loss:1.7928153276443481\n",
      "Training loss:1.8078219890594482\n",
      "Training loss:1.806894302368164\n",
      "Training loss:1.7505420446395874\n",
      "Training loss:1.7908283472061157\n",
      "Training loss:1.782123327255249\n",
      "Training loss:1.7217904329299927\n",
      "Training loss:1.7937594652175903\n",
      "Epoch: 0018 loss_train: 166.584937 acc_train: 0.337625 loss_val: 21.031604 acc_val: 0.347000 time: 43173.742549s\n",
      "Training loss:1.7481105327606201\n",
      "Training loss:1.8775728940963745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.808672308921814\n",
      "Training loss:1.7507973909378052\n",
      "Training loss:1.8185737133026123\n",
      "Training loss:1.8015204668045044\n",
      "Training loss:1.7331576347351074\n",
      "Training loss:1.8373878002166748\n",
      "Training loss:1.7972772121429443\n",
      "Training loss:1.8120094537734985\n",
      "Training loss:1.7444192171096802\n",
      "Training loss:1.7209633588790894\n",
      "Training loss:1.7293243408203125\n",
      "Training loss:1.815888524055481\n",
      "Training loss:1.7967958450317383\n",
      "Training loss:1.7807929515838623\n",
      "Training loss:1.7978137731552124\n",
      "Training loss:1.7691136598587036\n",
      "Training loss:1.7059012651443481\n",
      "Training loss:1.7911205291748047\n",
      "Training loss:1.7618064880371094\n",
      "Training loss:1.7491265535354614\n",
      "Training loss:1.802592396736145\n",
      "Training loss:1.7741326093673706\n",
      "Training loss:1.7449543476104736\n",
      "Training loss:1.8057743310928345\n",
      "Training loss:1.7257198095321655\n",
      "Training loss:1.7559545040130615\n",
      "Training loss:1.7701274156570435\n",
      "Training loss:1.7781951427459717\n",
      "Training loss:1.773518443107605\n",
      "Training loss:1.7783514261245728\n",
      "Training loss:1.8307992219924927\n",
      "Training loss:1.7605490684509277\n",
      "Training loss:1.7552697658538818\n",
      "Training loss:1.8017692565917969\n",
      "Training loss:1.7288779020309448\n",
      "Training loss:1.7527717351913452\n",
      "Training loss:1.7896068096160889\n",
      "Training loss:1.7428100109100342\n",
      "Training loss:1.8219844102859497\n",
      "Training loss:1.7517622709274292\n",
      "Training loss:1.7487646341323853\n",
      "Training loss:1.827540397644043\n",
      "Training loss:1.8121414184570312\n",
      "Training loss:1.7816709280014038\n",
      "Training loss:1.7705684900283813\n",
      "Training loss:1.785187005996704\n",
      "Training loss:1.8047447204589844\n",
      "Training loss:1.7650057077407837\n",
      "Training loss:1.7617177963256836\n",
      "Training loss:1.8552329540252686\n",
      "Training loss:1.7525694370269775\n",
      "Training loss:1.792209506034851\n",
      "Training loss:1.8595958948135376\n",
      "Training loss:1.7918765544891357\n",
      "Training loss:1.8760510683059692\n",
      "Training loss:1.7358585596084595\n",
      "Training loss:1.7821080684661865\n",
      "Training loss:1.8599083423614502\n",
      "Training loss:1.7369673252105713\n",
      "Training loss:1.7839049100875854\n",
      "Training loss:1.8030047416687012\n",
      "Training loss:1.698512315750122\n",
      "Training loss:1.764461636543274\n",
      "Training loss:1.8234283924102783\n",
      "Training loss:1.7517024278640747\n",
      "Training loss:1.7108099460601807\n",
      "Training loss:1.756489634513855\n",
      "Training loss:1.787832498550415\n",
      "Training loss:1.8472788333892822\n",
      "Training loss:1.7200902700424194\n",
      "Training loss:1.7028261423110962\n",
      "Training loss:1.722610592842102\n",
      "Training loss:1.7235174179077148\n",
      "Training loss:1.6923621892929077\n",
      "Training loss:1.756085991859436\n",
      "Training loss:1.7223871946334839\n",
      "Training loss:1.7961229085922241\n",
      "Training loss:1.7002239227294922\n",
      "Training loss:1.7205153703689575\n",
      "Training loss:1.7495406866073608\n",
      "Training loss:1.7716143131256104\n",
      "Training loss:1.701022744178772\n",
      "Training loss:1.7394007444381714\n",
      "Training loss:1.7767977714538574\n",
      "Training loss:1.708870768547058\n",
      "Training loss:1.743021845817566\n",
      "Training loss:1.8058629035949707\n",
      "Training loss:1.7585352659225464\n",
      "Training loss:1.7646558284759521\n",
      "Training loss:1.8136450052261353\n",
      "Training loss:1.7226225137710571\n",
      "Training loss:1.7781327962875366\n",
      "Epoch: 0019 loss_train: 166.511276 acc_train: 0.339521 loss_val: 21.031260 acc_val: 0.346667 time: 45571.501646s\n",
      "Training loss:1.7710530757904053\n",
      "Training loss:1.8542098999023438\n",
      "Training loss:1.7916617393493652\n",
      "Training loss:1.7890596389770508\n",
      "Training loss:1.7599228620529175\n",
      "Training loss:1.788469672203064\n",
      "Training loss:1.7980526685714722\n",
      "Training loss:1.7532833814620972\n",
      "Training loss:1.7739158868789673\n",
      "Training loss:1.7688508033752441\n",
      "Training loss:1.7817866802215576\n",
      "Training loss:1.8053447008132935\n",
      "Training loss:1.7874397039413452\n",
      "Training loss:1.8100998401641846\n",
      "Training loss:1.7494267225265503\n",
      "Training loss:1.7913146018981934\n",
      "Training loss:1.8271074295043945\n",
      "Training loss:1.7751059532165527\n",
      "Training loss:1.7762775421142578\n",
      "Training loss:1.7844418287277222\n",
      "Training loss:1.7999191284179688\n",
      "Training loss:1.7923568487167358\n",
      "Training loss:1.793613314628601\n",
      "Training loss:1.7902133464813232\n",
      "Training loss:1.7598226070404053\n",
      "Training loss:1.7731479406356812\n",
      "Training loss:1.781020164489746\n",
      "Training loss:1.7076770067214966\n",
      "Training loss:1.780372977256775\n",
      "Training loss:1.7149964570999146\n",
      "Training loss:1.7247707843780518\n",
      "Training loss:1.7150486707687378\n",
      "Training loss:1.7962801456451416\n",
      "Training loss:1.8205499649047852\n",
      "Training loss:1.841237187385559\n",
      "Training loss:1.7771662473678589\n",
      "Training loss:1.7747888565063477\n",
      "Training loss:1.7171224355697632\n",
      "Training loss:1.736969232559204\n",
      "Training loss:1.7987396717071533\n",
      "Training loss:1.807505488395691\n",
      "Training loss:1.7479575872421265\n",
      "Training loss:1.7522789239883423\n",
      "Training loss:1.7262182235717773\n",
      "Training loss:1.7001135349273682\n",
      "Training loss:1.829402208328247\n",
      "Training loss:1.7192161083221436\n",
      "Training loss:1.794619083404541\n",
      "Training loss:1.7721850872039795\n",
      "Training loss:1.66655433177948\n",
      "Training loss:1.781861662864685\n",
      "Training loss:1.7772173881530762\n",
      "Training loss:1.7905179262161255\n",
      "Training loss:1.8086131811141968\n",
      "Training loss:1.79831063747406\n",
      "Training loss:1.7495322227478027\n",
      "Training loss:1.6806026697158813\n",
      "Training loss:1.7190051078796387\n",
      "Training loss:1.7959532737731934\n",
      "Training loss:1.776052474975586\n",
      "Training loss:1.7705790996551514\n",
      "Training loss:1.7733150720596313\n",
      "Training loss:1.7503540515899658\n",
      "Training loss:1.7446696758270264\n",
      "Training loss:1.7843838930130005\n",
      "Training loss:1.7839065790176392\n",
      "Training loss:1.757827639579773\n",
      "Training loss:1.7606236934661865\n",
      "Training loss:1.7586300373077393\n",
      "Training loss:1.7514740228652954\n",
      "Training loss:1.7550917863845825\n",
      "Training loss:1.8492740392684937\n",
      "Training loss:1.7412081956863403\n",
      "Training loss:1.7774930000305176\n",
      "Training loss:1.7437467575073242\n",
      "Training loss:1.77558171749115\n",
      "Training loss:1.8153964281082153\n",
      "Training loss:1.7443134784698486\n",
      "Training loss:1.7814816236495972\n",
      "Training loss:1.739187240600586\n",
      "Training loss:1.8056195974349976\n",
      "Training loss:1.762704610824585\n",
      "Training loss:1.7521413564682007\n",
      "Training loss:1.7736207246780396\n",
      "Training loss:1.7863081693649292\n",
      "Training loss:1.7427740097045898\n",
      "Training loss:1.7419713735580444\n",
      "Training loss:1.729332447052002\n",
      "Training loss:1.7509421110153198\n",
      "Training loss:1.7597521543502808\n",
      "Training loss:1.7701902389526367\n",
      "Training loss:1.7805633544921875\n",
      "Training loss:1.7404811382293701\n",
      "Training loss:1.8095523118972778\n",
      "Epoch: 0020 loss_train: 166.390846 acc_train: 0.338479 loss_val: 20.953115 acc_val: 0.348000 time: 48005.010261s\n",
      "Training loss:1.7172094583511353\n",
      "Training loss:1.809301733970642\n",
      "Training loss:1.7132388353347778\n",
      "Training loss:1.8405591249465942\n",
      "Training loss:1.7629578113555908\n",
      "Training loss:1.8655176162719727\n",
      "Training loss:1.7663822174072266\n",
      "Training loss:1.768153429031372\n",
      "Training loss:1.6704221963882446\n",
      "Training loss:1.7638555765151978\n",
      "Training loss:1.721124529838562\n",
      "Training loss:1.753775954246521\n",
      "Training loss:1.697277307510376\n",
      "Training loss:1.8512972593307495\n",
      "Training loss:1.8413347005844116\n",
      "Training loss:1.7963624000549316\n",
      "Training loss:1.7401962280273438\n",
      "Training loss:1.7146037817001343\n",
      "Training loss:1.7529836893081665\n",
      "Training loss:1.7840018272399902\n",
      "Training loss:1.7343292236328125\n",
      "Training loss:1.7874946594238281\n",
      "Training loss:1.6829559803009033\n",
      "Training loss:1.7458598613739014\n",
      "Training loss:1.7632707357406616\n",
      "Training loss:1.704927682876587\n",
      "Training loss:1.7672879695892334\n",
      "Training loss:1.8186066150665283\n",
      "Training loss:1.799948811531067\n",
      "Training loss:1.71620512008667\n",
      "Training loss:1.765631914138794\n",
      "Training loss:1.7285529375076294\n",
      "Training loss:1.744027853012085\n",
      "Training loss:1.837262749671936\n",
      "Training loss:1.6810357570648193\n",
      "Training loss:1.7838207483291626\n",
      "Training loss:1.7300362586975098\n",
      "Training loss:1.7438617944717407\n",
      "Training loss:1.7552331686019897\n",
      "Training loss:1.7581889629364014\n",
      "Training loss:1.7076013088226318\n",
      "Training loss:1.803611159324646\n",
      "Training loss:1.776960849761963\n",
      "Training loss:1.7427634000778198\n",
      "Training loss:1.7665027379989624\n",
      "Training loss:1.7171294689178467\n",
      "Training loss:1.795623540878296\n",
      "Training loss:1.734494686126709\n",
      "Training loss:1.7610557079315186\n",
      "Training loss:1.7141042947769165\n",
      "Training loss:1.7599555253982544\n",
      "Training loss:1.9207795858383179\n",
      "Training loss:1.8473957777023315\n",
      "Training loss:1.801050066947937\n",
      "Training loss:1.7346711158752441\n",
      "Training loss:1.8075180053710938\n",
      "Training loss:1.7869404554367065\n",
      "Training loss:1.739647388458252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.7547719478607178\n",
      "Training loss:1.6584343910217285\n",
      "Training loss:1.7297419309616089\n",
      "Training loss:1.7761156558990479\n",
      "Training loss:1.7928637266159058\n",
      "Training loss:1.760175347328186\n",
      "Training loss:1.6568952798843384\n",
      "Training loss:1.8164796829223633\n",
      "Training loss:1.7659319639205933\n",
      "Training loss:1.7928844690322876\n",
      "Training loss:1.7457550764083862\n",
      "Training loss:1.7783054113388062\n",
      "Training loss:1.8181674480438232\n",
      "Training loss:1.7230033874511719\n",
      "Training loss:1.7766951322555542\n",
      "Training loss:1.740916132926941\n",
      "Training loss:1.866288661956787\n",
      "Training loss:1.8248835802078247\n",
      "Training loss:1.750759482383728\n",
      "Training loss:1.7063285112380981\n",
      "Training loss:1.7650902271270752\n",
      "Training loss:1.8054871559143066\n",
      "Training loss:1.6865178346633911\n",
      "Training loss:1.7621042728424072\n",
      "Training loss:1.7928684949874878\n",
      "Training loss:1.7526824474334717\n",
      "Training loss:1.7675689458847046\n",
      "Training loss:1.8498159646987915\n",
      "Training loss:1.7904607057571411\n",
      "Training loss:1.8026249408721924\n",
      "Training loss:1.674859642982483\n",
      "Training loss:1.8359705209732056\n",
      "Training loss:1.7503694295883179\n",
      "Training loss:1.7475122213363647\n",
      "Training loss:1.7485157251358032\n",
      "Training loss:1.744561791419983\n",
      "Epoch: 0021 loss_train: 165.809307 acc_train: 0.340042 loss_val: 20.953621 acc_val: 0.353833 time: 50455.885939s\n",
      "Training loss:1.7566717863082886\n",
      "Training loss:1.7080249786376953\n",
      "Training loss:1.7660719156265259\n",
      "Training loss:1.7673763036727905\n",
      "Training loss:1.78213369846344\n",
      "Training loss:1.793229579925537\n",
      "Training loss:1.803040623664856\n",
      "Training loss:1.7996153831481934\n",
      "Training loss:1.7369608879089355\n",
      "Training loss:1.7207638025283813\n",
      "Training loss:1.6809369325637817\n",
      "Training loss:1.800248384475708\n",
      "Training loss:1.7246134281158447\n",
      "Training loss:1.6927907466888428\n",
      "Training loss:1.7503851652145386\n",
      "Training loss:1.6747517585754395\n",
      "Training loss:1.7228727340698242\n",
      "Training loss:1.7417677640914917\n",
      "Training loss:1.7346150875091553\n",
      "Training loss:1.857197642326355\n",
      "Training loss:1.735960841178894\n",
      "Training loss:1.8133426904678345\n",
      "Training loss:1.6739994287490845\n",
      "Training loss:1.774275779724121\n",
      "Training loss:1.710866093635559\n",
      "Training loss:1.796454906463623\n",
      "Training loss:1.7461525201797485\n",
      "Training loss:1.7376230955123901\n",
      "Training loss:1.7360988855361938\n",
      "Training loss:1.7515875101089478\n",
      "Training loss:1.7728428840637207\n",
      "Training loss:1.759138822555542\n",
      "Training loss:1.7932050228118896\n",
      "Training loss:1.7556469440460205\n",
      "Training loss:1.6911256313323975\n",
      "Training loss:1.7683594226837158\n",
      "Training loss:1.7671356201171875\n",
      "Training loss:1.7731374502182007\n",
      "Training loss:1.7956359386444092\n",
      "Training loss:1.770532488822937\n",
      "Training loss:1.7497813701629639\n",
      "Training loss:1.7817060947418213\n",
      "Training loss:1.8217079639434814\n",
      "Training loss:1.7574470043182373\n",
      "Training loss:1.7757089138031006\n",
      "Training loss:1.7618573904037476\n",
      "Training loss:1.7534897327423096\n",
      "Training loss:1.7564456462860107\n",
      "Training loss:1.717150092124939\n",
      "Training loss:1.775179147720337\n",
      "Training loss:1.7191704511642456\n",
      "Training loss:1.742533802986145\n",
      "Training loss:1.750657081604004\n",
      "Training loss:1.7362728118896484\n",
      "Training loss:1.7847857475280762\n",
      "Training loss:1.7579766511917114\n",
      "Training loss:1.7219082117080688\n",
      "Training loss:1.714950442314148\n",
      "Training loss:1.7076772451400757\n",
      "Training loss:1.7992589473724365\n",
      "Training loss:1.8370389938354492\n",
      "Training loss:1.7816674709320068\n",
      "Training loss:1.7049914598464966\n",
      "Training loss:1.7963861227035522\n",
      "Training loss:1.759303092956543\n",
      "Training loss:1.7076267004013062\n",
      "Training loss:1.7958712577819824\n",
      "Training loss:1.7312346696853638\n",
      "Training loss:1.7732611894607544\n",
      "Training loss:1.8326449394226074\n",
      "Training loss:1.7483199834823608\n",
      "Training loss:1.7595006227493286\n",
      "Training loss:1.7568998336791992\n",
      "Training loss:1.78506338596344\n",
      "Training loss:1.7902560234069824\n",
      "Training loss:1.87800133228302\n",
      "Training loss:1.7952519655227661\n",
      "Training loss:1.7843143939971924\n",
      "Training loss:1.8217790126800537\n",
      "Training loss:1.7213138341903687\n",
      "Training loss:1.8185209035873413\n",
      "Training loss:1.7602406740188599\n",
      "Training loss:1.7523236274719238\n",
      "Training loss:1.7631970643997192\n",
      "Training loss:1.803396463394165\n",
      "Training loss:1.757446527481079\n",
      "Training loss:1.8011800050735474\n",
      "Training loss:1.823024868965149\n",
      "Training loss:1.8116564750671387\n",
      "Training loss:1.8357311487197876\n",
      "Training loss:1.7457671165466309\n",
      "Training loss:1.819836139678955\n",
      "Training loss:1.7501763105392456\n",
      "Training loss:1.706534504890442\n",
      "Epoch: 0022 loss_train: 165.732613 acc_train: 0.341896 loss_val: 21.827979 acc_val: 0.327833 time: 52926.212104s\n",
      "Training loss:1.8112637996673584\n",
      "Training loss:1.7623172998428345\n",
      "Training loss:1.8184810876846313\n",
      "Training loss:1.7293998003005981\n",
      "Training loss:1.7915440797805786\n",
      "Training loss:1.7439453601837158\n",
      "Training loss:1.7876129150390625\n",
      "Training loss:1.8107450008392334\n",
      "Training loss:1.8651323318481445\n",
      "Training loss:1.840848684310913\n",
      "Training loss:1.7840557098388672\n",
      "Training loss:1.7521785497665405\n",
      "Training loss:1.7191210985183716\n",
      "Training loss:1.761393666267395\n",
      "Training loss:1.7680429220199585\n",
      "Training loss:1.7124086618423462\n",
      "Training loss:1.796958565711975\n",
      "Training loss:1.7246607542037964\n",
      "Training loss:1.7395251989364624\n",
      "Training loss:1.7179120779037476\n",
      "Training loss:1.8265389204025269\n",
      "Training loss:1.7518795728683472\n",
      "Training loss:1.8025096654891968\n",
      "Training loss:1.7793633937835693\n",
      "Training loss:1.7446808815002441\n",
      "Training loss:1.7402981519699097\n",
      "Training loss:1.7743442058563232\n",
      "Training loss:1.7875487804412842\n",
      "Training loss:1.758402705192566\n",
      "Training loss:1.77305269241333\n",
      "Training loss:1.8100736141204834\n",
      "Training loss:1.6960214376449585\n",
      "Training loss:1.7844254970550537\n",
      "Training loss:1.7667944431304932\n",
      "Training loss:1.7188313007354736\n",
      "Training loss:1.8015905618667603\n",
      "Training loss:1.7621089220046997\n",
      "Training loss:1.7328710556030273\n",
      "Training loss:1.7121657133102417\n",
      "Training loss:1.7836450338363647\n",
      "Training loss:1.7433180809020996\n",
      "Training loss:1.777547836303711\n",
      "Training loss:1.7353066205978394\n",
      "Training loss:1.7594382762908936\n",
      "Training loss:1.735819697380066\n",
      "Training loss:1.7339115142822266\n",
      "Training loss:1.778678297996521\n",
      "Training loss:1.7800829410552979\n",
      "Training loss:1.7747855186462402\n",
      "Training loss:1.7183423042297363\n",
      "Training loss:1.8334825038909912\n",
      "Training loss:1.6979831457138062\n",
      "Training loss:1.6254388093948364\n",
      "Training loss:1.7648593187332153\n",
      "Training loss:1.732522964477539\n",
      "Training loss:1.7451316118240356\n",
      "Training loss:1.7315844297409058\n",
      "Training loss:1.783060073852539\n",
      "Training loss:1.6840829849243164\n",
      "Training loss:1.8264329433441162\n",
      "Training loss:1.742109775543213\n",
      "Training loss:1.7798835039138794\n",
      "Training loss:1.7474632263183594\n",
      "Training loss:1.86086106300354\n",
      "Training loss:1.7435508966445923\n",
      "Training loss:1.8183594942092896\n",
      "Training loss:1.7996578216552734\n",
      "Training loss:1.7251404523849487\n",
      "Training loss:1.8411107063293457\n",
      "Training loss:1.8447626829147339\n",
      "Training loss:1.7586934566497803\n",
      "Training loss:1.7659149169921875\n",
      "Training loss:1.7869911193847656\n",
      "Training loss:1.8507126569747925\n",
      "Training loss:1.7597503662109375\n",
      "Training loss:1.830302357673645\n",
      "Training loss:1.825523853302002\n",
      "Training loss:1.7287266254425049\n",
      "Training loss:1.7498716115951538\n",
      "Training loss:1.7997288703918457\n",
      "Training loss:1.741673231124878\n",
      "Training loss:1.7953840494155884\n",
      "Training loss:1.7432632446289062\n",
      "Training loss:1.629610538482666\n",
      "Training loss:1.8152223825454712\n",
      "Training loss:1.8076891899108887\n",
      "Training loss:1.7546569108963013\n",
      "Training loss:1.7772358655929565\n",
      "Training loss:1.8116780519485474\n",
      "Training loss:1.7846083641052246\n",
      "Training loss:1.8164581060409546\n",
      "Training loss:1.795745849609375\n",
      "Training loss:1.832032561302185\n",
      "Training loss:1.8070889711380005\n",
      "Epoch: 0023 loss_train: 166.377967 acc_train: 0.338271 loss_val: 20.884470 acc_val: 0.348333 time: 55401.656210s\n",
      "Training loss:1.758252739906311\n",
      "Training loss:1.7853646278381348\n",
      "Training loss:1.7657887935638428\n",
      "Training loss:1.757691740989685\n",
      "Training loss:1.7565340995788574\n",
      "Training loss:1.7450370788574219\n",
      "Training loss:1.7079726457595825\n",
      "Training loss:1.7771497964859009\n",
      "Training loss:1.70556640625\n",
      "Training loss:1.772552728652954\n",
      "Training loss:1.69893479347229\n",
      "Training loss:1.7633002996444702\n",
      "Training loss:1.8376743793487549\n",
      "Training loss:1.7564541101455688\n",
      "Training loss:1.8017528057098389\n",
      "Training loss:1.7709702253341675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.8274024724960327\n",
      "Training loss:1.8062533140182495\n",
      "Training loss:1.788286566734314\n",
      "Training loss:1.733967661857605\n",
      "Training loss:1.8587315082550049\n",
      "Training loss:1.7779113054275513\n",
      "Training loss:1.7863337993621826\n",
      "Training loss:1.7749065160751343\n",
      "Training loss:1.761639952659607\n",
      "Training loss:1.8126461505889893\n",
      "Training loss:1.7665537595748901\n",
      "Training loss:1.7783691883087158\n",
      "Training loss:1.8087153434753418\n",
      "Training loss:1.7694345712661743\n",
      "Training loss:1.7497886419296265\n",
      "Training loss:1.7176512479782104\n",
      "Training loss:1.7484009265899658\n",
      "Training loss:1.8078510761260986\n",
      "Training loss:1.7204885482788086\n",
      "Training loss:1.754773736000061\n",
      "Training loss:1.7234880924224854\n",
      "Training loss:1.7479264736175537\n",
      "Training loss:1.740417718887329\n",
      "Training loss:1.709382176399231\n",
      "Training loss:1.7623307704925537\n",
      "Training loss:1.7929776906967163\n",
      "Training loss:1.817383050918579\n",
      "Training loss:1.7136404514312744\n",
      "Training loss:1.6622071266174316\n",
      "Training loss:1.7119200229644775\n",
      "Training loss:1.7737994194030762\n",
      "Training loss:1.8001148700714111\n",
      "Training loss:1.7630380392074585\n",
      "Training loss:1.8162647485733032\n",
      "Training loss:1.7730077505111694\n",
      "Training loss:1.7477343082427979\n",
      "Training loss:1.7806665897369385\n",
      "Training loss:1.783895492553711\n",
      "Training loss:1.7548764944076538\n",
      "Training loss:1.7506922483444214\n",
      "Training loss:1.7537713050842285\n",
      "Training loss:1.7806692123413086\n",
      "Training loss:1.7183998823165894\n",
      "Training loss:1.790658712387085\n",
      "Training loss:1.7432531118392944\n",
      "Training loss:1.7635066509246826\n",
      "Training loss:1.7899181842803955\n",
      "Training loss:1.707433819770813\n",
      "Training loss:1.7340190410614014\n",
      "Training loss:1.7868037223815918\n",
      "Training loss:1.7694237232208252\n",
      "Training loss:1.7045553922653198\n",
      "Training loss:1.748509168624878\n",
      "Training loss:1.6919931173324585\n",
      "Training loss:1.8000181913375854\n",
      "Training loss:1.7277709245681763\n",
      "Training loss:1.7427321672439575\n",
      "Training loss:1.7872962951660156\n",
      "Training loss:1.6987019777297974\n",
      "Training loss:1.7531615495681763\n",
      "Training loss:1.8066872358322144\n",
      "Training loss:1.7379939556121826\n",
      "Training loss:1.7770963907241821\n",
      "Training loss:1.806653618812561\n",
      "Training loss:1.7533682584762573\n",
      "Training loss:1.7708262205123901\n",
      "Training loss:1.7601978778839111\n",
      "Training loss:1.744983196258545\n",
      "Training loss:1.739368200302124\n",
      "Training loss:1.804219126701355\n",
      "Training loss:1.6956212520599365\n",
      "Training loss:1.7737230062484741\n",
      "Training loss:1.7981311082839966\n",
      "Training loss:1.737743616104126\n",
      "Training loss:1.7561612129211426\n",
      "Training loss:1.7617031335830688\n",
      "Training loss:1.8296047449111938\n",
      "Training loss:1.723429799079895\n",
      "Epoch: 0024 loss_train: 165.606975 acc_train: 0.340562 loss_val: 21.000982 acc_val: 0.351167 time: 57880.129667s\n",
      "Training loss:1.7224597930908203\n",
      "Training loss:1.8289018869400024\n",
      "Training loss:1.7206085920333862\n",
      "Training loss:1.839860200881958\n",
      "Training loss:1.718779444694519\n",
      "Training loss:1.7188538312911987\n",
      "Training loss:1.851140022277832\n",
      "Training loss:1.7865647077560425\n",
      "Training loss:1.6899454593658447\n",
      "Training loss:1.7577636241912842\n",
      "Training loss:1.7374929189682007\n",
      "Training loss:1.8535701036453247\n",
      "Training loss:1.8417258262634277\n",
      "Training loss:1.7522202730178833\n",
      "Training loss:1.7308382987976074\n",
      "Training loss:1.7301034927368164\n",
      "Training loss:1.7491611242294312\n",
      "Training loss:1.7941038608551025\n",
      "Training loss:1.7414040565490723\n",
      "Training loss:1.6847137212753296\n",
      "Training loss:1.7907952070236206\n",
      "Training loss:1.7518938779830933\n",
      "Training loss:1.747663974761963\n",
      "Training loss:1.8145252466201782\n",
      "Training loss:1.769963026046753\n",
      "Training loss:1.7152373790740967\n",
      "Training loss:1.8130476474761963\n",
      "Training loss:1.8225667476654053\n",
      "Training loss:1.683464765548706\n",
      "Training loss:1.771411657333374\n",
      "Training loss:1.714450478553772\n",
      "Training loss:1.7545920610427856\n",
      "Training loss:1.805458903312683\n",
      "Training loss:1.7784432172775269\n",
      "Training loss:1.769473910331726\n",
      "Training loss:1.7721140384674072\n",
      "Training loss:1.7972363233566284\n",
      "Training loss:1.71543550491333\n",
      "Training loss:1.7897729873657227\n",
      "Training loss:1.7615981101989746\n",
      "Training loss:1.7457058429718018\n",
      "Training loss:1.7009986639022827\n",
      "Training loss:1.7880468368530273\n",
      "Training loss:1.7495430707931519\n",
      "Training loss:1.6591613292694092\n",
      "Training loss:1.748125433921814\n",
      "Training loss:1.8269290924072266\n",
      "Training loss:1.7759768962860107\n",
      "Training loss:1.6778721809387207\n",
      "Training loss:1.7477893829345703\n",
      "Training loss:1.7296905517578125\n",
      "Training loss:1.7063881158828735\n",
      "Training loss:1.716978907585144\n",
      "Training loss:1.7565606832504272\n",
      "Training loss:1.728774070739746\n",
      "Training loss:1.7643864154815674\n",
      "Training loss:1.778666615486145\n",
      "Training loss:1.741556167602539\n",
      "Training loss:1.8212254047393799\n",
      "Training loss:1.768301248550415\n",
      "Training loss:1.7033108472824097\n",
      "Training loss:1.7947449684143066\n",
      "Training loss:1.798624038696289\n",
      "Training loss:1.731770634651184\n",
      "Training loss:1.7855381965637207\n",
      "Training loss:1.759291410446167\n",
      "Training loss:1.773910403251648\n",
      "Training loss:1.7877947092056274\n",
      "Training loss:1.714260220527649\n",
      "Training loss:1.765379786491394\n",
      "Training loss:1.8272215127944946\n",
      "Training loss:1.752718448638916\n",
      "Training loss:1.793097734451294\n",
      "Training loss:1.759955883026123\n",
      "Training loss:1.7016525268554688\n",
      "Training loss:1.719943881034851\n",
      "Training loss:1.753642201423645\n",
      "Training loss:1.768435001373291\n",
      "Training loss:1.832416296005249\n",
      "Training loss:1.7856632471084595\n",
      "Training loss:1.7632195949554443\n",
      "Training loss:1.732123851776123\n",
      "Training loss:1.7337735891342163\n",
      "Training loss:1.809587836265564\n",
      "Training loss:1.8137344121932983\n",
      "Training loss:1.815571904182434\n",
      "Training loss:1.7134634256362915\n",
      "Training loss:1.7161396741867065\n",
      "Training loss:1.8349004983901978\n",
      "Training loss:1.7315913438796997\n",
      "Training loss:1.7175768613815308\n",
      "Training loss:1.7881314754486084\n",
      "Training loss:1.7044588327407837\n",
      "Training loss:1.7466293573379517\n",
      "Epoch: 0025 loss_train: 165.452308 acc_train: 0.341604 loss_val: 20.939087 acc_val: 0.356500 time: 60362.742651s\n",
      "Training loss:1.7231720685958862\n",
      "Training loss:1.763137936592102\n",
      "Training loss:1.72311532497406\n",
      "Training loss:1.796576738357544\n",
      "Training loss:1.6843189001083374\n",
      "Training loss:1.7868255376815796\n",
      "Training loss:1.7762542963027954\n",
      "Training loss:1.7782527208328247\n",
      "Training loss:1.7530535459518433\n",
      "Training loss:1.7682689428329468\n",
      "Training loss:1.7589458227157593\n",
      "Training loss:1.667409896850586\n",
      "Training loss:1.8176432847976685\n",
      "Training loss:1.765270471572876\n",
      "Training loss:1.8234896659851074\n",
      "Training loss:1.7696179151535034\n",
      "Training loss:1.7132867574691772\n",
      "Training loss:1.7827720642089844\n",
      "Training loss:1.7443910837173462\n",
      "Training loss:1.751602292060852\n",
      "Training loss:1.7596381902694702\n",
      "Training loss:1.6890084743499756\n",
      "Training loss:1.763586163520813\n",
      "Training loss:1.7165251970291138\n",
      "Training loss:1.7499500513076782\n",
      "Training loss:1.7021262645721436\n",
      "Training loss:1.6677799224853516\n",
      "Training loss:1.8216338157653809\n",
      "Training loss:1.797623872756958\n",
      "Training loss:1.8096357583999634\n",
      "Training loss:1.719223141670227\n",
      "Training loss:1.7637419700622559\n",
      "Training loss:1.7645182609558105\n",
      "Training loss:1.777527093887329\n",
      "Training loss:1.8305566310882568\n",
      "Training loss:1.6826810836791992\n",
      "Training loss:1.8050587177276611\n",
      "Training loss:1.7822933197021484\n",
      "Training loss:1.8091884851455688\n",
      "Training loss:1.7086721658706665\n",
      "Training loss:1.8307290077209473\n",
      "Training loss:1.7600972652435303\n",
      "Training loss:1.7377293109893799\n",
      "Training loss:1.784623622894287\n",
      "Training loss:1.8051857948303223\n",
      "Training loss:1.7514357566833496\n",
      "Training loss:1.8230504989624023\n",
      "Training loss:1.7692580223083496\n",
      "Training loss:1.7275364398956299\n",
      "Training loss:1.7301874160766602\n",
      "Training loss:1.7558408975601196\n",
      "Training loss:1.707456111907959\n",
      "Training loss:1.7451844215393066\n",
      "Training loss:1.7160414457321167\n",
      "Training loss:1.7749072313308716\n",
      "Training loss:1.7482479810714722\n",
      "Training loss:1.7447572946548462\n",
      "Training loss:1.7354766130447388\n",
      "Training loss:1.7376296520233154\n",
      "Training loss:1.7292004823684692\n",
      "Training loss:1.7560443878173828\n",
      "Training loss:1.675526738166809\n",
      "Training loss:1.7286354303359985\n",
      "Training loss:1.7987332344055176\n",
      "Training loss:1.7238686084747314\n",
      "Training loss:1.752463459968567\n",
      "Training loss:1.7480549812316895\n",
      "Training loss:1.7617526054382324\n",
      "Training loss:1.7663683891296387\n",
      "Training loss:1.7612531185150146\n",
      "Training loss:1.7777800559997559\n",
      "Training loss:1.7153205871582031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.737887978553772\n",
      "Training loss:1.711265206336975\n",
      "Training loss:1.7170088291168213\n",
      "Training loss:1.672418236732483\n",
      "Training loss:1.8211982250213623\n",
      "Training loss:1.7431023120880127\n",
      "Training loss:1.76848304271698\n",
      "Training loss:1.7496365308761597\n",
      "Training loss:1.7820484638214111\n",
      "Training loss:1.743646264076233\n",
      "Training loss:1.7747807502746582\n",
      "Training loss:1.7192858457565308\n",
      "Training loss:1.7968159914016724\n",
      "Training loss:1.755525827407837\n",
      "Training loss:1.8006824254989624\n",
      "Training loss:1.7077887058258057\n",
      "Training loss:1.69538414478302\n",
      "Training loss:1.8502446413040161\n",
      "Training loss:1.698592185974121\n",
      "Training loss:1.8038897514343262\n",
      "Training loss:1.7643946409225464\n",
      "Training loss:1.8423452377319336\n",
      "Epoch: 0026 loss_train: 165.005176 acc_train: 0.344625 loss_val: 21.042383 acc_val: 0.346167 time: 62839.788179s\n",
      "Training loss:1.718482255935669\n",
      "Training loss:1.6810053586959839\n",
      "Training loss:1.668433666229248\n",
      "Training loss:1.8115735054016113\n",
      "Training loss:1.7368626594543457\n",
      "Training loss:1.7412190437316895\n",
      "Training loss:1.7571698427200317\n",
      "Training loss:1.7392566204071045\n",
      "Training loss:1.6964133977890015\n",
      "Training loss:1.706031084060669\n",
      "Training loss:1.6713075637817383\n",
      "Training loss:1.7597944736480713\n",
      "Training loss:1.747672200202942\n",
      "Training loss:1.7870774269104004\n",
      "Training loss:1.7149182558059692\n",
      "Training loss:1.8785076141357422\n",
      "Training loss:1.7327858209609985\n",
      "Training loss:1.7636473178863525\n",
      "Training loss:1.7418186664581299\n",
      "Training loss:1.8023594617843628\n",
      "Training loss:1.7152597904205322\n",
      "Training loss:1.707375168800354\n",
      "Training loss:1.825656771659851\n",
      "Training loss:1.777629017829895\n",
      "Training loss:1.7873706817626953\n",
      "Training loss:1.7536379098892212\n",
      "Training loss:1.7587988376617432\n",
      "Training loss:1.7271764278411865\n",
      "Training loss:1.7467272281646729\n",
      "Training loss:1.7890331745147705\n",
      "Training loss:1.823764443397522\n",
      "Training loss:1.7670353651046753\n",
      "Training loss:1.8035390377044678\n",
      "Training loss:1.770468831062317\n",
      "Training loss:1.7768293619155884\n",
      "Training loss:1.7195616960525513\n",
      "Training loss:1.7353646755218506\n",
      "Training loss:1.7781800031661987\n",
      "Training loss:1.7465012073516846\n",
      "Training loss:1.7226155996322632\n",
      "Training loss:1.7645716667175293\n",
      "Training loss:1.7416332960128784\n",
      "Training loss:1.7658166885375977\n",
      "Training loss:1.8388663530349731\n",
      "Training loss:1.7299604415893555\n",
      "Training loss:1.7494839429855347\n",
      "Training loss:1.7740596532821655\n",
      "Training loss:1.7238625288009644\n",
      "Training loss:1.8414288759231567\n",
      "Training loss:1.7308470010757446\n",
      "Training loss:1.8140093088150024\n",
      "Training loss:1.7227011919021606\n",
      "Training loss:1.8429144620895386\n",
      "Training loss:1.7572251558303833\n",
      "Training loss:1.702585220336914\n",
      "Training loss:1.8036221265792847\n",
      "Training loss:1.7762503623962402\n",
      "Training loss:1.6681692600250244\n",
      "Training loss:1.7155731916427612\n",
      "Training loss:1.7361841201782227\n",
      "Training loss:1.7519184350967407\n",
      "Training loss:1.7696722745895386\n",
      "Training loss:1.7801027297973633\n",
      "Training loss:1.7293263673782349\n",
      "Training loss:1.70352041721344\n",
      "Training loss:1.7265204191207886\n",
      "Training loss:1.7445499897003174\n",
      "Training loss:1.7852691411972046\n",
      "Training loss:1.770357608795166\n",
      "Training loss:1.7445671558380127\n",
      "Training loss:1.7634657621383667\n",
      "Training loss:1.7895393371582031\n",
      "Training loss:1.7513067722320557\n",
      "Training loss:1.7833393812179565\n",
      "Training loss:1.7493432760238647\n",
      "Training loss:1.780056357383728\n",
      "Training loss:1.7838956117630005\n",
      "Training loss:1.7681291103363037\n",
      "Training loss:1.8006694316864014\n",
      "Training loss:1.7937487363815308\n",
      "Training loss:1.7214950323104858\n",
      "Training loss:1.6971369981765747\n",
      "Training loss:1.7549248933792114\n",
      "Training loss:1.744215488433838\n",
      "Training loss:1.805222988128662\n",
      "Training loss:1.7710776329040527\n",
      "Training loss:1.8353766202926636\n",
      "Training loss:1.7330882549285889\n",
      "Training loss:1.7252516746520996\n",
      "Training loss:1.788465976715088\n",
      "Training loss:1.8102144002914429\n",
      "Training loss:1.7615667581558228\n",
      "Training loss:1.7547727823257446\n",
      "Training loss:1.7717500925064087\n",
      "Epoch: 0027 loss_train: 165.232484 acc_train: 0.341792 loss_val: 20.802668 acc_val: 0.353833 time: 65315.768931s\n",
      "Training loss:1.702683925628662\n",
      "Training loss:1.7441664934158325\n",
      "Training loss:1.770938515663147\n",
      "Training loss:1.7752935886383057\n",
      "Training loss:1.7419742345809937\n",
      "Training loss:1.756899356842041\n",
      "Training loss:1.7816848754882812\n",
      "Training loss:1.7213244438171387\n",
      "Training loss:1.7637944221496582\n",
      "Training loss:1.7494773864746094\n",
      "Training loss:1.7246239185333252\n",
      "Training loss:1.8217793703079224\n",
      "Training loss:1.7704442739486694\n",
      "Training loss:1.7535580396652222\n",
      "Training loss:1.7323668003082275\n",
      "Training loss:1.7708089351654053\n",
      "Training loss:1.6735548973083496\n",
      "Training loss:1.7445306777954102\n",
      "Training loss:1.7821335792541504\n",
      "Training loss:1.7838176488876343\n",
      "Training loss:1.7451233863830566\n",
      "Training loss:1.7212607860565186\n",
      "Training loss:1.7691872119903564\n",
      "Training loss:1.7813304662704468\n",
      "Training loss:1.7087135314941406\n",
      "Training loss:1.7374873161315918\n",
      "Training loss:1.7410025596618652\n",
      "Training loss:1.880202293395996\n",
      "Training loss:1.7543201446533203\n",
      "Training loss:1.8010607957839966\n",
      "Training loss:1.734406590461731\n",
      "Training loss:1.6740691661834717\n",
      "Training loss:1.732967495918274\n",
      "Training loss:1.773850679397583\n",
      "Training loss:1.7420156002044678\n",
      "Training loss:1.8297761678695679\n",
      "Training loss:1.741081714630127\n",
      "Training loss:1.7011351585388184\n",
      "Training loss:1.851952314376831\n",
      "Training loss:1.7316994667053223\n",
      "Training loss:1.8362681865692139\n",
      "Training loss:1.8002355098724365\n",
      "Training loss:1.7312687635421753\n",
      "Training loss:1.8131905794143677\n",
      "Training loss:1.779109239578247\n",
      "Training loss:1.6970564126968384\n",
      "Training loss:1.8678609132766724\n",
      "Training loss:1.716600775718689\n",
      "Training loss:1.7749773263931274\n",
      "Training loss:1.7266029119491577\n",
      "Training loss:1.798476219177246\n",
      "Training loss:1.730637550354004\n",
      "Training loss:1.7430659532546997\n",
      "Training loss:1.7960753440856934\n",
      "Training loss:1.6840327978134155\n",
      "Training loss:1.7497692108154297\n",
      "Training loss:1.7880687713623047\n",
      "Training loss:1.7316501140594482\n",
      "Training loss:1.7476575374603271\n",
      "Training loss:1.8019068241119385\n",
      "Training loss:1.8492217063903809\n",
      "Training loss:1.7737971544265747\n",
      "Training loss:1.7474502325057983\n",
      "Training loss:1.7366632223129272\n",
      "Training loss:1.7333701848983765\n",
      "Training loss:1.804694652557373\n",
      "Training loss:1.7826709747314453\n",
      "Training loss:1.7527462244033813\n",
      "Training loss:1.7856199741363525\n",
      "Training loss:1.727476954460144\n",
      "Training loss:1.802222728729248\n",
      "Training loss:1.7591934204101562\n",
      "Training loss:1.7363202571868896\n",
      "Training loss:1.7504856586456299\n",
      "Training loss:1.6918426752090454\n",
      "Training loss:1.775583028793335\n",
      "Training loss:1.729584813117981\n",
      "Training loss:1.8063104152679443\n",
      "Training loss:1.7449791431427002\n",
      "Training loss:1.7774558067321777\n",
      "Training loss:1.739397406578064\n",
      "Training loss:1.6869035959243774\n",
      "Training loss:1.7134685516357422\n",
      "Training loss:1.7261860370635986\n",
      "Training loss:1.7281522750854492\n",
      "Training loss:1.7289973497390747\n",
      "Training loss:1.7840768098831177\n",
      "Training loss:1.7572948932647705\n",
      "Training loss:1.699773907661438\n",
      "Training loss:1.7742557525634766\n",
      "Training loss:1.6828181743621826\n",
      "Training loss:1.7425504922866821\n",
      "Training loss:1.7540620565414429\n",
      "Training loss:1.7296110391616821\n",
      "Epoch: 0028 loss_train: 165.048247 acc_train: 0.343896 loss_val: 20.817954 acc_val: 0.356667 time: 67790.649022s\n",
      "Training loss:1.792244553565979\n",
      "Training loss:1.7145216464996338\n",
      "Training loss:1.7719014883041382\n",
      "Training loss:1.7152447700500488\n",
      "Training loss:1.7852592468261719\n",
      "Training loss:1.7577654123306274\n",
      "Training loss:1.6441373825073242\n",
      "Training loss:1.7645224332809448\n",
      "Training loss:1.7332077026367188\n",
      "Training loss:1.7843505144119263\n",
      "Training loss:1.7076566219329834\n",
      "Training loss:1.710530161857605\n",
      "Training loss:1.734599232673645\n",
      "Training loss:1.7877464294433594\n",
      "Training loss:1.6820749044418335\n",
      "Training loss:1.714961051940918\n",
      "Training loss:1.734086513519287\n",
      "Training loss:1.7371541261672974\n",
      "Training loss:1.7893495559692383\n",
      "Training loss:1.7363213300704956\n",
      "Training loss:1.7288544178009033\n",
      "Training loss:1.7679064273834229\n",
      "Training loss:1.7380239963531494\n",
      "Training loss:1.7576632499694824\n",
      "Training loss:1.687541127204895\n",
      "Training loss:1.7549577951431274\n",
      "Training loss:1.689608097076416\n",
      "Training loss:1.843319058418274\n",
      "Training loss:1.752562165260315\n",
      "Training loss:1.7721138000488281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.8065208196640015\n",
      "Training loss:1.700025200843811\n",
      "Training loss:1.77414071559906\n",
      "Training loss:1.7807097434997559\n",
      "Training loss:1.7221301794052124\n",
      "Training loss:1.7803266048431396\n",
      "Training loss:1.6347640752792358\n",
      "Training loss:1.8890866041183472\n",
      "Training loss:1.8162378072738647\n",
      "Training loss:1.7429697513580322\n",
      "Training loss:1.7268673181533813\n",
      "Training loss:1.7624326944351196\n",
      "Training loss:1.7469391822814941\n",
      "Training loss:1.7947522401809692\n",
      "Training loss:1.7515435218811035\n",
      "Training loss:1.7015353441238403\n",
      "Training loss:1.6951005458831787\n",
      "Training loss:1.7425827980041504\n",
      "Training loss:1.7528434991836548\n",
      "Training loss:1.8126219511032104\n",
      "Training loss:1.7108172178268433\n",
      "Training loss:1.809838891029358\n",
      "Training loss:1.715614676475525\n",
      "Training loss:1.7209570407867432\n",
      "Training loss:1.7458444833755493\n",
      "Training loss:1.7832553386688232\n",
      "Training loss:1.7912969589233398\n",
      "Training loss:1.704971432685852\n",
      "Training loss:1.8313343524932861\n",
      "Training loss:1.736480474472046\n",
      "Training loss:1.7068427801132202\n",
      "Training loss:1.7898920774459839\n",
      "Training loss:1.83927583694458\n",
      "Training loss:1.7674542665481567\n",
      "Training loss:1.7351001501083374\n",
      "Training loss:1.722354769706726\n",
      "Training loss:1.7779552936553955\n",
      "Training loss:1.706256628036499\n",
      "Training loss:1.7833787202835083\n",
      "Training loss:1.7276743650436401\n",
      "Training loss:1.7108052968978882\n",
      "Training loss:1.6804980039596558\n",
      "Training loss:1.8279540538787842\n",
      "Training loss:1.8100413084030151\n",
      "Training loss:1.7040932178497314\n",
      "Training loss:1.7276848554611206\n",
      "Training loss:1.7346093654632568\n",
      "Training loss:1.7500666379928589\n",
      "Training loss:1.7373634576797485\n",
      "Training loss:1.7912458181381226\n",
      "Training loss:1.7756316661834717\n",
      "Training loss:1.6736395359039307\n",
      "Training loss:1.7674223184585571\n",
      "Training loss:1.750939130783081\n",
      "Training loss:1.7243369817733765\n",
      "Training loss:1.7614389657974243\n",
      "Training loss:1.6902674436569214\n",
      "Training loss:1.7104191780090332\n",
      "Training loss:1.695821762084961\n",
      "Training loss:1.7349674701690674\n",
      "Training loss:1.8254013061523438\n",
      "Training loss:1.7639226913452148\n",
      "Training loss:1.7876691818237305\n",
      "Training loss:1.732426643371582\n",
      "Epoch: 0029 loss_train: 164.399578 acc_train: 0.346708 loss_val: 21.040625 acc_val: 0.346167 time: 70319.647050s\n",
      "Training loss:1.727732539176941\n",
      "Training loss:1.7633190155029297\n",
      "Training loss:1.728609561920166\n",
      "Training loss:1.7296046018600464\n",
      "Training loss:1.7707493305206299\n",
      "Training loss:1.8321410417556763\n",
      "Training loss:1.756816029548645\n",
      "Training loss:1.8574033975601196\n",
      "Training loss:1.726017713546753\n",
      "Training loss:1.730780839920044\n",
      "Training loss:1.7993417978286743\n",
      "Training loss:1.760527491569519\n",
      "Training loss:1.739001989364624\n",
      "Training loss:1.762974500656128\n",
      "Training loss:1.790826678276062\n",
      "Training loss:1.7896761894226074\n",
      "Training loss:1.6544467210769653\n",
      "Training loss:1.7831836938858032\n",
      "Training loss:1.7590572834014893\n",
      "Training loss:1.8060110807418823\n",
      "Training loss:1.7794560194015503\n",
      "Training loss:1.771090030670166\n",
      "Training loss:1.6959060430526733\n",
      "Training loss:1.702541708946228\n",
      "Training loss:1.7838828563690186\n",
      "Training loss:1.7688145637512207\n",
      "Training loss:1.6826077699661255\n",
      "Training loss:1.795088529586792\n",
      "Training loss:1.807459831237793\n",
      "Training loss:1.7472468614578247\n",
      "Training loss:1.763478398323059\n",
      "Training loss:1.7506588697433472\n",
      "Training loss:1.7430185079574585\n",
      "Training loss:1.7350953817367554\n",
      "Training loss:1.7701126337051392\n",
      "Training loss:1.816825032234192\n",
      "Training loss:1.7074134349822998\n",
      "Training loss:1.7055751085281372\n",
      "Training loss:1.706681728363037\n",
      "Training loss:1.7231545448303223\n",
      "Training loss:1.7492446899414062\n",
      "Training loss:1.827023983001709\n",
      "Training loss:1.8063708543777466\n",
      "Training loss:1.748339056968689\n",
      "Training loss:1.7421422004699707\n",
      "Training loss:1.7361305952072144\n",
      "Training loss:1.740633487701416\n",
      "Training loss:1.6744282245635986\n",
      "Training loss:1.7694023847579956\n",
      "Training loss:1.7459146976470947\n",
      "Training loss:1.8064359426498413\n",
      "Training loss:1.7521183490753174\n",
      "Training loss:1.6820684671401978\n",
      "Training loss:1.8096356391906738\n",
      "Training loss:1.7780256271362305\n",
      "Training loss:1.6982996463775635\n",
      "Training loss:1.809451699256897\n",
      "Training loss:1.7815054655075073\n",
      "Training loss:1.7734698057174683\n",
      "Training loss:1.820796012878418\n",
      "Training loss:1.809104323387146\n",
      "Training loss:1.6947137117385864\n",
      "Training loss:1.7551358938217163\n",
      "Training loss:1.7050411701202393\n",
      "Training loss:1.7403314113616943\n",
      "Training loss:1.7362339496612549\n",
      "Training loss:1.7585980892181396\n",
      "Training loss:1.7047197818756104\n",
      "Training loss:1.7863537073135376\n",
      "Training loss:1.769092321395874\n",
      "Training loss:1.7481147050857544\n",
      "Training loss:1.7367504835128784\n",
      "Training loss:1.7718112468719482\n",
      "Training loss:1.7277172803878784\n",
      "Training loss:1.6866778135299683\n",
      "Training loss:1.6740610599517822\n",
      "Training loss:1.753956913948059\n",
      "Training loss:1.6907048225402832\n",
      "Training loss:1.7744892835617065\n",
      "Training loss:1.7202425003051758\n",
      "Training loss:1.6857163906097412\n",
      "Training loss:1.7750625610351562\n",
      "Training loss:1.7201924324035645\n",
      "Training loss:1.7750903367996216\n",
      "Training loss:1.7725725173950195\n",
      "Training loss:1.7190051078796387\n",
      "Training loss:1.7222273349761963\n",
      "Training loss:1.7732977867126465\n",
      "Training loss:1.6916015148162842\n",
      "Training loss:1.7400909662246704\n",
      "Training loss:1.70502507686615\n",
      "Training loss:1.71112859249115\n",
      "Training loss:1.7783527374267578\n",
      "Training loss:1.7600656747817993\n",
      "Epoch: 0030 loss_train: 164.549046 acc_train: 0.345229 loss_val: 20.805059 acc_val: 0.355167 time: 72851.601426s\n",
      "Training loss:1.7873369455337524\n",
      "Training loss:1.7949951887130737\n",
      "Training loss:1.7468816041946411\n",
      "Training loss:1.7531288862228394\n",
      "Training loss:1.7370198965072632\n",
      "Training loss:1.7263387441635132\n",
      "Training loss:1.7835397720336914\n",
      "Training loss:1.7376295328140259\n",
      "Training loss:1.7062996625900269\n",
      "Training loss:1.7324950695037842\n",
      "Training loss:1.7388218641281128\n",
      "Training loss:1.7011750936508179\n",
      "Training loss:1.692016363143921\n",
      "Training loss:1.6930971145629883\n",
      "Training loss:1.7193231582641602\n",
      "Training loss:1.755123257637024\n",
      "Training loss:1.8010796308517456\n",
      "Training loss:1.7019144296646118\n",
      "Training loss:1.8080518245697021\n",
      "Training loss:1.8131839036941528\n",
      "Training loss:1.7754685878753662\n",
      "Training loss:1.7480589151382446\n",
      "Training loss:1.820408821105957\n",
      "Training loss:1.752164363861084\n",
      "Training loss:1.7558187246322632\n",
      "Training loss:1.7388468980789185\n",
      "Training loss:1.7415331602096558\n",
      "Training loss:1.7201128005981445\n",
      "Training loss:1.6746848821640015\n",
      "Training loss:1.6471161842346191\n",
      "Training loss:1.7200576066970825\n",
      "Training loss:1.7783242464065552\n",
      "Training loss:1.7305246591567993\n",
      "Training loss:1.6500208377838135\n",
      "Training loss:1.6853688955307007\n",
      "Training loss:1.7285420894622803\n",
      "Training loss:1.8035813570022583\n",
      "Training loss:1.7788476943969727\n",
      "Training loss:1.800373911857605\n",
      "Training loss:1.6875196695327759\n",
      "Training loss:1.7871872186660767\n",
      "Training loss:1.7636469602584839\n",
      "Training loss:1.7465565204620361\n",
      "Training loss:1.6632119417190552\n",
      "Training loss:1.6922329664230347\n",
      "Training loss:1.7835824489593506\n",
      "Training loss:1.7166441679000854\n",
      "Training loss:1.735958218574524\n",
      "Training loss:1.8362456560134888\n",
      "Training loss:1.8013674020767212\n",
      "Training loss:1.6988826990127563\n",
      "Training loss:1.7627642154693604\n",
      "Training loss:1.7032407522201538\n",
      "Training loss:1.7005751132965088\n",
      "Training loss:1.7805829048156738\n",
      "Training loss:1.808035135269165\n",
      "Training loss:1.7186670303344727\n",
      "Training loss:1.730656623840332\n",
      "Training loss:1.7481136322021484\n",
      "Training loss:1.8049852848052979\n",
      "Training loss:1.7215030193328857\n",
      "Training loss:1.7772094011306763\n",
      "Training loss:1.7128835916519165\n",
      "Training loss:1.754070520401001\n",
      "Training loss:1.7439650297164917\n",
      "Training loss:1.6730765104293823\n",
      "Training loss:1.7349892854690552\n",
      "Training loss:1.754878282546997\n",
      "Training loss:1.770979404449463\n",
      "Training loss:1.7221930027008057\n",
      "Training loss:1.717606544494629\n",
      "Training loss:1.8065119981765747\n",
      "Training loss:1.7773820161819458\n",
      "Training loss:1.733018159866333\n",
      "Training loss:1.6725883483886719\n",
      "Training loss:1.7343257665634155\n",
      "Training loss:1.7080191373825073\n",
      "Training loss:1.8337939977645874\n",
      "Training loss:1.7604318857192993\n",
      "Training loss:1.7181190252304077\n",
      "Training loss:1.7735140323638916\n",
      "Training loss:1.7236114740371704\n",
      "Training loss:1.7501897811889648\n",
      "Training loss:1.6952720880508423\n",
      "Training loss:1.6912347078323364\n",
      "Training loss:1.7567164897918701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.707772970199585\n",
      "Training loss:1.7588586807250977\n",
      "Training loss:1.7187248468399048\n",
      "Training loss:1.7380234003067017\n",
      "Training loss:1.7506253719329834\n",
      "Training loss:1.7182984352111816\n",
      "Training loss:1.7799559831619263\n",
      "Training loss:1.7683601379394531\n",
      "Epoch: 0031 loss_train: 163.808666 acc_train: 0.346875 loss_val: 20.703178 acc_val: 0.363000 time: 75395.976559s\n",
      "Training loss:1.707047700881958\n",
      "Training loss:1.7496757507324219\n",
      "Training loss:1.7325831651687622\n",
      "Training loss:1.7414357662200928\n",
      "Training loss:1.767301082611084\n",
      "Training loss:1.74712336063385\n",
      "Training loss:1.6788851022720337\n",
      "Training loss:1.706764817237854\n",
      "Training loss:1.7063566446304321\n",
      "Training loss:1.7787156105041504\n",
      "Training loss:1.7849655151367188\n",
      "Training loss:1.7239892482757568\n",
      "Training loss:1.7228965759277344\n",
      "Training loss:1.6355419158935547\n",
      "Training loss:1.7270666360855103\n",
      "Training loss:1.7502880096435547\n",
      "Training loss:1.793816328048706\n",
      "Training loss:1.7767325639724731\n",
      "Training loss:1.6991188526153564\n",
      "Training loss:1.7238236665725708\n",
      "Training loss:1.719827651977539\n",
      "Training loss:1.6763699054718018\n",
      "Training loss:1.8335596323013306\n",
      "Training loss:1.8094412088394165\n",
      "Training loss:1.7837023735046387\n",
      "Training loss:1.6883065700531006\n",
      "Training loss:1.7389663457870483\n",
      "Training loss:1.7232905626296997\n",
      "Training loss:1.8800095319747925\n",
      "Training loss:1.7528808116912842\n",
      "Training loss:1.827089548110962\n",
      "Training loss:1.745883822441101\n",
      "Training loss:1.7884740829467773\n",
      "Training loss:1.7598671913146973\n",
      "Training loss:1.705974817276001\n",
      "Training loss:1.6707966327667236\n",
      "Training loss:1.7939269542694092\n",
      "Training loss:1.69362473487854\n",
      "Training loss:1.758862853050232\n",
      "Training loss:1.7979904413223267\n",
      "Training loss:1.7436273097991943\n",
      "Training loss:1.7696689367294312\n",
      "Training loss:1.7246819734573364\n",
      "Training loss:1.7500333786010742\n",
      "Training loss:1.7416201829910278\n",
      "Training loss:1.7516430616378784\n",
      "Training loss:1.6744086742401123\n",
      "Training loss:1.8091931343078613\n",
      "Training loss:1.7466400861740112\n",
      "Training loss:1.7206732034683228\n",
      "Training loss:1.7100290060043335\n",
      "Training loss:1.7134320735931396\n",
      "Training loss:1.7649863958358765\n",
      "Training loss:1.7428058385849\n",
      "Training loss:1.7613517045974731\n",
      "Training loss:1.7773798704147339\n",
      "Training loss:1.725782871246338\n",
      "Training loss:1.7058558464050293\n",
      "Training loss:1.806098461151123\n",
      "Training loss:1.8498353958129883\n",
      "Training loss:1.6727617979049683\n",
      "Training loss:1.7310543060302734\n",
      "Training loss:1.746754765510559\n",
      "Training loss:1.7545380592346191\n",
      "Training loss:1.697180151939392\n",
      "Training loss:1.6970250606536865\n",
      "Training loss:1.7644314765930176\n",
      "Training loss:1.796054720878601\n",
      "Training loss:1.7670469284057617\n",
      "Training loss:1.7635416984558105\n",
      "Training loss:1.7994712591171265\n",
      "Training loss:1.8242733478546143\n",
      "Training loss:1.693597674369812\n",
      "Training loss:1.664320945739746\n",
      "Training loss:1.7819257974624634\n",
      "Training loss:1.7217336893081665\n",
      "Training loss:1.756450891494751\n",
      "Training loss:1.8005517721176147\n",
      "Training loss:1.7429810762405396\n",
      "Training loss:1.7847304344177246\n",
      "Training loss:1.7106680870056152\n",
      "Training loss:1.7145185470581055\n",
      "Training loss:1.6986817121505737\n",
      "Training loss:1.6937401294708252\n",
      "Training loss:1.760636568069458\n",
      "Training loss:1.7413966655731201\n",
      "Training loss:1.7719165086746216\n",
      "Training loss:1.778625249862671\n",
      "Training loss:1.7472115755081177\n",
      "Training loss:1.7460044622421265\n",
      "Training loss:1.678882122039795\n",
      "Training loss:1.716623306274414\n",
      "Training loss:1.765992522239685\n",
      "Training loss:1.805227279663086\n",
      "Epoch: 0032 loss_train: 164.083272 acc_train: 0.346604 loss_val: 20.726581 acc_val: 0.354333 time: 77887.576840s\n",
      "Training loss:1.7924035787582397\n",
      "Training loss:1.7979981899261475\n",
      "Training loss:1.7514369487762451\n",
      "Training loss:1.7873104810714722\n",
      "Training loss:1.6979854106903076\n",
      "Training loss:1.7525253295898438\n",
      "Training loss:1.708845615386963\n",
      "Training loss:1.7916662693023682\n",
      "Training loss:1.7107034921646118\n",
      "Training loss:1.695738673210144\n",
      "Training loss:1.6268454790115356\n",
      "Training loss:1.7049769163131714\n",
      "Training loss:1.7135095596313477\n",
      "Training loss:1.7192165851593018\n",
      "Training loss:1.730875015258789\n",
      "Training loss:1.7441102266311646\n",
      "Training loss:1.796265959739685\n",
      "Training loss:1.7782882452011108\n",
      "Training loss:1.7471919059753418\n",
      "Training loss:1.8738809823989868\n",
      "Training loss:1.7361122369766235\n",
      "Training loss:1.8392874002456665\n",
      "Training loss:1.7284362316131592\n",
      "Training loss:1.7632478475570679\n",
      "Training loss:1.7882583141326904\n",
      "Training loss:1.7523019313812256\n",
      "Training loss:1.7723749876022339\n",
      "Training loss:1.7770500183105469\n",
      "Training loss:1.7566394805908203\n",
      "Training loss:1.7708771228790283\n",
      "Training loss:1.6661298274993896\n",
      "Training loss:1.6845842599868774\n",
      "Training loss:1.7338303327560425\n",
      "Training loss:1.7713536024093628\n",
      "Training loss:1.7730844020843506\n",
      "Training loss:1.7292879819869995\n",
      "Training loss:1.6934059858322144\n",
      "Training loss:1.7348551750183105\n",
      "Training loss:1.705187201499939\n",
      "Training loss:1.695779800415039\n",
      "Training loss:1.744052529335022\n",
      "Training loss:1.723202109336853\n",
      "Training loss:1.7118128538131714\n",
      "Training loss:1.7848857641220093\n",
      "Training loss:1.718407154083252\n",
      "Training loss:1.689488172531128\n",
      "Training loss:1.7151967287063599\n",
      "Training loss:1.6987967491149902\n",
      "Training loss:1.7161915302276611\n",
      "Training loss:1.7182490825653076\n",
      "Training loss:1.7224228382110596\n",
      "Training loss:1.7073428630828857\n",
      "Training loss:1.7898486852645874\n",
      "Training loss:1.75425124168396\n",
      "Training loss:1.7770512104034424\n",
      "Training loss:1.7676656246185303\n",
      "Training loss:1.741438865661621\n",
      "Training loss:1.6981074810028076\n",
      "Training loss:1.7083767652511597\n",
      "Training loss:1.6945542097091675\n",
      "Training loss:1.7079755067825317\n",
      "Training loss:1.7912333011627197\n",
      "Training loss:1.71208918094635\n",
      "Training loss:1.7535057067871094\n",
      "Training loss:1.7578372955322266\n",
      "Training loss:1.7005524635314941\n",
      "Training loss:1.7222272157669067\n",
      "Training loss:1.7352548837661743\n",
      "Training loss:1.7330290079116821\n",
      "Training loss:1.7531285285949707\n",
      "Training loss:1.700322151184082\n",
      "Training loss:1.688065767288208\n",
      "Training loss:1.7697350978851318\n",
      "Training loss:1.6738072633743286\n",
      "Training loss:1.7785561084747314\n",
      "Training loss:1.7190697193145752\n",
      "Training loss:1.692893624305725\n",
      "Training loss:1.7665815353393555\n",
      "Training loss:1.7981187105178833\n",
      "Training loss:1.7618485689163208\n",
      "Training loss:1.7687475681304932\n",
      "Training loss:1.7147655487060547\n",
      "Training loss:1.747835397720337\n",
      "Training loss:1.7734335660934448\n",
      "Training loss:1.808588981628418\n",
      "Training loss:1.7016658782958984\n",
      "Training loss:1.6934895515441895\n",
      "Training loss:1.7294446229934692\n",
      "Training loss:1.775682806968689\n",
      "Training loss:1.69534432888031\n",
      "Training loss:1.7392187118530273\n",
      "Training loss:1.7766311168670654\n",
      "Training loss:1.7555354833602905\n",
      "Training loss:1.7439851760864258\n",
      "Epoch: 0033 loss_train: 163.515402 acc_train: 0.350479 loss_val: 20.772049 acc_val: 0.351333 time: 80440.270588s\n",
      "Training loss:1.7724523544311523\n",
      "Training loss:1.73920738697052\n",
      "Training loss:1.7204749584197998\n",
      "Training loss:1.7938629388809204\n",
      "Training loss:1.7731695175170898\n",
      "Training loss:1.690719723701477\n",
      "Training loss:1.721933364868164\n",
      "Training loss:1.6394654512405396\n",
      "Training loss:1.7165523767471313\n",
      "Training loss:1.756818175315857\n",
      "Training loss:1.7452794313430786\n",
      "Training loss:1.8048433065414429\n",
      "Training loss:1.775183081626892\n",
      "Training loss:1.6825670003890991\n",
      "Training loss:1.6600512266159058\n",
      "Training loss:1.7435681819915771\n",
      "Training loss:1.720292091369629\n",
      "Training loss:1.7539891004562378\n",
      "Training loss:1.7756223678588867\n",
      "Training loss:1.661417841911316\n",
      "Training loss:1.7357046604156494\n",
      "Training loss:1.7385002374649048\n",
      "Training loss:1.7592161893844604\n",
      "Training loss:1.7404353618621826\n",
      "Training loss:1.7263545989990234\n",
      "Training loss:1.7093194723129272\n",
      "Training loss:1.8101427555084229\n",
      "Training loss:1.7282404899597168\n",
      "Training loss:1.7121968269348145\n",
      "Training loss:1.7788368463516235\n",
      "Training loss:1.7972933053970337\n",
      "Training loss:1.7675758600234985\n",
      "Training loss:1.7180248498916626\n",
      "Training loss:1.7513266801834106\n",
      "Training loss:1.7516216039657593\n",
      "Training loss:1.7564592361450195\n",
      "Training loss:1.769762396812439\n",
      "Training loss:1.7329238653182983\n",
      "Training loss:1.8341357707977295\n",
      "Training loss:1.855085849761963\n",
      "Training loss:1.7782974243164062\n",
      "Training loss:1.7691011428833008\n",
      "Training loss:1.7126485109329224\n",
      "Training loss:1.7045507431030273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.7640239000320435\n",
      "Training loss:1.746937870979309\n",
      "Training loss:1.7980704307556152\n",
      "Training loss:1.755387544631958\n",
      "Training loss:1.6925655603408813\n",
      "Training loss:1.8490217924118042\n",
      "Training loss:1.663136601448059\n",
      "Training loss:1.747202754020691\n",
      "Training loss:1.7396234273910522\n",
      "Training loss:1.7679533958435059\n",
      "Training loss:1.754907488822937\n",
      "Training loss:1.7322804927825928\n",
      "Training loss:1.7364857196807861\n",
      "Training loss:1.688551664352417\n",
      "Training loss:1.7008329629898071\n",
      "Training loss:1.738445520401001\n",
      "Training loss:1.7392473220825195\n",
      "Training loss:1.6817487478256226\n",
      "Training loss:1.6911916732788086\n",
      "Training loss:1.7692745923995972\n",
      "Training loss:1.758351445198059\n",
      "Training loss:1.7377475500106812\n",
      "Training loss:1.7400249242782593\n",
      "Training loss:1.840698480606079\n",
      "Training loss:1.7569109201431274\n",
      "Training loss:1.75579833984375\n",
      "Training loss:1.7571637630462646\n",
      "Training loss:1.7368378639221191\n",
      "Training loss:1.84970223903656\n",
      "Training loss:1.6632921695709229\n",
      "Training loss:1.7485452890396118\n",
      "Training loss:1.7766759395599365\n",
      "Training loss:1.7603157758712769\n",
      "Training loss:1.770649790763855\n",
      "Training loss:1.7570675611495972\n",
      "Training loss:1.763902187347412\n",
      "Training loss:1.7398275136947632\n",
      "Training loss:1.7599866390228271\n",
      "Training loss:1.6795276403427124\n",
      "Training loss:1.7323256731033325\n",
      "Training loss:1.7132112979888916\n",
      "Training loss:1.7410597801208496\n",
      "Training loss:1.7425843477249146\n",
      "Training loss:1.7419990301132202\n",
      "Training loss:1.7906978130340576\n",
      "Training loss:1.736697793006897\n",
      "Training loss:1.692531943321228\n",
      "Training loss:1.7212748527526855\n",
      "Training loss:1.8347063064575195\n",
      "Training loss:1.6932597160339355\n",
      "Epoch: 0034 loss_train: 164.035489 acc_train: 0.347938 loss_val: 20.846366 acc_val: 0.353000 time: 83060.996515s\n",
      "Training loss:1.7429856061935425\n",
      "Training loss:1.7475616931915283\n",
      "Training loss:1.6646116971969604\n",
      "Training loss:1.7405883073806763\n",
      "Training loss:1.7491894960403442\n",
      "Training loss:1.8009711503982544\n",
      "Training loss:1.7591426372528076\n",
      "Training loss:1.7240636348724365\n",
      "Training loss:1.7588474750518799\n",
      "Training loss:1.7270005941390991\n",
      "Training loss:1.625300645828247\n",
      "Training loss:1.675861120223999\n",
      "Training loss:1.8129833936691284\n",
      "Training loss:1.6653093099594116\n",
      "Training loss:1.7655041217803955\n",
      "Training loss:1.722826600074768\n",
      "Training loss:1.7375822067260742\n",
      "Training loss:1.776164174079895\n",
      "Training loss:1.6731406450271606\n",
      "Training loss:1.7056317329406738\n",
      "Training loss:1.621727705001831\n",
      "Training loss:1.7726625204086304\n",
      "Training loss:1.7581572532653809\n",
      "Training loss:1.7253295183181763\n",
      "Training loss:1.713000774383545\n",
      "Training loss:1.6753257513046265\n",
      "Training loss:1.7984780073165894\n",
      "Training loss:1.7317516803741455\n",
      "Training loss:1.7093782424926758\n",
      "Training loss:1.7870166301727295\n",
      "Training loss:1.7572166919708252\n",
      "Training loss:1.7021141052246094\n",
      "Training loss:1.7660943269729614\n",
      "Training loss:1.6991263628005981\n",
      "Training loss:1.7356863021850586\n",
      "Training loss:1.8364912271499634\n",
      "Training loss:1.7373766899108887\n",
      "Training loss:1.760838508605957\n",
      "Training loss:1.7422049045562744\n",
      "Training loss:1.8266186714172363\n",
      "Training loss:1.7601975202560425\n",
      "Training loss:1.7787293195724487\n",
      "Training loss:1.7480597496032715\n",
      "Training loss:1.7707353830337524\n",
      "Training loss:1.7969868183135986\n",
      "Training loss:1.7230477333068848\n",
      "Training loss:1.8439490795135498\n",
      "Training loss:1.7464628219604492\n",
      "Training loss:1.6885063648223877\n",
      "Training loss:1.6439096927642822\n",
      "Training loss:1.8135164976119995\n",
      "Training loss:1.7386988401412964\n",
      "Training loss:1.7581629753112793\n",
      "Training loss:1.7469267845153809\n",
      "Training loss:1.7523231506347656\n",
      "Training loss:1.7797908782958984\n",
      "Training loss:1.735220193862915\n",
      "Training loss:1.7263641357421875\n",
      "Training loss:1.7238397598266602\n",
      "Training loss:1.689810872077942\n",
      "Training loss:1.7731006145477295\n",
      "Training loss:1.729259967803955\n",
      "Training loss:1.720312237739563\n",
      "Training loss:1.7273589372634888\n",
      "Training loss:1.7541359663009644\n",
      "Training loss:1.7587864398956299\n",
      "Training loss:1.7228736877441406\n",
      "Training loss:1.7142484188079834\n",
      "Training loss:1.6738836765289307\n",
      "Training loss:1.7688281536102295\n",
      "Training loss:1.7860260009765625\n",
      "Training loss:1.7064306735992432\n",
      "Training loss:1.6700565814971924\n",
      "Training loss:1.77195143699646\n",
      "Training loss:1.6986616849899292\n",
      "Training loss:1.7509454488754272\n",
      "Training loss:1.7241345643997192\n",
      "Training loss:1.7608447074890137\n",
      "Training loss:1.8108162879943848\n",
      "Training loss:1.7088170051574707\n",
      "Training loss:1.706978678703308\n",
      "Training loss:1.7307926416397095\n",
      "Training loss:1.7767128944396973\n",
      "Training loss:1.772964596748352\n",
      "Training loss:1.7004783153533936\n",
      "Training loss:1.676047444343567\n",
      "Training loss:1.721807599067688\n",
      "Training loss:1.7470908164978027\n",
      "Training loss:1.8258799314498901\n",
      "Training loss:1.7988675832748413\n",
      "Training loss:1.6992958784103394\n",
      "Training loss:1.7726500034332275\n",
      "Training loss:1.7701112031936646\n",
      "Training loss:1.7063173055648804\n",
      "Epoch: 0035 loss_train: 163.534568 acc_train: 0.350979 loss_val: 20.904490 acc_val: 0.355667 time: 85660.703203s\n",
      "Training loss:1.7408753633499146\n",
      "Training loss:1.8334264755249023\n",
      "Training loss:1.6394685506820679\n",
      "Training loss:1.7386232614517212\n",
      "Training loss:1.7451897859573364\n",
      "Training loss:1.7249687910079956\n",
      "Training loss:1.7330595254898071\n",
      "Training loss:1.7668298482894897\n",
      "Training loss:1.7500702142715454\n",
      "Training loss:1.70084547996521\n",
      "Training loss:1.72251558303833\n",
      "Training loss:1.6916711330413818\n",
      "Training loss:1.7087501287460327\n",
      "Training loss:1.7547569274902344\n",
      "Training loss:1.769917368888855\n",
      "Training loss:1.7021046876907349\n",
      "Training loss:1.6992226839065552\n",
      "Training loss:1.7187803983688354\n",
      "Training loss:1.756713628768921\n",
      "Training loss:1.7381969690322876\n",
      "Training loss:1.7062684297561646\n",
      "Training loss:1.7158550024032593\n",
      "Training loss:1.792588472366333\n",
      "Training loss:1.686571478843689\n",
      "Training loss:1.7401577234268188\n",
      "Training loss:1.7160985469818115\n",
      "Training loss:1.7324247360229492\n",
      "Training loss:1.8130218982696533\n",
      "Training loss:1.6946358680725098\n",
      "Training loss:1.7127476930618286\n",
      "Training loss:1.7398747205734253\n",
      "Training loss:1.676571011543274\n",
      "Training loss:1.7809031009674072\n",
      "Training loss:1.7601923942565918\n",
      "Training loss:1.7612299919128418\n",
      "Training loss:1.752388596534729\n",
      "Training loss:1.7228838205337524\n",
      "Training loss:1.7186613082885742\n",
      "Training loss:1.7314625978469849\n",
      "Training loss:1.753902554512024\n",
      "Training loss:1.7289048433303833\n",
      "Training loss:1.7659766674041748\n",
      "Training loss:1.78944730758667\n",
      "Training loss:1.8060005903244019\n",
      "Training loss:1.7318590879440308\n",
      "Training loss:1.6906006336212158\n",
      "Training loss:1.6899502277374268\n",
      "Training loss:1.671238899230957\n",
      "Training loss:1.6959165334701538\n",
      "Training loss:1.7114468812942505\n",
      "Training loss:1.746894121170044\n",
      "Training loss:1.7085427045822144\n",
      "Training loss:1.7142854928970337\n",
      "Training loss:1.6550519466400146\n",
      "Training loss:1.695655345916748\n",
      "Training loss:1.6787583827972412\n",
      "Training loss:1.7832504510879517\n",
      "Training loss:1.7429167032241821\n",
      "Training loss:1.7657768726348877\n",
      "Training loss:1.7119791507720947\n",
      "Training loss:1.7510122060775757\n",
      "Training loss:1.717905044555664\n",
      "Training loss:1.7544740438461304\n",
      "Training loss:1.8423563241958618\n",
      "Training loss:1.7461555004119873\n",
      "Training loss:1.834060549736023\n",
      "Training loss:1.7232989072799683\n",
      "Training loss:1.7288844585418701\n",
      "Training loss:1.8007560968399048\n",
      "Training loss:1.6716129779815674\n",
      "Training loss:1.8356006145477295\n",
      "Training loss:1.7399462461471558\n",
      "Training loss:1.729736089706421\n",
      "Training loss:1.7152918577194214\n",
      "Training loss:1.7119557857513428\n",
      "Training loss:1.7712019681930542\n",
      "Training loss:1.7033766508102417\n",
      "Training loss:1.8083264827728271\n",
      "Training loss:1.842487096786499\n",
      "Training loss:1.694167971611023\n",
      "Training loss:1.7636860609054565\n",
      "Training loss:1.6687227487564087\n",
      "Training loss:1.8401368856430054\n",
      "Training loss:1.7449822425842285\n",
      "Training loss:1.858904242515564\n",
      "Training loss:1.7876609563827515\n",
      "Training loss:1.7128932476043701\n",
      "Training loss:1.7386183738708496\n",
      "Training loss:1.7056702375411987\n",
      "Training loss:1.7197672128677368\n",
      "Training loss:1.8196841478347778\n",
      "Training loss:1.7063676118850708\n",
      "Training loss:1.7606267929077148\n",
      "Training loss:1.7845687866210938\n",
      "Epoch: 0036 loss_train: 163.563776 acc_train: 0.352104 loss_val: 20.720302 acc_val: 0.355500 time: 88214.438774s\n",
      "Training loss:1.702039122581482\n",
      "Training loss:1.7909555435180664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.7290934324264526\n",
      "Training loss:1.7221834659576416\n",
      "Training loss:1.7389901876449585\n",
      "Training loss:1.769433856010437\n",
      "Training loss:1.7309404611587524\n",
      "Training loss:1.7047895193099976\n",
      "Training loss:1.7699460983276367\n",
      "Training loss:1.6496808528900146\n",
      "Training loss:1.717644453048706\n",
      "Training loss:1.7502328157424927\n",
      "Training loss:1.7327516078948975\n",
      "Training loss:1.7632633447647095\n",
      "Training loss:1.716755986213684\n",
      "Training loss:1.8091702461242676\n",
      "Training loss:1.692363977432251\n",
      "Training loss:1.6613062620162964\n",
      "Training loss:1.7545316219329834\n",
      "Training loss:1.7790604829788208\n",
      "Training loss:1.7108969688415527\n",
      "Training loss:1.7659821510314941\n",
      "Training loss:1.7320877313613892\n",
      "Training loss:1.7389074563980103\n",
      "Training loss:1.6711357831954956\n",
      "Training loss:1.7982405424118042\n",
      "Training loss:1.7446101903915405\n",
      "Training loss:1.6700468063354492\n",
      "Training loss:1.759230136871338\n",
      "Training loss:1.7768734693527222\n",
      "Training loss:1.7656526565551758\n",
      "Training loss:1.7582359313964844\n",
      "Training loss:1.7517476081848145\n",
      "Training loss:1.7223191261291504\n",
      "Training loss:1.6606783866882324\n",
      "Training loss:1.826487421989441\n",
      "Training loss:1.7835116386413574\n",
      "Training loss:1.7458151578903198\n",
      "Training loss:1.696850061416626\n",
      "Training loss:1.7944053411483765\n",
      "Training loss:1.742892861366272\n",
      "Training loss:1.7682963609695435\n",
      "Training loss:1.7874120473861694\n",
      "Training loss:1.739876389503479\n",
      "Training loss:1.731048345565796\n",
      "Training loss:1.722886562347412\n",
      "Training loss:1.6392954587936401\n",
      "Training loss:1.69566011428833\n",
      "Training loss:1.6973588466644287\n",
      "Training loss:1.7843745946884155\n",
      "Training loss:1.7555519342422485\n",
      "Training loss:1.7150198221206665\n",
      "Training loss:1.7182371616363525\n",
      "Training loss:1.6604270935058594\n",
      "Training loss:1.7239915132522583\n",
      "Training loss:1.705649495124817\n",
      "Training loss:1.6702598333358765\n",
      "Training loss:1.741644024848938\n",
      "Training loss:1.789077877998352\n",
      "Training loss:1.7620126008987427\n",
      "Training loss:1.6661659479141235\n",
      "Training loss:1.689184308052063\n",
      "Training loss:1.8051122426986694\n",
      "Training loss:1.8209586143493652\n",
      "Training loss:1.7519080638885498\n",
      "Training loss:1.725098729133606\n",
      "Training loss:1.783911108970642\n",
      "Training loss:1.8387346267700195\n",
      "Training loss:1.7195314168930054\n",
      "Training loss:1.7058552503585815\n",
      "Training loss:1.6519920825958252\n",
      "Training loss:1.7998534440994263\n",
      "Training loss:1.6950068473815918\n",
      "Training loss:1.6977641582489014\n",
      "Training loss:1.7694095373153687\n",
      "Training loss:1.7237977981567383\n",
      "Training loss:1.654292106628418\n",
      "Training loss:1.7380082607269287\n",
      "Training loss:1.813780665397644\n",
      "Training loss:1.7534332275390625\n",
      "Training loss:1.6973193883895874\n",
      "Training loss:1.689814805984497\n",
      "Training loss:1.6902166604995728\n",
      "Training loss:1.7251532077789307\n",
      "Training loss:1.6647796630859375\n",
      "Training loss:1.7134922742843628\n",
      "Training loss:1.7547848224639893\n",
      "Training loss:1.7421668767929077\n",
      "Training loss:1.6876968145370483\n",
      "Training loss:1.774416446685791\n",
      "Training loss:1.6939048767089844\n",
      "Training loss:1.7175928354263306\n",
      "Training loss:1.7556966543197632\n",
      "Training loss:1.736330509185791\n",
      "Epoch: 0037 loss_train: 162.956985 acc_train: 0.353896 loss_val: 20.703337 acc_val: 0.353333 time: 90766.452629s\n",
      "Training loss:1.7500689029693604\n",
      "Training loss:1.685679316520691\n",
      "Training loss:1.8105305433273315\n",
      "Training loss:1.7411803007125854\n",
      "Training loss:1.7301805019378662\n",
      "Training loss:1.7041077613830566\n",
      "Training loss:1.7035449743270874\n",
      "Training loss:1.801810383796692\n",
      "Training loss:1.714374303817749\n",
      "Training loss:1.8140100240707397\n",
      "Training loss:1.6904475688934326\n",
      "Training loss:1.757961630821228\n",
      "Training loss:1.6700069904327393\n",
      "Training loss:1.7062742710113525\n",
      "Training loss:1.7343488931655884\n",
      "Training loss:1.7335528135299683\n",
      "Training loss:1.678301453590393\n",
      "Training loss:1.807223916053772\n",
      "Training loss:1.7413372993469238\n",
      "Training loss:1.7934590578079224\n",
      "Training loss:1.8209216594696045\n",
      "Training loss:1.8269935846328735\n",
      "Training loss:1.7655504941940308\n",
      "Training loss:1.6997654438018799\n",
      "Training loss:1.817203164100647\n",
      "Training loss:1.744463562965393\n",
      "Training loss:1.7020738124847412\n",
      "Training loss:1.6890438795089722\n",
      "Training loss:1.6921404600143433\n",
      "Training loss:1.7593928575515747\n",
      "Training loss:1.8050953149795532\n",
      "Training loss:1.720710039138794\n",
      "Training loss:1.698403239250183\n",
      "Training loss:1.7113405466079712\n",
      "Training loss:1.7160618305206299\n",
      "Training loss:1.728460431098938\n",
      "Training loss:1.7783339023590088\n",
      "Training loss:1.6980022192001343\n",
      "Training loss:1.8090310096740723\n",
      "Training loss:1.7328699827194214\n",
      "Training loss:1.8213213682174683\n",
      "Training loss:1.6472159624099731\n",
      "Training loss:1.7791380882263184\n",
      "Training loss:1.7824532985687256\n",
      "Training loss:1.7288689613342285\n",
      "Training loss:1.7124383449554443\n",
      "Training loss:1.7470593452453613\n",
      "Training loss:1.7157020568847656\n",
      "Training loss:1.7728286981582642\n",
      "Training loss:1.7616572380065918\n",
      "Training loss:1.742899775505066\n",
      "Training loss:1.7150441408157349\n",
      "Training loss:1.7753382921218872\n",
      "Training loss:1.697823166847229\n",
      "Training loss:1.7399019002914429\n",
      "Training loss:1.7521369457244873\n",
      "Training loss:1.7891607284545898\n",
      "Training loss:1.6574110984802246\n",
      "Training loss:1.750849723815918\n",
      "Training loss:1.7281057834625244\n",
      "Training loss:1.7150228023529053\n",
      "Training loss:1.725987195968628\n",
      "Training loss:1.786844253540039\n",
      "Training loss:1.6504522562026978\n",
      "Training loss:1.6930509805679321\n",
      "Training loss:1.7808775901794434\n",
      "Training loss:1.707531213760376\n",
      "Training loss:1.6572095155715942\n",
      "Training loss:1.7696199417114258\n",
      "Training loss:1.666894793510437\n",
      "Training loss:1.7332204580307007\n",
      "Training loss:1.7634217739105225\n",
      "Training loss:1.6768120527267456\n",
      "Training loss:1.6671311855316162\n",
      "Training loss:1.6779921054840088\n",
      "Training loss:1.6446003913879395\n",
      "Training loss:1.809502363204956\n",
      "Training loss:1.729757308959961\n",
      "Training loss:1.7372398376464844\n",
      "Training loss:1.7531318664550781\n",
      "Training loss:1.748974323272705\n",
      "Training loss:1.6800838708877563\n",
      "Training loss:1.6586805582046509\n",
      "Training loss:1.7448965311050415\n",
      "Training loss:1.7679877281188965\n",
      "Training loss:1.804694414138794\n",
      "Training loss:1.7326586246490479\n",
      "Training loss:1.6952552795410156\n",
      "Training loss:1.7712699174880981\n",
      "Training loss:1.7436937093734741\n",
      "Training loss:1.7610036134719849\n",
      "Training loss:1.6720587015151978\n",
      "Training loss:1.6893398761749268\n",
      "Training loss:1.7723112106323242\n",
      "Epoch: 0038 loss_train: 163.090822 acc_train: 0.353229 loss_val: 20.704315 acc_val: 0.353667 time: 93279.573784s\n",
      "Training loss:1.6555970907211304\n",
      "Training loss:1.8099009990692139\n",
      "Training loss:1.751136064529419\n",
      "Training loss:1.6688315868377686\n",
      "Training loss:1.7385766506195068\n",
      "Training loss:1.7284736633300781\n",
      "Training loss:1.7640849351882935\n",
      "Training loss:1.7569599151611328\n",
      "Training loss:1.7523839473724365\n",
      "Training loss:1.7096872329711914\n",
      "Training loss:1.723212718963623\n",
      "Training loss:1.7242473363876343\n",
      "Training loss:1.7379118204116821\n",
      "Training loss:1.7409199476242065\n",
      "Training loss:1.7155648469924927\n",
      "Training loss:1.7192310094833374\n",
      "Training loss:1.6914629936218262\n",
      "Training loss:1.6902250051498413\n",
      "Training loss:1.7130225896835327\n",
      "Training loss:1.7597194910049438\n",
      "Training loss:1.7476016283035278\n",
      "Training loss:1.6866344213485718\n",
      "Training loss:1.8316115140914917\n",
      "Training loss:1.768746256828308\n",
      "Training loss:1.716264009475708\n",
      "Training loss:1.740333080291748\n",
      "Training loss:1.8360525369644165\n",
      "Training loss:1.789504051208496\n",
      "Training loss:1.728652834892273\n",
      "Training loss:1.7652620077133179\n",
      "Training loss:1.7200742959976196\n",
      "Training loss:1.7743022441864014\n",
      "Training loss:1.74154794216156\n",
      "Training loss:1.7116961479187012\n",
      "Training loss:1.8038140535354614\n",
      "Training loss:1.6948528289794922\n",
      "Training loss:1.7187730073928833\n",
      "Training loss:1.7434238195419312\n",
      "Training loss:1.7406690120697021\n",
      "Training loss:1.814670205116272\n",
      "Training loss:1.735993504524231\n",
      "Training loss:1.7566951513290405\n",
      "Training loss:1.7359113693237305\n",
      "Training loss:1.7632317543029785\n",
      "Training loss:1.7541050910949707\n",
      "Training loss:1.7526421546936035\n",
      "Training loss:1.6820392608642578\n",
      "Training loss:1.762579083442688\n",
      "Training loss:1.6606438159942627\n",
      "Training loss:1.6657090187072754\n",
      "Training loss:1.780274510383606\n",
      "Training loss:1.8036365509033203\n",
      "Training loss:1.7377817630767822\n",
      "Training loss:1.812422752380371\n",
      "Training loss:1.7851676940917969\n",
      "Training loss:1.676346778869629\n",
      "Training loss:1.7375503778457642\n",
      "Training loss:1.7690016031265259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.8489867448806763\n",
      "Training loss:1.8048129081726074\n",
      "Training loss:1.7223609685897827\n",
      "Training loss:1.7217257022857666\n",
      "Training loss:1.7009271383285522\n",
      "Training loss:1.7392914295196533\n",
      "Training loss:1.770376443862915\n",
      "Training loss:1.7299482822418213\n",
      "Training loss:1.7443655729293823\n",
      "Training loss:1.7895108461380005\n",
      "Training loss:1.673351764678955\n",
      "Training loss:1.7449886798858643\n",
      "Training loss:1.7689968347549438\n",
      "Training loss:1.7646440267562866\n",
      "Training loss:1.6842007637023926\n",
      "Training loss:1.7799396514892578\n",
      "Training loss:1.7201184034347534\n",
      "Training loss:1.7472400665283203\n",
      "Training loss:1.7250174283981323\n",
      "Training loss:1.7039357423782349\n",
      "Training loss:1.7516340017318726\n",
      "Training loss:1.7207403182983398\n",
      "Training loss:1.7091399431228638\n",
      "Training loss:1.7311558723449707\n",
      "Training loss:1.6854686737060547\n",
      "Training loss:1.6948885917663574\n",
      "Training loss:1.632601261138916\n",
      "Training loss:1.7509846687316895\n",
      "Training loss:1.7328349351882935\n",
      "Training loss:1.716424584388733\n",
      "Training loss:1.6973587274551392\n",
      "Training loss:1.7819699048995972\n",
      "Training loss:1.7767369747161865\n",
      "Training loss:1.7855745553970337\n",
      "Training loss:1.8474675416946411\n",
      "Training loss:1.731634259223938\n",
      "Epoch: 0039 loss_train: 163.628722 acc_train: 0.351938 loss_val: 20.705992 acc_val: 0.356000 time: 95800.674686s\n",
      "Training loss:1.7284693717956543\n",
      "Training loss:1.762709140777588\n",
      "Training loss:1.7273904085159302\n",
      "Training loss:1.7019060850143433\n",
      "Training loss:1.7405409812927246\n",
      "Training loss:1.719524621963501\n",
      "Training loss:1.706688642501831\n",
      "Training loss:1.761296272277832\n",
      "Training loss:1.7244540452957153\n",
      "Training loss:1.7832530736923218\n",
      "Training loss:1.8248450756072998\n",
      "Training loss:1.700218677520752\n",
      "Training loss:1.6942787170410156\n",
      "Training loss:1.73215913772583\n",
      "Training loss:1.7114728689193726\n",
      "Training loss:1.7548493146896362\n",
      "Training loss:1.7012709379196167\n",
      "Training loss:1.7731319665908813\n",
      "Training loss:1.647186040878296\n",
      "Training loss:1.8047889471054077\n",
      "Training loss:1.7854323387145996\n",
      "Training loss:1.7327508926391602\n",
      "Training loss:1.7153431177139282\n",
      "Training loss:1.8519704341888428\n",
      "Training loss:1.808827519416809\n",
      "Training loss:1.690779447555542\n",
      "Training loss:1.7690281867980957\n",
      "Training loss:1.690840482711792\n",
      "Training loss:1.7251036167144775\n",
      "Training loss:1.7652682065963745\n",
      "Training loss:1.7529906034469604\n",
      "Training loss:1.7756574153900146\n",
      "Training loss:1.685340404510498\n",
      "Training loss:1.7725597620010376\n",
      "Training loss:1.7809710502624512\n",
      "Training loss:1.786977767944336\n",
      "Training loss:1.7772597074508667\n",
      "Training loss:1.7037808895111084\n",
      "Training loss:1.8006538152694702\n",
      "Training loss:1.72269606590271\n",
      "Training loss:1.709010362625122\n",
      "Training loss:1.6786284446716309\n",
      "Training loss:1.7066607475280762\n",
      "Training loss:1.7921713590621948\n",
      "Training loss:1.7270556688308716\n",
      "Training loss:1.6625723838806152\n",
      "Training loss:1.783049464225769\n",
      "Training loss:1.737816333770752\n",
      "Training loss:1.730650782585144\n",
      "Training loss:1.670884609222412\n",
      "Training loss:1.6471644639968872\n",
      "Training loss:1.7850596904754639\n",
      "Training loss:1.7057985067367554\n",
      "Training loss:1.7358763217926025\n",
      "Training loss:1.6752047538757324\n",
      "Training loss:1.6742464303970337\n",
      "Training loss:1.759588360786438\n",
      "Training loss:1.713792085647583\n",
      "Training loss:1.7804479598999023\n",
      "Training loss:1.7661582231521606\n",
      "Training loss:1.678662657737732\n",
      "Training loss:1.8100250959396362\n",
      "Training loss:1.770742416381836\n",
      "Training loss:1.6937804222106934\n",
      "Training loss:1.6697509288787842\n",
      "Training loss:1.6682977676391602\n",
      "Training loss:1.7435059547424316\n",
      "Training loss:1.698052167892456\n",
      "Training loss:1.7209782600402832\n",
      "Training loss:1.713161826133728\n",
      "Training loss:1.7321600914001465\n",
      "Training loss:1.7530765533447266\n",
      "Training loss:1.7573434114456177\n",
      "Training loss:1.7058327198028564\n",
      "Training loss:1.6915316581726074\n",
      "Training loss:1.7518482208251953\n",
      "Training loss:1.7848604917526245\n",
      "Training loss:1.6663302183151245\n",
      "Training loss:1.7586437463760376\n",
      "Training loss:1.720278024673462\n",
      "Training loss:1.758092999458313\n",
      "Training loss:1.7408450841903687\n",
      "Training loss:1.7322732210159302\n",
      "Training loss:1.7047197818756104\n",
      "Training loss:1.7431186437606812\n",
      "Training loss:1.7355839014053345\n",
      "Training loss:1.7709527015686035\n",
      "Training loss:1.7425823211669922\n",
      "Training loss:1.7281444072723389\n",
      "Training loss:1.8105353116989136\n",
      "Training loss:1.7578976154327393\n",
      "Training loss:1.7632851600646973\n",
      "Training loss:1.6812866926193237\n",
      "Training loss:1.7804826498031616\n",
      "Epoch: 0040 loss_train: 163.175166 acc_train: 0.354687 loss_val: 20.558801 acc_val: 0.357667 time: 98313.379780s\n",
      "Training loss:1.7727445363998413\n",
      "Training loss:1.7297366857528687\n",
      "Training loss:1.6434502601623535\n",
      "Training loss:1.7187341451644897\n",
      "Training loss:1.6706478595733643\n",
      "Training loss:1.7873989343643188\n",
      "Training loss:1.7329235076904297\n",
      "Training loss:1.6846123933792114\n",
      "Training loss:1.7281289100646973\n",
      "Training loss:1.726694107055664\n",
      "Training loss:1.7139946222305298\n",
      "Training loss:1.703902244567871\n",
      "Training loss:1.7019637823104858\n",
      "Training loss:1.7246015071868896\n",
      "Training loss:1.7273396253585815\n",
      "Training loss:1.7558187246322632\n",
      "Training loss:1.7257609367370605\n",
      "Training loss:1.7156063318252563\n",
      "Training loss:1.7693723440170288\n",
      "Training loss:1.6636626720428467\n",
      "Training loss:1.6845134496688843\n",
      "Training loss:1.7393460273742676\n",
      "Training loss:1.7528879642486572\n",
      "Training loss:1.7454785108566284\n",
      "Training loss:1.8022245168685913\n",
      "Training loss:1.7469582557678223\n",
      "Training loss:1.76826810836792\n",
      "Training loss:1.7262400388717651\n",
      "Training loss:1.6877810955047607\n",
      "Training loss:1.7928262948989868\n",
      "Training loss:1.7119183540344238\n",
      "Training loss:1.699630856513977\n",
      "Training loss:1.7713024616241455\n",
      "Training loss:1.6513149738311768\n",
      "Training loss:1.7258315086364746\n",
      "Training loss:1.8779186010360718\n",
      "Training loss:1.779996633529663\n",
      "Training loss:1.7801023721694946\n",
      "Training loss:1.688075304031372\n",
      "Training loss:1.7408976554870605\n",
      "Training loss:1.8098571300506592\n",
      "Training loss:1.7460888624191284\n",
      "Training loss:1.759528636932373\n",
      "Training loss:1.7307229042053223\n",
      "Training loss:1.7131210565567017\n",
      "Training loss:1.757238507270813\n",
      "Training loss:1.750961184501648\n",
      "Training loss:1.7215498685836792\n",
      "Training loss:1.7094790935516357\n",
      "Training loss:1.7335209846496582\n",
      "Training loss:1.869592308998108\n",
      "Training loss:1.6663076877593994\n",
      "Training loss:1.7696502208709717\n",
      "Training loss:1.7943754196166992\n",
      "Training loss:1.7583949565887451\n",
      "Training loss:1.708294153213501\n",
      "Training loss:1.788857102394104\n",
      "Training loss:1.7268507480621338\n",
      "Training loss:1.7909854650497437\n",
      "Training loss:1.7390246391296387\n",
      "Training loss:1.7344168424606323\n",
      "Training loss:1.683516502380371\n",
      "Training loss:1.7580409049987793\n",
      "Training loss:1.7582118511199951\n",
      "Training loss:1.7323025465011597\n",
      "Training loss:1.7492657899856567\n",
      "Training loss:1.6553179025650024\n",
      "Training loss:1.7543585300445557\n",
      "Training loss:1.7203140258789062\n",
      "Training loss:1.75371253490448\n",
      "Training loss:1.7360765933990479\n",
      "Training loss:1.7687469720840454\n",
      "Training loss:1.7443742752075195\n",
      "Training loss:1.708970308303833\n",
      "Training loss:1.7859504222869873\n",
      "Training loss:1.7239434719085693\n",
      "Training loss:1.6665449142456055\n",
      "Training loss:1.6523057222366333\n",
      "Training loss:1.808359146118164\n",
      "Training loss:1.7766697406768799\n",
      "Training loss:1.7509709596633911\n",
      "Training loss:1.7344814538955688\n",
      "Training loss:1.7626910209655762\n",
      "Training loss:1.7042169570922852\n",
      "Training loss:1.705359697341919\n",
      "Training loss:1.681331753730774\n",
      "Training loss:1.7301843166351318\n",
      "Training loss:1.8152049779891968\n",
      "Training loss:1.7396297454833984\n",
      "Training loss:1.7096736431121826\n",
      "Training loss:1.7706530094146729\n",
      "Training loss:1.7370939254760742\n",
      "Training loss:1.7945804595947266\n",
      "Training loss:1.6645182371139526\n",
      "Epoch: 0041 loss_train: 163.286997 acc_train: 0.353063 loss_val: 20.779824 acc_val: 0.358167 time: 100828.309045s\n",
      "Training loss:1.7922208309173584\n",
      "Training loss:1.7009474039077759\n",
      "Training loss:1.7836406230926514\n",
      "Training loss:1.7296080589294434\n",
      "Training loss:1.798860788345337\n",
      "Training loss:1.7526249885559082\n",
      "Training loss:1.6938393115997314\n",
      "Training loss:1.7905941009521484\n",
      "Training loss:1.7870131731033325\n",
      "Training loss:1.7929521799087524\n",
      "Training loss:1.7512811422348022\n",
      "Training loss:1.8143212795257568\n",
      "Training loss:1.7249847650527954\n",
      "Training loss:1.7320671081542969\n",
      "Training loss:1.7191214561462402\n",
      "Training loss:1.6810272932052612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.7210768461227417\n",
      "Training loss:1.6392344236373901\n",
      "Training loss:1.6744146347045898\n",
      "Training loss:1.8073190450668335\n",
      "Training loss:1.8296970129013062\n",
      "Training loss:1.7097851037979126\n",
      "Training loss:1.7131218910217285\n",
      "Training loss:1.7499287128448486\n",
      "Training loss:1.710654854774475\n",
      "Training loss:1.717940330505371\n",
      "Training loss:1.756783366203308\n",
      "Training loss:1.7481168508529663\n",
      "Training loss:1.694112777709961\n",
      "Training loss:1.7491374015808105\n",
      "Training loss:1.7502353191375732\n",
      "Training loss:1.7402763366699219\n",
      "Training loss:1.725081205368042\n",
      "Training loss:1.6601285934448242\n",
      "Training loss:1.7056539058685303\n",
      "Training loss:1.7665441036224365\n",
      "Training loss:1.6701176166534424\n",
      "Training loss:1.6472846269607544\n",
      "Training loss:1.741265058517456\n",
      "Training loss:1.747058629989624\n",
      "Training loss:1.6834726333618164\n",
      "Training loss:1.7586289644241333\n",
      "Training loss:1.7549902200698853\n",
      "Training loss:1.7863677740097046\n",
      "Training loss:1.7456268072128296\n",
      "Training loss:1.731289267539978\n",
      "Training loss:1.7188588380813599\n",
      "Training loss:1.7358092069625854\n",
      "Training loss:1.6780530214309692\n",
      "Training loss:1.7606651782989502\n",
      "Training loss:1.6903069019317627\n",
      "Training loss:1.7333937883377075\n",
      "Training loss:1.7663151025772095\n",
      "Training loss:1.7005064487457275\n",
      "Training loss:1.6482385396957397\n",
      "Training loss:1.7245121002197266\n",
      "Training loss:1.7334685325622559\n",
      "Training loss:1.7729321718215942\n",
      "Training loss:1.6618674993515015\n",
      "Training loss:1.7699564695358276\n",
      "Training loss:1.7139902114868164\n",
      "Training loss:1.7056071758270264\n",
      "Training loss:1.706735372543335\n",
      "Training loss:1.7352707386016846\n",
      "Training loss:1.7421112060546875\n",
      "Training loss:1.734373688697815\n",
      "Training loss:1.71619713306427\n",
      "Training loss:1.6950688362121582\n",
      "Training loss:1.7044121026992798\n",
      "Training loss:1.7053167819976807\n",
      "Training loss:1.653605341911316\n",
      "Training loss:1.6678162813186646\n",
      "Training loss:1.7578781843185425\n",
      "Training loss:1.7593845129013062\n",
      "Training loss:1.7305781841278076\n",
      "Training loss:1.671086311340332\n",
      "Training loss:1.7527776956558228\n",
      "Training loss:1.7495884895324707\n",
      "Training loss:1.657837986946106\n",
      "Training loss:1.749397873878479\n",
      "Training loss:1.7200291156768799\n",
      "Training loss:1.761906385421753\n",
      "Training loss:1.7229101657867432\n",
      "Training loss:1.7557861804962158\n",
      "Training loss:1.7049336433410645\n",
      "Training loss:1.7235666513442993\n",
      "Training loss:1.73004150390625\n",
      "Training loss:1.704056978225708\n",
      "Training loss:1.7122725248336792\n",
      "Training loss:1.7116886377334595\n",
      "Training loss:1.7654573917388916\n",
      "Training loss:1.6885101795196533\n",
      "Training loss:1.7966313362121582\n",
      "Training loss:1.7111167907714844\n",
      "Epoch: 0042 loss_train: 162.493274 acc_train: 0.357521 loss_val: 20.749945 acc_val: 0.357000 time: 103336.321579s\n",
      "Training loss:1.7394870519638062\n",
      "Training loss:1.754368543624878\n",
      "Training loss:1.7785587310791016\n",
      "Training loss:1.7467436790466309\n",
      "Training loss:1.669222354888916\n",
      "Training loss:1.7785484790802002\n",
      "Training loss:1.7447913885116577\n",
      "Training loss:1.7482277154922485\n",
      "Training loss:1.7732518911361694\n",
      "Training loss:1.8099561929702759\n",
      "Training loss:1.7453216314315796\n",
      "Training loss:1.774868130683899\n",
      "Training loss:1.6762371063232422\n",
      "Training loss:1.758500576019287\n",
      "Training loss:1.679072618484497\n",
      "Training loss:1.76271653175354\n",
      "Training loss:1.737734317779541\n",
      "Training loss:1.766422152519226\n",
      "Training loss:1.6998616456985474\n",
      "Training loss:1.7177560329437256\n",
      "Training loss:1.8062729835510254\n",
      "Training loss:1.7510290145874023\n",
      "Training loss:1.7120600938796997\n",
      "Training loss:1.7182830572128296\n",
      "Training loss:1.718106985092163\n",
      "Training loss:1.7356008291244507\n",
      "Training loss:1.7717187404632568\n",
      "Training loss:1.7562880516052246\n",
      "Training loss:1.765174150466919\n",
      "Training loss:1.657671570777893\n",
      "Training loss:1.7577579021453857\n",
      "Training loss:1.7887879610061646\n",
      "Training loss:1.6290374994277954\n",
      "Training loss:1.718709945678711\n",
      "Training loss:1.7147047519683838\n",
      "Training loss:1.6823877096176147\n",
      "Training loss:1.701866865158081\n",
      "Training loss:1.7435557842254639\n",
      "Training loss:1.6589765548706055\n",
      "Training loss:1.7342455387115479\n",
      "Training loss:1.7908411026000977\n",
      "Training loss:1.777605414390564\n",
      "Training loss:1.7105826139450073\n",
      "Training loss:1.689612865447998\n",
      "Training loss:1.7024879455566406\n",
      "Training loss:1.748093605041504\n",
      "Training loss:1.7310606241226196\n",
      "Training loss:1.7324317693710327\n",
      "Training loss:1.7699161767959595\n",
      "Training loss:1.668756365776062\n",
      "Training loss:1.729793906211853\n",
      "Training loss:1.7553120851516724\n",
      "Training loss:1.7698819637298584\n",
      "Training loss:1.8343853950500488\n",
      "Training loss:1.751306414604187\n",
      "Training loss:1.7116689682006836\n",
      "Training loss:1.713622808456421\n",
      "Training loss:1.77938711643219\n",
      "Training loss:1.7311286926269531\n",
      "Training loss:1.7768718004226685\n",
      "Training loss:1.7337254285812378\n",
      "Training loss:1.7749215364456177\n",
      "Training loss:1.7362531423568726\n",
      "Training loss:1.6805599927902222\n",
      "Training loss:1.7232202291488647\n",
      "Training loss:1.7595537900924683\n",
      "Training loss:1.7099270820617676\n",
      "Training loss:1.7196779251098633\n",
      "Training loss:1.7431591749191284\n",
      "Training loss:1.74191153049469\n",
      "Training loss:1.7103171348571777\n",
      "Training loss:1.7297199964523315\n",
      "Training loss:1.6320838928222656\n",
      "Training loss:1.6623567342758179\n",
      "Training loss:1.8358650207519531\n",
      "Training loss:1.736243724822998\n",
      "Training loss:1.7023909091949463\n",
      "Training loss:1.7581062316894531\n",
      "Training loss:1.725225567817688\n",
      "Training loss:1.7582978010177612\n",
      "Training loss:1.7570037841796875\n",
      "Training loss:1.757104516029358\n",
      "Training loss:1.6787389516830444\n",
      "Training loss:1.797743797302246\n",
      "Training loss:1.6467835903167725\n",
      "Training loss:1.6730327606201172\n",
      "Training loss:1.7393497228622437\n",
      "Training loss:1.732655644416809\n",
      "Training loss:1.7870420217514038\n",
      "Training loss:1.7354564666748047\n",
      "Training loss:1.7809520959854126\n",
      "Training loss:1.706524133682251\n",
      "Training loss:1.7813743352890015\n",
      "Training loss:1.6086286306381226\n",
      "Epoch: 0043 loss_train: 163.014538 acc_train: 0.354104 loss_val: 20.676894 acc_val: 0.359333 time: 105874.207171s\n",
      "Training loss:1.762576937675476\n",
      "Training loss:1.6837491989135742\n",
      "Training loss:1.73654043674469\n",
      "Training loss:1.751426100730896\n",
      "Training loss:1.7174396514892578\n",
      "Training loss:1.6092686653137207\n",
      "Training loss:1.7406551837921143\n",
      "Training loss:1.6985785961151123\n",
      "Training loss:1.7308688163757324\n",
      "Training loss:1.7627480030059814\n",
      "Training loss:1.616182565689087\n",
      "Training loss:1.7503107786178589\n",
      "Training loss:1.7082953453063965\n",
      "Training loss:1.6809048652648926\n",
      "Training loss:1.778428554534912\n",
      "Training loss:1.7897236347198486\n",
      "Training loss:1.759106993675232\n",
      "Training loss:1.7116477489471436\n",
      "Training loss:1.7899749279022217\n",
      "Training loss:1.7261192798614502\n",
      "Training loss:1.7530981302261353\n",
      "Training loss:1.7397602796554565\n",
      "Training loss:1.6989493370056152\n",
      "Training loss:1.781295895576477\n",
      "Training loss:1.7009340524673462\n",
      "Training loss:1.789194941520691\n",
      "Training loss:1.6929051876068115\n",
      "Training loss:1.7320690155029297\n",
      "Training loss:1.7179378271102905\n",
      "Training loss:1.6601378917694092\n",
      "Training loss:1.722686529159546\n",
      "Training loss:1.6540881395339966\n",
      "Training loss:1.7913795709609985\n",
      "Training loss:1.6999210119247437\n",
      "Training loss:1.7058387994766235\n",
      "Training loss:1.7821269035339355\n",
      "Training loss:1.8271458148956299\n",
      "Training loss:1.7241560220718384\n",
      "Training loss:1.7130509614944458\n",
      "Training loss:1.7911587953567505\n",
      "Training loss:1.733028769493103\n",
      "Training loss:1.7928175926208496\n",
      "Training loss:1.7948275804519653\n",
      "Training loss:1.7438483238220215\n",
      "Training loss:1.714506983757019\n",
      "Training loss:1.7411363124847412\n",
      "Training loss:1.7058392763137817\n",
      "Training loss:1.769429326057434\n",
      "Training loss:1.6696357727050781\n",
      "Training loss:1.771719217300415\n",
      "Training loss:1.7178523540496826\n",
      "Training loss:1.6418899297714233\n",
      "Training loss:1.7115498781204224\n",
      "Training loss:1.6738048791885376\n",
      "Training loss:1.7944170236587524\n",
      "Training loss:1.7388046979904175\n",
      "Training loss:1.662413477897644\n",
      "Training loss:1.650827407836914\n",
      "Training loss:1.7167754173278809\n",
      "Training loss:1.7274500131607056\n",
      "Training loss:1.7410225868225098\n",
      "Training loss:1.734571099281311\n",
      "Training loss:1.6788289546966553\n",
      "Training loss:1.7376223802566528\n",
      "Training loss:1.7719429731369019\n",
      "Training loss:1.7554011344909668\n",
      "Training loss:1.7779133319854736\n",
      "Training loss:1.7856930494308472\n",
      "Training loss:1.7455198764801025\n",
      "Training loss:1.7013957500457764\n",
      "Training loss:1.8157862424850464\n",
      "Training loss:1.7240103483200073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6623060703277588\n",
      "Training loss:1.6898421049118042\n",
      "Training loss:1.691695213317871\n",
      "Training loss:1.8532267808914185\n",
      "Training loss:1.7184350490570068\n",
      "Training loss:1.6573643684387207\n",
      "Training loss:1.781419038772583\n",
      "Training loss:1.7531977891921997\n",
      "Training loss:1.7277907133102417\n",
      "Training loss:1.7176061868667603\n",
      "Training loss:1.7849427461624146\n",
      "Training loss:1.696413278579712\n",
      "Training loss:1.7159475088119507\n",
      "Training loss:1.719465970993042\n",
      "Training loss:1.7163991928100586\n",
      "Training loss:1.659498929977417\n",
      "Training loss:1.773504614830017\n",
      "Training loss:1.7878423929214478\n",
      "Training loss:1.8068599700927734\n",
      "Training loss:1.7173646688461304\n",
      "Training loss:1.7216331958770752\n",
      "Training loss:1.7075012922286987\n",
      "Epoch: 0044 loss_train: 162.684920 acc_train: 0.356687 loss_val: 20.530360 acc_val: 0.364167 time: 108438.390725s\n",
      "Training loss:1.6923608779907227\n",
      "Training loss:1.8080273866653442\n",
      "Training loss:1.6724810600280762\n",
      "Training loss:1.721195101737976\n",
      "Training loss:1.6927036046981812\n",
      "Training loss:1.7901581525802612\n",
      "Training loss:1.6154648065567017\n",
      "Training loss:1.7309393882751465\n",
      "Training loss:1.7048956155776978\n",
      "Training loss:1.8024471998214722\n",
      "Training loss:1.675894856452942\n",
      "Training loss:1.689997911453247\n",
      "Training loss:1.7604048252105713\n",
      "Training loss:1.6953709125518799\n",
      "Training loss:1.737310528755188\n",
      "Training loss:1.704343318939209\n",
      "Training loss:1.6869878768920898\n",
      "Training loss:1.7440574169158936\n",
      "Training loss:1.6972541809082031\n",
      "Training loss:1.8276073932647705\n",
      "Training loss:1.7009456157684326\n",
      "Training loss:1.7984237670898438\n",
      "Training loss:1.7053558826446533\n",
      "Training loss:1.6910027265548706\n",
      "Training loss:1.7053067684173584\n",
      "Training loss:1.7242543697357178\n",
      "Training loss:1.8420192003250122\n",
      "Training loss:1.6773008108139038\n",
      "Training loss:1.7034019231796265\n",
      "Training loss:1.7368706464767456\n",
      "Training loss:1.7463887929916382\n",
      "Training loss:1.6861426830291748\n",
      "Training loss:1.695998191833496\n",
      "Training loss:1.7405076026916504\n",
      "Training loss:1.7700878381729126\n",
      "Training loss:1.6365938186645508\n",
      "Training loss:1.7728605270385742\n",
      "Training loss:1.7349666357040405\n",
      "Training loss:1.724070429801941\n",
      "Training loss:1.6537045240402222\n",
      "Training loss:1.7537429332733154\n",
      "Training loss:1.7176728248596191\n",
      "Training loss:1.7948369979858398\n",
      "Training loss:1.728447675704956\n",
      "Training loss:1.7443172931671143\n",
      "Training loss:1.7025203704833984\n",
      "Training loss:1.7539535760879517\n",
      "Training loss:1.755055546760559\n",
      "Training loss:1.6814792156219482\n",
      "Training loss:1.747230052947998\n",
      "Training loss:1.758764624595642\n",
      "Training loss:1.7338393926620483\n",
      "Training loss:1.6438227891921997\n",
      "Training loss:1.7107607126235962\n",
      "Training loss:1.762965202331543\n",
      "Training loss:1.7341138124465942\n",
      "Training loss:1.6795700788497925\n",
      "Training loss:1.7351610660552979\n",
      "Training loss:1.7651413679122925\n",
      "Training loss:1.702942967414856\n",
      "Training loss:1.703511118888855\n",
      "Training loss:1.7555251121520996\n",
      "Training loss:1.7894753217697144\n",
      "Training loss:1.6973891258239746\n",
      "Training loss:1.7186444997787476\n",
      "Training loss:1.7258583307266235\n",
      "Training loss:1.7155296802520752\n",
      "Training loss:1.7244329452514648\n",
      "Training loss:1.7982290983200073\n",
      "Training loss:1.697458267211914\n",
      "Training loss:1.6366347074508667\n",
      "Training loss:1.7497451305389404\n",
      "Training loss:1.7739883661270142\n",
      "Training loss:1.7342958450317383\n",
      "Training loss:1.7015613317489624\n",
      "Training loss:1.7088109254837036\n",
      "Training loss:1.718355655670166\n",
      "Training loss:1.6593327522277832\n",
      "Training loss:1.6968215703964233\n",
      "Training loss:1.7008954286575317\n",
      "Training loss:1.6937243938446045\n",
      "Training loss:1.7040917873382568\n",
      "Training loss:1.661854863166809\n",
      "Training loss:1.703299880027771\n",
      "Training loss:1.7548789978027344\n",
      "Training loss:1.7279869318008423\n",
      "Training loss:1.7318689823150635\n",
      "Training loss:1.7440654039382935\n",
      "Training loss:1.6812527179718018\n",
      "Training loss:1.683661699295044\n",
      "Training loss:1.7292035818099976\n",
      "Training loss:1.8329870700836182\n",
      "Training loss:1.6682218313217163\n",
      "Training loss:1.7088433504104614\n",
      "Epoch: 0045 loss_train: 161.934880 acc_train: 0.360167 loss_val: 20.508854 acc_val: 0.364500 time: 113476.346588s\n",
      "Training loss:1.6627832651138306\n",
      "Training loss:1.6874712705612183\n",
      "Training loss:1.685023307800293\n",
      "Training loss:1.7103865146636963\n",
      "Training loss:1.7484948635101318\n",
      "Training loss:1.7435929775238037\n",
      "Training loss:1.6936441659927368\n",
      "Training loss:1.6743324995040894\n",
      "Training loss:1.7580043077468872\n",
      "Training loss:1.7605023384094238\n",
      "Training loss:1.8525235652923584\n",
      "Training loss:1.7076126337051392\n",
      "Training loss:1.7342689037322998\n",
      "Training loss:1.7436890602111816\n",
      "Training loss:1.7572498321533203\n",
      "Training loss:1.745516300201416\n",
      "Training loss:1.6824735403060913\n",
      "Training loss:1.7551997900009155\n",
      "Training loss:1.803374171257019\n",
      "Training loss:1.7244240045547485\n",
      "Training loss:1.712597370147705\n",
      "Training loss:1.6974965333938599\n",
      "Training loss:1.8071541786193848\n",
      "Training loss:1.6670175790786743\n",
      "Training loss:1.74929940700531\n",
      "Training loss:1.6879936456680298\n",
      "Training loss:1.7489303350448608\n",
      "Training loss:1.6621919870376587\n",
      "Training loss:1.715258240699768\n",
      "Training loss:1.7548707723617554\n",
      "Training loss:1.7384084463119507\n",
      "Training loss:1.694049596786499\n",
      "Training loss:1.6792094707489014\n",
      "Training loss:1.7388743162155151\n",
      "Training loss:1.7390581369400024\n",
      "Training loss:1.7785518169403076\n",
      "Training loss:1.7041900157928467\n",
      "Training loss:1.7904131412506104\n",
      "Training loss:1.6965731382369995\n",
      "Training loss:1.6886261701583862\n",
      "Training loss:1.7611585855484009\n",
      "Training loss:1.7936800718307495\n",
      "Training loss:1.8088122606277466\n",
      "Training loss:1.7718697786331177\n",
      "Training loss:1.7028684616088867\n",
      "Training loss:1.7226719856262207\n",
      "Training loss:1.687821388244629\n",
      "Training loss:1.6876329183578491\n",
      "Training loss:1.6557918787002563\n",
      "Training loss:1.672286033630371\n",
      "Training loss:1.6396303176879883\n",
      "Training loss:1.7449086904525757\n",
      "Training loss:1.7022149562835693\n",
      "Training loss:1.776368498802185\n",
      "Training loss:1.7563951015472412\n",
      "Training loss:1.715856671333313\n",
      "Training loss:1.754740595817566\n",
      "Training loss:1.6878753900527954\n",
      "Training loss:1.6948745250701904\n",
      "Training loss:1.739292860031128\n",
      "Training loss:1.7125720977783203\n",
      "Training loss:1.7301433086395264\n",
      "Training loss:1.6823643445968628\n",
      "Training loss:1.694496750831604\n",
      "Training loss:1.7747833728790283\n",
      "Training loss:1.7644662857055664\n",
      "Training loss:1.7060573101043701\n",
      "Training loss:1.7289847135543823\n",
      "Training loss:1.761656403541565\n",
      "Training loss:1.791914463043213\n",
      "Training loss:1.6737710237503052\n",
      "Training loss:1.7864456176757812\n",
      "Training loss:1.7566548585891724\n",
      "Training loss:1.6271990537643433\n",
      "Training loss:1.7341755628585815\n",
      "Training loss:1.7121788263320923\n",
      "Training loss:1.6918401718139648\n",
      "Training loss:1.7481080293655396\n",
      "Training loss:1.7053813934326172\n",
      "Training loss:1.7275091409683228\n",
      "Training loss:1.7939876317977905\n",
      "Training loss:1.702433705329895\n",
      "Training loss:1.795925259590149\n",
      "Training loss:1.7879539728164673\n",
      "Training loss:1.7361584901809692\n",
      "Training loss:1.6415536403656006\n",
      "Training loss:1.6275321245193481\n",
      "Training loss:1.8218369483947754\n",
      "Training loss:1.70944082736969\n",
      "Training loss:1.7325516939163208\n",
      "Training loss:1.7185055017471313\n",
      "Training loss:1.704652190208435\n",
      "Training loss:1.705006718635559\n",
      "Training loss:1.8254948854446411\n",
      "Epoch: 0046 loss_train: 162.371819 acc_train: 0.356229 loss_val: 20.462692 acc_val: 0.365833 time: 116100.650868s\n",
      "Training loss:1.7027900218963623\n",
      "Training loss:1.7132909297943115\n",
      "Training loss:1.696970820426941\n",
      "Training loss:1.764718770980835\n",
      "Training loss:1.7506259679794312\n",
      "Training loss:1.7033909559249878\n",
      "Training loss:1.725756287574768\n",
      "Training loss:1.757572889328003\n",
      "Training loss:1.7441679239273071\n",
      "Training loss:1.7737245559692383\n",
      "Training loss:1.666046142578125\n",
      "Training loss:1.6052675247192383\n",
      "Training loss:1.7256914377212524\n",
      "Training loss:1.64365816116333\n",
      "Training loss:1.7179266214370728\n",
      "Training loss:1.6865426301956177\n",
      "Training loss:1.7141112089157104\n",
      "Training loss:1.7066007852554321\n",
      "Training loss:1.7651382684707642\n",
      "Training loss:1.800971269607544\n",
      "Training loss:1.7261183261871338\n",
      "Training loss:1.7525559663772583\n",
      "Training loss:1.7491390705108643\n",
      "Training loss:1.7316373586654663\n",
      "Training loss:1.687333583831787\n",
      "Training loss:1.6830898523330688\n",
      "Training loss:1.7085155248641968\n",
      "Training loss:1.7215601205825806\n",
      "Training loss:1.6898478269577026\n",
      "Training loss:1.7304940223693848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6763542890548706\n",
      "Training loss:1.8332750797271729\n",
      "Training loss:1.728346586227417\n",
      "Training loss:1.6991649866104126\n",
      "Training loss:1.7223597764968872\n",
      "Training loss:1.7984739542007446\n",
      "Training loss:1.7204997539520264\n",
      "Training loss:1.7483220100402832\n",
      "Training loss:1.662032127380371\n",
      "Training loss:1.691921591758728\n",
      "Training loss:1.6923080682754517\n",
      "Training loss:1.7747148275375366\n",
      "Training loss:1.7376092672348022\n",
      "Training loss:1.7245076894760132\n",
      "Training loss:1.7539942264556885\n",
      "Training loss:1.744377851486206\n",
      "Training loss:1.8115421533584595\n",
      "Training loss:1.7002885341644287\n",
      "Training loss:1.725158452987671\n",
      "Training loss:1.6947522163391113\n",
      "Training loss:1.6812238693237305\n",
      "Training loss:1.6685497760772705\n",
      "Training loss:1.7957504987716675\n",
      "Training loss:1.7212311029434204\n",
      "Training loss:1.6745202541351318\n",
      "Training loss:1.813076376914978\n",
      "Training loss:1.789050579071045\n",
      "Training loss:1.6653358936309814\n",
      "Training loss:1.715965986251831\n",
      "Training loss:1.703798532485962\n",
      "Training loss:1.677871584892273\n",
      "Training loss:1.7066863775253296\n",
      "Training loss:1.7091851234436035\n",
      "Training loss:1.6807456016540527\n",
      "Training loss:1.72629976272583\n",
      "Training loss:1.6885035037994385\n",
      "Training loss:1.7251814603805542\n",
      "Training loss:1.7436352968215942\n",
      "Training loss:1.6766825914382935\n",
      "Training loss:1.8026041984558105\n",
      "Training loss:1.640938401222229\n",
      "Training loss:1.6886495351791382\n",
      "Training loss:1.8154079914093018\n",
      "Training loss:1.714363694190979\n",
      "Training loss:1.6972116231918335\n",
      "Training loss:1.7690297365188599\n",
      "Training loss:1.7124143838882446\n",
      "Training loss:1.6576699018478394\n",
      "Training loss:1.7796984910964966\n",
      "Training loss:1.740808129310608\n",
      "Training loss:1.7080754041671753\n",
      "Training loss:1.7135440111160278\n",
      "Training loss:1.7280230522155762\n",
      "Training loss:1.6558197736740112\n",
      "Training loss:1.692006230354309\n",
      "Training loss:1.7742732763290405\n",
      "Training loss:1.7334390878677368\n",
      "Training loss:1.7312625646591187\n",
      "Training loss:1.6775590181350708\n",
      "Training loss:1.68548583984375\n",
      "Training loss:1.7543660402297974\n",
      "Training loss:1.6384780406951904\n",
      "Training loss:1.720083236694336\n",
      "Training loss:1.7276514768600464\n",
      "Epoch: 0047 loss_train: 161.707416 acc_train: 0.359125 loss_val: 20.697906 acc_val: 0.361167 time: 119382.170351s\n",
      "Training loss:1.7605284452438354\n",
      "Training loss:1.6881686449050903\n",
      "Training loss:1.7167682647705078\n",
      "Training loss:1.7333121299743652\n",
      "Training loss:1.7705037593841553\n",
      "Training loss:1.7251176834106445\n",
      "Training loss:1.6583518981933594\n",
      "Training loss:1.662459135055542\n",
      "Training loss:1.8233078718185425\n",
      "Training loss:1.6185494661331177\n",
      "Training loss:1.7531744241714478\n",
      "Training loss:1.7365435361862183\n",
      "Training loss:1.7354081869125366\n",
      "Training loss:1.7128040790557861\n",
      "Training loss:1.672135591506958\n",
      "Training loss:1.7240620851516724\n",
      "Training loss:1.6765713691711426\n",
      "Training loss:1.7619181871414185\n",
      "Training loss:1.7093544006347656\n",
      "Training loss:1.6784738302230835\n",
      "Training loss:1.6756885051727295\n",
      "Training loss:1.7470951080322266\n",
      "Training loss:1.7004742622375488\n",
      "Training loss:1.7112690210342407\n",
      "Training loss:1.6777642965316772\n",
      "Training loss:1.7750581502914429\n",
      "Training loss:1.7223000526428223\n",
      "Training loss:1.7141809463500977\n",
      "Training loss:1.7232633829116821\n",
      "Training loss:1.7801601886749268\n",
      "Training loss:1.7428785562515259\n",
      "Training loss:1.6839077472686768\n",
      "Training loss:1.6664905548095703\n",
      "Training loss:1.746358036994934\n",
      "Training loss:1.735077977180481\n",
      "Training loss:1.7451025247573853\n",
      "Training loss:1.693334937095642\n",
      "Training loss:1.7628624439239502\n",
      "Training loss:1.7602092027664185\n",
      "Training loss:1.7337366342544556\n",
      "Training loss:1.7298892736434937\n",
      "Training loss:1.6827322244644165\n",
      "Training loss:1.7487529516220093\n",
      "Training loss:1.7547378540039062\n",
      "Training loss:1.678368330001831\n",
      "Training loss:1.7650130987167358\n",
      "Training loss:1.6831213235855103\n",
      "Training loss:1.6605677604675293\n",
      "Training loss:1.6580941677093506\n",
      "Training loss:1.7837069034576416\n",
      "Training loss:1.7231813669204712\n",
      "Training loss:1.7647725343704224\n",
      "Training loss:1.736043095588684\n",
      "Training loss:1.6850571632385254\n",
      "Training loss:1.739924669265747\n",
      "Training loss:1.7310796976089478\n",
      "Training loss:1.6966103315353394\n",
      "Training loss:1.7348039150238037\n",
      "Training loss:1.7229782342910767\n",
      "Training loss:1.7404810190200806\n",
      "Training loss:1.7256087064743042\n",
      "Training loss:1.7310924530029297\n",
      "Training loss:1.6631075143814087\n",
      "Training loss:1.673370361328125\n",
      "Training loss:1.689098596572876\n",
      "Training loss:1.6909582614898682\n",
      "Training loss:1.632720708847046\n",
      "Training loss:1.6748243570327759\n",
      "Training loss:1.7149606943130493\n",
      "Training loss:1.7187280654907227\n",
      "Training loss:1.7044768333435059\n",
      "Training loss:1.802668809890747\n",
      "Training loss:1.7463041543960571\n",
      "Training loss:1.7173049449920654\n",
      "Training loss:1.677154779434204\n",
      "Training loss:1.7197085618972778\n",
      "Training loss:1.6627904176712036\n",
      "Training loss:1.7894760370254517\n",
      "Training loss:1.7406750917434692\n",
      "Training loss:1.7581547498703003\n",
      "Training loss:1.6788359880447388\n",
      "Training loss:1.720018982887268\n",
      "Training loss:1.7546088695526123\n",
      "Training loss:1.6606885194778442\n",
      "Training loss:1.6717280149459839\n",
      "Training loss:1.756338119506836\n",
      "Training loss:1.7480615377426147\n",
      "Training loss:1.7290037870407104\n",
      "Training loss:1.7156169414520264\n",
      "Training loss:1.754941701889038\n",
      "Training loss:1.7648680210113525\n",
      "Training loss:1.7757633924484253\n",
      "Training loss:1.7068969011306763\n",
      "Training loss:1.76357901096344\n",
      "Epoch: 0048 loss_train: 161.668775 acc_train: 0.361667 loss_val: 20.682858 acc_val: 0.356833 time: 122018.293891s\n",
      "Training loss:1.6377263069152832\n",
      "Training loss:1.6543790102005005\n",
      "Training loss:1.740749716758728\n",
      "Training loss:1.7140966653823853\n",
      "Training loss:1.7247923612594604\n",
      "Training loss:1.7056132555007935\n",
      "Training loss:1.6095672845840454\n",
      "Training loss:1.7685226202011108\n",
      "Training loss:1.7153295278549194\n",
      "Training loss:1.6853207349777222\n",
      "Training loss:1.7199934720993042\n",
      "Training loss:1.7037687301635742\n",
      "Training loss:1.6783726215362549\n",
      "Training loss:1.7227743864059448\n",
      "Training loss:1.658661961555481\n",
      "Training loss:1.7194995880126953\n",
      "Training loss:1.7197415828704834\n",
      "Training loss:1.7575647830963135\n",
      "Training loss:1.671931266784668\n",
      "Training loss:1.7505756616592407\n",
      "Training loss:1.7372575998306274\n",
      "Training loss:1.7429100275039673\n",
      "Training loss:1.776807188987732\n",
      "Training loss:1.7431271076202393\n",
      "Training loss:1.7552530765533447\n",
      "Training loss:1.713434100151062\n",
      "Training loss:1.6642649173736572\n",
      "Training loss:1.642735242843628\n",
      "Training loss:1.7602907419204712\n",
      "Training loss:1.7739944458007812\n",
      "Training loss:1.7371219396591187\n",
      "Training loss:1.6782523393630981\n",
      "Training loss:1.731476902961731\n",
      "Training loss:1.6785110235214233\n",
      "Training loss:1.6989247798919678\n",
      "Training loss:1.7382159233093262\n",
      "Training loss:1.6473400592803955\n",
      "Training loss:1.6791521310806274\n",
      "Training loss:1.7071664333343506\n",
      "Training loss:1.776726484298706\n",
      "Training loss:1.7559322118759155\n",
      "Training loss:1.6917251348495483\n",
      "Training loss:1.6859067678451538\n",
      "Training loss:1.7852469682693481\n",
      "Training loss:1.7404742240905762\n",
      "Training loss:1.7180147171020508\n",
      "Training loss:1.718248724937439\n",
      "Training loss:1.6952449083328247\n",
      "Training loss:1.7753252983093262\n",
      "Training loss:1.7183769941329956\n",
      "Training loss:1.6882082223892212\n",
      "Training loss:1.6478967666625977\n",
      "Training loss:1.755798101425171\n",
      "Training loss:1.747143030166626\n",
      "Training loss:1.7634210586547852\n",
      "Training loss:1.791689157485962\n",
      "Training loss:1.6905642747879028\n",
      "Training loss:1.719469428062439\n",
      "Training loss:1.7937726974487305\n",
      "Training loss:1.7413910627365112\n",
      "Training loss:1.7512750625610352\n",
      "Training loss:1.7358695268630981\n",
      "Training loss:1.6973470449447632\n",
      "Training loss:1.7397054433822632\n",
      "Training loss:1.7136690616607666\n",
      "Training loss:1.6409436464309692\n",
      "Training loss:1.8134945631027222\n",
      "Training loss:1.6878981590270996\n",
      "Training loss:1.7619749307632446\n",
      "Training loss:1.6511199474334717\n",
      "Training loss:1.7375017404556274\n",
      "Training loss:1.7255150079727173\n",
      "Training loss:1.6809486150741577\n",
      "Training loss:1.7238247394561768\n",
      "Training loss:1.742372751235962\n",
      "Training loss:1.7022135257720947\n",
      "Training loss:1.7454075813293457\n",
      "Training loss:1.752309799194336\n",
      "Training loss:1.698835849761963\n",
      "Training loss:1.7239081859588623\n",
      "Training loss:1.6594609022140503\n",
      "Training loss:1.7172386646270752\n",
      "Training loss:1.6612401008605957\n",
      "Training loss:1.778387427330017\n",
      "Training loss:1.7343305349349976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.7222282886505127\n",
      "Training loss:1.6124658584594727\n",
      "Training loss:1.7204468250274658\n",
      "Training loss:1.7402666807174683\n",
      "Training loss:1.7415169477462769\n",
      "Training loss:1.7470154762268066\n",
      "Training loss:1.8049489259719849\n",
      "Training loss:1.6778830289840698\n",
      "Training loss:1.6722393035888672\n",
      "Epoch: 0049 loss_train: 161.481598 acc_train: 0.360375 loss_val: 20.650436 acc_val: 0.364500 time: 124535.945037s\n",
      "Training loss:1.6573165655136108\n",
      "Training loss:1.6883054971694946\n",
      "Training loss:1.7211027145385742\n",
      "Training loss:1.6388895511627197\n",
      "Training loss:1.767038106918335\n",
      "Training loss:1.7156805992126465\n",
      "Training loss:1.7204194068908691\n",
      "Training loss:1.7153748273849487\n",
      "Training loss:1.7118046283721924\n",
      "Training loss:1.6873375177383423\n",
      "Training loss:1.726477026939392\n",
      "Training loss:1.6405065059661865\n",
      "Training loss:1.7282838821411133\n",
      "Training loss:1.7497578859329224\n",
      "Training loss:1.7353838682174683\n",
      "Training loss:1.7033839225769043\n",
      "Training loss:1.7461878061294556\n",
      "Training loss:1.7669974565505981\n",
      "Training loss:1.775549292564392\n",
      "Training loss:1.7030233144760132\n",
      "Training loss:1.7452138662338257\n",
      "Training loss:1.845994472503662\n",
      "Training loss:1.757206916809082\n",
      "Training loss:1.8128092288970947\n",
      "Training loss:1.7683674097061157\n",
      "Training loss:1.7522127628326416\n",
      "Training loss:1.7271924018859863\n",
      "Training loss:1.754999041557312\n",
      "Training loss:1.6989572048187256\n",
      "Training loss:1.7541112899780273\n",
      "Training loss:1.7515509128570557\n",
      "Training loss:1.7238516807556152\n",
      "Training loss:1.750923752784729\n",
      "Training loss:1.75049889087677\n",
      "Training loss:1.7460131645202637\n",
      "Training loss:1.7560840845108032\n",
      "Training loss:1.7434496879577637\n",
      "Training loss:1.806875228881836\n",
      "Training loss:1.7501765489578247\n",
      "Training loss:1.7379004955291748\n",
      "Training loss:1.6725295782089233\n",
      "Training loss:1.6963216066360474\n",
      "Training loss:1.7124454975128174\n",
      "Training loss:1.772881031036377\n",
      "Training loss:1.7229597568511963\n",
      "Training loss:1.7047995328903198\n",
      "Training loss:1.793021559715271\n",
      "Training loss:1.6855072975158691\n",
      "Training loss:1.7187111377716064\n",
      "Training loss:1.7295194864273071\n",
      "Training loss:1.7064235210418701\n",
      "Training loss:1.736289620399475\n",
      "Training loss:1.7458465099334717\n",
      "Training loss:1.7080037593841553\n",
      "Training loss:1.7277201414108276\n",
      "Training loss:1.7074319124221802\n",
      "Training loss:1.6854079961776733\n",
      "Training loss:1.7213712930679321\n",
      "Training loss:1.7332724332809448\n",
      "Training loss:1.7154325246810913\n",
      "Training loss:1.6739035844802856\n",
      "Training loss:1.7277894020080566\n",
      "Training loss:1.6701408624649048\n",
      "Training loss:1.6888785362243652\n",
      "Training loss:1.7187858819961548\n",
      "Training loss:1.7014247179031372\n",
      "Training loss:1.7726240158081055\n",
      "Training loss:1.6510347127914429\n",
      "Training loss:1.6809067726135254\n",
      "Training loss:1.684498906135559\n",
      "Training loss:1.7035835981369019\n",
      "Training loss:1.6504416465759277\n",
      "Training loss:1.7443771362304688\n",
      "Training loss:1.6545815467834473\n",
      "Training loss:1.7473623752593994\n",
      "Training loss:1.6837255954742432\n",
      "Training loss:1.642250895500183\n",
      "Training loss:1.7446508407592773\n",
      "Training loss:1.72707998752594\n",
      "Training loss:1.7202337980270386\n",
      "Training loss:1.7631582021713257\n",
      "Training loss:1.7334537506103516\n",
      "Training loss:1.7461622953414917\n",
      "Training loss:1.676592230796814\n",
      "Training loss:1.6883163452148438\n",
      "Training loss:1.6793080568313599\n",
      "Training loss:1.7750401496887207\n",
      "Training loss:1.7208592891693115\n",
      "Training loss:1.6404974460601807\n",
      "Training loss:1.7482142448425293\n",
      "Training loss:1.7078020572662354\n",
      "Training loss:1.8030403852462769\n",
      "Training loss:1.6376826763153076\n",
      "Training loss:1.751846432685852\n",
      "Epoch: 0050 loss_train: 161.889354 acc_train: 0.358083 loss_val: 20.365430 acc_val: 0.368333 time: 127031.403986s\n",
      "Training loss:1.684476375579834\n",
      "Training loss:1.7053169012069702\n",
      "Training loss:1.7536652088165283\n",
      "Training loss:1.6842420101165771\n",
      "Training loss:1.7249757051467896\n",
      "Training loss:1.7135792970657349\n",
      "Training loss:1.7402766942977905\n",
      "Training loss:1.6950839757919312\n",
      "Training loss:1.6483763456344604\n",
      "Training loss:1.7141872644424438\n",
      "Training loss:1.6346243619918823\n",
      "Training loss:1.7239018678665161\n",
      "Training loss:1.7160431146621704\n",
      "Training loss:1.7534751892089844\n",
      "Training loss:1.758543610572815\n",
      "Training loss:1.719456672668457\n",
      "Training loss:1.7028162479400635\n",
      "Training loss:1.7630144357681274\n",
      "Training loss:1.7048280239105225\n",
      "Training loss:1.7272700071334839\n",
      "Training loss:1.672279953956604\n",
      "Training loss:1.6676584482192993\n",
      "Training loss:1.7217833995819092\n",
      "Training loss:1.773257851600647\n",
      "Training loss:1.7392618656158447\n",
      "Training loss:1.7383098602294922\n",
      "Training loss:1.691249132156372\n",
      "Training loss:1.7465113401412964\n",
      "Training loss:1.7555760145187378\n",
      "Training loss:1.7652909755706787\n",
      "Training loss:1.6403745412826538\n",
      "Training loss:1.6886085271835327\n",
      "Training loss:1.7633140087127686\n",
      "Training loss:1.717750906944275\n",
      "Training loss:1.6486762762069702\n",
      "Training loss:1.7102949619293213\n",
      "Training loss:1.7008745670318604\n",
      "Training loss:1.7656164169311523\n",
      "Training loss:1.715382695198059\n",
      "Training loss:1.7089787721633911\n",
      "Training loss:1.6456151008605957\n",
      "Training loss:1.7118057012557983\n",
      "Training loss:1.6645898818969727\n",
      "Training loss:1.6583772897720337\n",
      "Training loss:1.7176105976104736\n",
      "Training loss:1.704582929611206\n",
      "Training loss:1.8053125143051147\n",
      "Training loss:1.7330652475357056\n",
      "Training loss:1.7330152988433838\n",
      "Training loss:1.8030807971954346\n",
      "Training loss:1.6329158544540405\n",
      "Training loss:1.7087336778640747\n",
      "Training loss:1.729414463043213\n",
      "Training loss:1.6656603813171387\n",
      "Training loss:1.6634507179260254\n",
      "Training loss:1.6722468137741089\n",
      "Training loss:1.7111996412277222\n",
      "Training loss:1.7818199396133423\n",
      "Training loss:1.6677583456039429\n",
      "Training loss:1.7568491697311401\n",
      "Training loss:1.6926277875900269\n",
      "Training loss:1.6862125396728516\n",
      "Training loss:1.7519041299819946\n",
      "Training loss:1.7133573293685913\n",
      "Training loss:1.7021677494049072\n",
      "Training loss:1.6478525400161743\n",
      "Training loss:1.6881221532821655\n",
      "Training loss:1.7179553508758545\n",
      "Training loss:1.7419500350952148\n",
      "Training loss:1.727821707725525\n",
      "Training loss:1.7562323808670044\n",
      "Training loss:1.735099196434021\n",
      "Training loss:1.6392008066177368\n",
      "Training loss:1.724344253540039\n",
      "Training loss:1.7287973165512085\n",
      "Training loss:1.7852636575698853\n",
      "Training loss:1.7242101430892944\n",
      "Training loss:1.6642545461654663\n",
      "Training loss:1.7014933824539185\n",
      "Training loss:1.669678807258606\n",
      "Training loss:1.6692019701004028\n",
      "Training loss:1.7088377475738525\n",
      "Training loss:1.7334299087524414\n",
      "Training loss:1.6874470710754395\n",
      "Training loss:1.8814762830734253\n",
      "Training loss:1.6691535711288452\n",
      "Training loss:1.67312753200531\n",
      "Training loss:1.7149425745010376\n",
      "Training loss:1.6916639804840088\n",
      "Training loss:1.7487009763717651\n",
      "Training loss:1.7133187055587769\n",
      "Training loss:1.665643334388733\n",
      "Training loss:1.6995593309402466\n",
      "Training loss:1.7454890012741089\n",
      "Epoch: 0051 loss_train: 160.966846 acc_train: 0.361667 loss_val: 20.394370 acc_val: 0.366000 time: 129520.564188s\n",
      "Training loss:1.6932587623596191\n",
      "Training loss:1.7249388694763184\n",
      "Training loss:1.6845126152038574\n",
      "Training loss:1.7010149955749512\n",
      "Training loss:1.756583333015442\n",
      "Training loss:1.7368686199188232\n",
      "Training loss:1.7034660577774048\n",
      "Training loss:1.6420118808746338\n",
      "Training loss:1.7340788841247559\n",
      "Training loss:1.7249926328659058\n",
      "Training loss:1.6977884769439697\n",
      "Training loss:1.752493143081665\n",
      "Training loss:1.6984456777572632\n",
      "Training loss:1.7039179801940918\n",
      "Training loss:1.7938238382339478\n",
      "Training loss:1.7425084114074707\n",
      "Training loss:1.7531338930130005\n",
      "Training loss:1.692594289779663\n",
      "Training loss:1.6854884624481201\n",
      "Training loss:1.7485650777816772\n",
      "Training loss:1.6581119298934937\n",
      "Training loss:1.6806272268295288\n",
      "Training loss:1.6649363040924072\n",
      "Training loss:1.7264659404754639\n",
      "Training loss:1.7015098333358765\n",
      "Training loss:1.7035187482833862\n",
      "Training loss:1.7741841077804565\n",
      "Training loss:1.660898208618164\n",
      "Training loss:1.7277061939239502\n",
      "Training loss:1.6707379817962646\n",
      "Training loss:1.7176910638809204\n",
      "Training loss:1.7909661531448364\n",
      "Training loss:1.707473635673523\n",
      "Training loss:1.7714465856552124\n",
      "Training loss:1.691677451133728\n",
      "Training loss:1.6067585945129395\n",
      "Training loss:1.7109593152999878\n",
      "Training loss:1.7082148790359497\n",
      "Training loss:1.7298527956008911\n",
      "Training loss:1.6665645837783813\n",
      "Training loss:1.7111328840255737\n",
      "Training loss:1.7382341623306274\n",
      "Training loss:1.7593321800231934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6949974298477173\n",
      "Training loss:1.6452974081039429\n",
      "Training loss:1.736873745918274\n",
      "Training loss:1.6647310256958008\n",
      "Training loss:1.648713231086731\n",
      "Training loss:1.713025689125061\n",
      "Training loss:1.7705507278442383\n",
      "Training loss:1.6930943727493286\n",
      "Training loss:1.6786220073699951\n",
      "Training loss:1.6987481117248535\n",
      "Training loss:1.7782574892044067\n",
      "Training loss:1.7847987413406372\n",
      "Training loss:1.6871474981307983\n",
      "Training loss:1.6980187892913818\n",
      "Training loss:1.6841363906860352\n",
      "Training loss:1.7081979513168335\n",
      "Training loss:1.7870218753814697\n",
      "Training loss:1.7005252838134766\n",
      "Training loss:1.727810263633728\n",
      "Training loss:1.6627026796340942\n",
      "Training loss:1.6704776287078857\n",
      "Training loss:1.7654820680618286\n",
      "Training loss:1.7520325183868408\n",
      "Training loss:1.657733678817749\n",
      "Training loss:1.6782715320587158\n",
      "Training loss:1.7008004188537598\n",
      "Training loss:1.6709879636764526\n",
      "Training loss:1.720330834388733\n",
      "Training loss:1.6591007709503174\n",
      "Training loss:1.7165580987930298\n",
      "Training loss:1.7131129503250122\n",
      "Training loss:1.7539938688278198\n",
      "Training loss:1.6991726160049438\n",
      "Training loss:1.7052578926086426\n",
      "Training loss:1.733277440071106\n",
      "Training loss:1.6786812543869019\n",
      "Training loss:1.690316915512085\n",
      "Training loss:1.7010488510131836\n",
      "Training loss:1.7640841007232666\n",
      "Training loss:1.7341580390930176\n",
      "Training loss:1.725419521331787\n",
      "Training loss:1.6664692163467407\n",
      "Training loss:1.7081432342529297\n",
      "Training loss:1.7070802450180054\n",
      "Training loss:1.727548360824585\n",
      "Training loss:1.720214605331421\n",
      "Training loss:1.6495375633239746\n",
      "Training loss:1.729693055152893\n",
      "Training loss:1.7403254508972168\n",
      "Training loss:1.6785112619400024\n",
      "Training loss:1.6743413209915161\n",
      "Epoch: 0052 loss_train: 160.704921 acc_train: 0.364667 loss_val: 20.372823 acc_val: 0.370333 time: 132009.602896s\n",
      "Training loss:1.7291897535324097\n",
      "Training loss:1.6842200756072998\n",
      "Training loss:1.7333672046661377\n",
      "Training loss:1.6951251029968262\n",
      "Training loss:1.713024377822876\n",
      "Training loss:1.7376277446746826\n",
      "Training loss:1.6847753524780273\n",
      "Training loss:1.6939003467559814\n",
      "Training loss:1.752023458480835\n",
      "Training loss:1.6616487503051758\n",
      "Training loss:1.7426140308380127\n",
      "Training loss:1.6558077335357666\n",
      "Training loss:1.6896611452102661\n",
      "Training loss:1.806947946548462\n",
      "Training loss:1.704651117324829\n",
      "Training loss:1.774275541305542\n",
      "Training loss:1.7547787427902222\n",
      "Training loss:1.6955646276474\n",
      "Training loss:1.8056737184524536\n",
      "Training loss:1.7734391689300537\n",
      "Training loss:1.7652430534362793\n",
      "Training loss:1.735887885093689\n",
      "Training loss:1.741281509399414\n",
      "Training loss:1.7411365509033203\n",
      "Training loss:1.7817256450653076\n",
      "Training loss:1.6662640571594238\n",
      "Training loss:1.6754474639892578\n",
      "Training loss:1.7316099405288696\n",
      "Training loss:1.7369756698608398\n",
      "Training loss:1.7003190517425537\n",
      "Training loss:1.7022897005081177\n",
      "Training loss:1.6989396810531616\n",
      "Training loss:1.645356297492981\n",
      "Training loss:1.759702205657959\n",
      "Training loss:1.7353801727294922\n",
      "Training loss:1.7933813333511353\n",
      "Training loss:1.7522296905517578\n",
      "Training loss:1.7273035049438477\n",
      "Training loss:1.6980966329574585\n",
      "Training loss:1.68984055519104\n",
      "Training loss:1.7328029870986938\n",
      "Training loss:1.6896697282791138\n",
      "Training loss:1.7268025875091553\n",
      "Training loss:1.7042267322540283\n",
      "Training loss:1.6661230325698853\n",
      "Training loss:1.7311598062515259\n",
      "Training loss:1.728301763534546\n",
      "Training loss:1.6946160793304443\n",
      "Training loss:1.6082223653793335\n",
      "Training loss:1.6771094799041748\n",
      "Training loss:1.6790913343429565\n",
      "Training loss:1.733358383178711\n",
      "Training loss:1.8068349361419678\n",
      "Training loss:1.767094612121582\n",
      "Training loss:1.7231999635696411\n",
      "Training loss:1.6588687896728516\n",
      "Training loss:1.7270567417144775\n",
      "Training loss:1.7351877689361572\n",
      "Training loss:1.7480446100234985\n",
      "Training loss:1.7172433137893677\n",
      "Training loss:1.7214550971984863\n",
      "Training loss:1.7528629302978516\n",
      "Training loss:1.6954481601715088\n",
      "Training loss:1.6348495483398438\n",
      "Training loss:1.7068884372711182\n",
      "Training loss:1.6685924530029297\n",
      "Training loss:1.66166090965271\n",
      "Training loss:1.672491192817688\n",
      "Training loss:1.7349172830581665\n",
      "Training loss:1.7245063781738281\n",
      "Training loss:1.7286325693130493\n",
      "Training loss:1.7066982984542847\n",
      "Training loss:1.7608038187026978\n",
      "Training loss:1.6482869386672974\n",
      "Training loss:1.6844156980514526\n",
      "Training loss:1.66952645778656\n",
      "Training loss:1.626354455947876\n",
      "Training loss:1.694823980331421\n",
      "Training loss:1.647830843925476\n",
      "Training loss:1.7497018575668335\n",
      "Training loss:1.715494155883789\n",
      "Training loss:1.677324891090393\n",
      "Training loss:1.7791517972946167\n",
      "Training loss:1.7595247030258179\n",
      "Training loss:1.6118643283843994\n",
      "Training loss:1.6791892051696777\n",
      "Training loss:1.7034348249435425\n",
      "Training loss:1.6770204305648804\n",
      "Training loss:1.7814546823501587\n",
      "Training loss:1.7120108604431152\n",
      "Training loss:1.762198805809021\n",
      "Training loss:1.689774751663208\n",
      "Training loss:1.6687829494476318\n",
      "Training loss:1.7329899072647095\n",
      "Epoch: 0053 loss_train: 161.062705 acc_train: 0.361729 loss_val: 20.462400 acc_val: 0.362333 time: 134494.701531s\n",
      "Training loss:1.694324016571045\n",
      "Training loss:1.7363280057907104\n",
      "Training loss:1.7261369228363037\n",
      "Training loss:1.7543967962265015\n",
      "Training loss:1.7475700378417969\n",
      "Training loss:1.645666241645813\n",
      "Training loss:1.7369641065597534\n",
      "Training loss:1.702874779701233\n",
      "Training loss:1.663722038269043\n",
      "Training loss:1.6475902795791626\n",
      "Training loss:1.6484465599060059\n",
      "Training loss:1.7035363912582397\n",
      "Training loss:1.7591596841812134\n",
      "Training loss:1.7334649562835693\n",
      "Training loss:1.6322327852249146\n",
      "Training loss:1.7516870498657227\n",
      "Training loss:1.7017817497253418\n",
      "Training loss:1.6956329345703125\n",
      "Training loss:1.6814601421356201\n",
      "Training loss:1.6524784564971924\n",
      "Training loss:1.6916736364364624\n",
      "Training loss:1.6986956596374512\n",
      "Training loss:1.6677873134613037\n",
      "Training loss:1.6402026414871216\n",
      "Training loss:1.8055838346481323\n",
      "Training loss:1.7202425003051758\n",
      "Training loss:1.7110968828201294\n",
      "Training loss:1.6802750825881958\n",
      "Training loss:1.6756459474563599\n",
      "Training loss:1.6969871520996094\n",
      "Training loss:1.685154676437378\n",
      "Training loss:1.7303798198699951\n",
      "Training loss:1.6566674709320068\n",
      "Training loss:1.7163417339324951\n",
      "Training loss:1.7291922569274902\n",
      "Training loss:1.6668840646743774\n",
      "Training loss:1.6440788507461548\n",
      "Training loss:1.6642035245895386\n",
      "Training loss:1.6755369901657104\n",
      "Training loss:1.7479594945907593\n",
      "Training loss:1.7966406345367432\n",
      "Training loss:1.6328086853027344\n",
      "Training loss:1.7281522750854492\n",
      "Training loss:1.7243765592575073\n",
      "Training loss:1.7317924499511719\n",
      "Training loss:1.7484935522079468\n",
      "Training loss:1.6673738956451416\n",
      "Training loss:1.6639255285263062\n",
      "Training loss:1.8192089796066284\n",
      "Training loss:1.74690580368042\n",
      "Training loss:1.6739014387130737\n",
      "Training loss:1.6803375482559204\n",
      "Training loss:1.742983341217041\n",
      "Training loss:1.7528597116470337\n",
      "Training loss:1.6860320568084717\n",
      "Training loss:1.7195041179656982\n",
      "Training loss:1.7077300548553467\n",
      "Training loss:1.7370070219039917\n",
      "Training loss:1.6611316204071045\n",
      "Training loss:1.6444008350372314\n",
      "Training loss:1.6892036199569702\n",
      "Training loss:1.6998622417449951\n",
      "Training loss:1.7268455028533936\n",
      "Training loss:1.8096668720245361\n",
      "Training loss:1.682106375694275\n",
      "Training loss:1.658328890800476\n",
      "Training loss:1.716863989830017\n",
      "Training loss:1.746924877166748\n",
      "Training loss:1.7141482830047607\n",
      "Training loss:1.6495691537857056\n",
      "Training loss:1.7175235748291016\n",
      "Training loss:1.7460349798202515\n",
      "Training loss:1.6974891424179077\n",
      "Training loss:1.7536611557006836\n",
      "Training loss:1.7141447067260742\n",
      "Training loss:1.697034239768982\n",
      "Training loss:1.7202212810516357\n",
      "Training loss:1.7233638763427734\n",
      "Training loss:1.7135225534439087\n",
      "Training loss:1.70639967918396\n",
      "Training loss:1.6952799558639526\n",
      "Training loss:1.7494676113128662\n",
      "Training loss:1.7226064205169678\n",
      "Training loss:1.6794278621673584\n",
      "Training loss:1.766029715538025\n",
      "Training loss:1.6929419040679932\n",
      "Training loss:1.7011555433273315\n",
      "Training loss:1.6469897031784058\n",
      "Training loss:1.7885531187057495\n",
      "Training loss:1.6843281984329224\n",
      "Training loss:1.6378356218338013\n",
      "Training loss:1.7550914287567139\n",
      "Training loss:1.7296249866485596\n",
      "Training loss:1.752001404762268\n",
      "Epoch: 0054 loss_train: 160.469858 acc_train: 0.365792 loss_val: 20.292985 acc_val: 0.366667 time: 136984.270459s\n",
      "Training loss:1.6932727098464966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.7439277172088623\n",
      "Training loss:1.714307188987732\n",
      "Training loss:1.7713065147399902\n",
      "Training loss:1.762839674949646\n",
      "Training loss:1.6822030544281006\n",
      "Training loss:1.6382150650024414\n",
      "Training loss:1.7354918718338013\n",
      "Training loss:1.6548893451690674\n",
      "Training loss:1.6972405910491943\n",
      "Training loss:1.7574622631072998\n",
      "Training loss:1.6797674894332886\n",
      "Training loss:1.6772938966751099\n",
      "Training loss:1.747673749923706\n",
      "Training loss:1.8327797651290894\n",
      "Training loss:1.7114723920822144\n",
      "Training loss:1.7217148542404175\n",
      "Training loss:1.707338571548462\n",
      "Training loss:1.717638373374939\n",
      "Training loss:1.7352495193481445\n",
      "Training loss:1.7137595415115356\n",
      "Training loss:1.6600065231323242\n",
      "Training loss:1.7157460451126099\n",
      "Training loss:1.6781822443008423\n",
      "Training loss:1.6479841470718384\n",
      "Training loss:1.7346025705337524\n",
      "Training loss:1.6920461654663086\n",
      "Training loss:1.6805229187011719\n",
      "Training loss:1.7444653511047363\n",
      "Training loss:1.646362543106079\n",
      "Training loss:1.742649793624878\n",
      "Training loss:1.6905591487884521\n",
      "Training loss:1.765759825706482\n",
      "Training loss:1.7284610271453857\n",
      "Training loss:1.7504981756210327\n",
      "Training loss:1.7493852376937866\n",
      "Training loss:1.6986719369888306\n",
      "Training loss:1.6590806245803833\n",
      "Training loss:1.6713299751281738\n",
      "Training loss:1.7281770706176758\n",
      "Training loss:1.5971816778182983\n",
      "Training loss:1.6775035858154297\n",
      "Training loss:1.771621584892273\n",
      "Training loss:1.7203558683395386\n",
      "Training loss:1.7045127153396606\n",
      "Training loss:1.7072441577911377\n",
      "Training loss:1.7287843227386475\n",
      "Training loss:1.7761818170547485\n",
      "Training loss:1.7316960096359253\n",
      "Training loss:1.7587271928787231\n",
      "Training loss:1.689422845840454\n",
      "Training loss:1.7147252559661865\n",
      "Training loss:1.7267190217971802\n",
      "Training loss:1.7458349466323853\n",
      "Training loss:1.7560410499572754\n",
      "Training loss:1.7049881219863892\n",
      "Training loss:1.7516498565673828\n",
      "Training loss:1.720703125\n",
      "Training loss:1.7034046649932861\n",
      "Training loss:1.781538486480713\n",
      "Training loss:1.7243465185165405\n",
      "Training loss:1.7368015050888062\n",
      "Training loss:1.705078363418579\n",
      "Training loss:1.7077637910842896\n",
      "Training loss:1.6677188873291016\n",
      "Training loss:1.6894478797912598\n",
      "Training loss:1.6766942739486694\n",
      "Training loss:1.7464475631713867\n",
      "Training loss:1.653924584388733\n",
      "Training loss:1.7123850584030151\n",
      "Training loss:1.7647695541381836\n",
      "Training loss:1.7392016649246216\n",
      "Training loss:1.7023564577102661\n",
      "Training loss:1.6239110231399536\n",
      "Training loss:1.629781723022461\n",
      "Training loss:1.7294608354568481\n",
      "Training loss:1.6808891296386719\n",
      "Training loss:1.68580961227417\n",
      "Training loss:1.7147047519683838\n",
      "Training loss:1.805370807647705\n",
      "Training loss:1.7042497396469116\n",
      "Training loss:1.6809078454971313\n",
      "Training loss:1.7344954013824463\n",
      "Training loss:1.7263529300689697\n",
      "Training loss:1.6866791248321533\n",
      "Training loss:1.6792144775390625\n",
      "Training loss:1.6677790880203247\n",
      "Training loss:1.7412185668945312\n",
      "Training loss:1.654505729675293\n",
      "Training loss:1.6921100616455078\n",
      "Training loss:1.649954915046692\n",
      "Training loss:1.5857818126678467\n",
      "Training loss:1.682877779006958\n",
      "Training loss:1.7356657981872559\n",
      "Epoch: 0055 loss_train: 160.667829 acc_train: 0.364438 loss_val: 20.284592 acc_val: 0.368167 time: 139466.628002s\n",
      "Training loss:1.6649998426437378\n",
      "Training loss:1.7199182510375977\n",
      "Training loss:1.6888222694396973\n",
      "Training loss:1.691998839378357\n",
      "Training loss:1.6911547183990479\n",
      "Training loss:1.7534886598587036\n",
      "Training loss:1.6080000400543213\n",
      "Training loss:1.6769825220108032\n",
      "Training loss:1.652838110923767\n",
      "Training loss:1.6473677158355713\n",
      "Training loss:1.7114291191101074\n",
      "Training loss:1.6553832292556763\n",
      "Training loss:1.6909576654434204\n",
      "Training loss:1.6195951700210571\n",
      "Training loss:1.6469799280166626\n",
      "Training loss:1.6628978252410889\n",
      "Training loss:1.7894951105117798\n",
      "Training loss:1.6937683820724487\n",
      "Training loss:1.671242117881775\n",
      "Training loss:1.747393012046814\n",
      "Training loss:1.5975253582000732\n",
      "Training loss:1.7067902088165283\n",
      "Training loss:1.7373937368392944\n",
      "Training loss:1.733947992324829\n",
      "Training loss:1.631614327430725\n",
      "Training loss:1.6967589855194092\n",
      "Training loss:1.7589614391326904\n",
      "Training loss:1.7819269895553589\n",
      "Training loss:1.7077723741531372\n",
      "Training loss:1.7126507759094238\n",
      "Training loss:1.7386770248413086\n",
      "Training loss:1.7260990142822266\n",
      "Training loss:1.6822198629379272\n",
      "Training loss:1.6995742321014404\n",
      "Training loss:1.6733115911483765\n",
      "Training loss:1.6441402435302734\n",
      "Training loss:1.7257187366485596\n",
      "Training loss:1.6923496723175049\n",
      "Training loss:1.6493555307388306\n",
      "Training loss:1.7319767475128174\n",
      "Training loss:1.704271674156189\n",
      "Training loss:1.691507339477539\n",
      "Training loss:1.7333652973175049\n",
      "Training loss:1.7654615640640259\n",
      "Training loss:1.7257798910140991\n",
      "Training loss:1.794158935546875\n",
      "Training loss:1.7255274057388306\n",
      "Training loss:1.6897087097167969\n",
      "Training loss:1.6639418601989746\n",
      "Training loss:1.74679696559906\n",
      "Training loss:1.6980301141738892\n",
      "Training loss:1.6283886432647705\n",
      "Training loss:1.6803789138793945\n",
      "Training loss:1.6847912073135376\n",
      "Training loss:1.7258870601654053\n",
      "Training loss:1.7390154600143433\n",
      "Training loss:1.6850656270980835\n",
      "Training loss:1.743813395500183\n",
      "Training loss:1.7314854860305786\n",
      "Training loss:1.7520012855529785\n",
      "Training loss:1.706422209739685\n",
      "Training loss:1.6793384552001953\n",
      "Training loss:1.766223669052124\n",
      "Training loss:1.6435904502868652\n",
      "Training loss:1.7755768299102783\n",
      "Training loss:1.7409768104553223\n",
      "Training loss:1.6973918676376343\n",
      "Training loss:1.8506507873535156\n",
      "Training loss:1.6742613315582275\n",
      "Training loss:1.7150506973266602\n",
      "Training loss:1.773832082748413\n",
      "Training loss:1.6115102767944336\n",
      "Training loss:1.7168577909469604\n",
      "Training loss:1.8037182092666626\n",
      "Training loss:1.7044063806533813\n",
      "Training loss:1.7038254737854004\n",
      "Training loss:1.7815370559692383\n",
      "Training loss:1.6807506084442139\n",
      "Training loss:1.8039343357086182\n",
      "Training loss:1.759255051612854\n",
      "Training loss:1.7417645454406738\n",
      "Training loss:1.794501543045044\n",
      "Training loss:1.7573521137237549\n",
      "Training loss:1.6587367057800293\n",
      "Training loss:1.7503489255905151\n",
      "Training loss:1.6795611381530762\n",
      "Training loss:1.6876848936080933\n",
      "Training loss:1.7263973951339722\n",
      "Training loss:1.8163306713104248\n",
      "Training loss:1.6913673877716064\n",
      "Training loss:1.6529780626296997\n",
      "Training loss:1.7060365676879883\n",
      "Training loss:1.689055323600769\n",
      "Training loss:1.7794300317764282\n",
      "Epoch: 0056 loss_train: 160.743510 acc_train: 0.364958 loss_val: 20.414924 acc_val: 0.370000 time: 141951.857644s\n",
      "Training loss:1.727097511291504\n",
      "Training loss:1.6932893991470337\n",
      "Training loss:1.6798533201217651\n",
      "Training loss:1.6958417892456055\n",
      "Training loss:1.7770994901657104\n",
      "Training loss:1.6694906949996948\n",
      "Training loss:1.6753133535385132\n",
      "Training loss:1.713235855102539\n",
      "Training loss:1.664035439491272\n",
      "Training loss:1.7106812000274658\n",
      "Training loss:1.6925679445266724\n",
      "Training loss:1.7615495920181274\n",
      "Training loss:1.6767334938049316\n",
      "Training loss:1.7206528186798096\n",
      "Training loss:1.642229437828064\n",
      "Training loss:1.672625184059143\n",
      "Training loss:1.741632342338562\n",
      "Training loss:1.7988789081573486\n",
      "Training loss:1.700738549232483\n",
      "Training loss:1.6729668378829956\n",
      "Training loss:1.6703327894210815\n",
      "Training loss:1.6854259967803955\n",
      "Training loss:1.7200785875320435\n",
      "Training loss:1.693227767944336\n",
      "Training loss:1.672098159790039\n",
      "Training loss:1.7402043342590332\n",
      "Training loss:1.7274596691131592\n",
      "Training loss:1.7180867195129395\n",
      "Training loss:1.7461110353469849\n",
      "Training loss:1.705728530883789\n",
      "Training loss:1.7727617025375366\n",
      "Training loss:1.690894365310669\n",
      "Training loss:1.7167123556137085\n",
      "Training loss:1.6948902606964111\n",
      "Training loss:1.6524631977081299\n",
      "Training loss:1.6622321605682373\n",
      "Training loss:1.6713082790374756\n",
      "Training loss:1.6646190881729126\n",
      "Training loss:1.771268367767334\n",
      "Training loss:1.732908844947815\n",
      "Training loss:1.6786541938781738\n",
      "Training loss:1.7305257320404053\n",
      "Training loss:1.8046998977661133\n",
      "Training loss:1.7417320013046265\n",
      "Training loss:1.7530971765518188\n",
      "Training loss:1.7055991888046265\n",
      "Training loss:1.6956613063812256\n",
      "Training loss:1.7630674839019775\n",
      "Training loss:1.7135196924209595\n",
      "Training loss:1.6677926778793335\n",
      "Training loss:1.7293328046798706\n",
      "Training loss:1.6963450908660889\n",
      "Training loss:1.709591031074524\n",
      "Training loss:1.6773505210876465\n",
      "Training loss:1.7188873291015625\n",
      "Training loss:1.6908512115478516\n",
      "Training loss:1.7378267049789429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6436645984649658\n",
      "Training loss:1.6054803133010864\n",
      "Training loss:1.6527025699615479\n",
      "Training loss:1.7589393854141235\n",
      "Training loss:1.6112396717071533\n",
      "Training loss:1.6591581106185913\n",
      "Training loss:1.7315657138824463\n",
      "Training loss:1.6852201223373413\n",
      "Training loss:1.69596266746521\n",
      "Training loss:1.6534943580627441\n",
      "Training loss:1.6600608825683594\n",
      "Training loss:1.773550033569336\n",
      "Training loss:1.6815950870513916\n",
      "Training loss:1.6546146869659424\n",
      "Training loss:1.652274250984192\n",
      "Training loss:1.7032759189605713\n",
      "Training loss:1.728334903717041\n",
      "Training loss:1.6547688245773315\n",
      "Training loss:1.7569043636322021\n",
      "Training loss:1.71915602684021\n",
      "Training loss:1.7764256000518799\n",
      "Training loss:1.7702579498291016\n",
      "Training loss:1.646281361579895\n",
      "Training loss:1.6676805019378662\n",
      "Training loss:1.6766213178634644\n",
      "Training loss:1.6700632572174072\n",
      "Training loss:1.6985419988632202\n",
      "Training loss:1.7084848880767822\n",
      "Training loss:1.719857931137085\n",
      "Training loss:1.6957950592041016\n",
      "Training loss:1.7778056859970093\n",
      "Training loss:1.7518689632415771\n",
      "Training loss:1.715463399887085\n",
      "Training loss:1.6811028718948364\n",
      "Training loss:1.735991358757019\n",
      "Training loss:1.658732533454895\n",
      "Training loss:1.6421741247177124\n",
      "Epoch: 0057 loss_train: 160.084971 acc_train: 0.368333 loss_val: 20.359169 acc_val: 0.365167 time: 144432.213485s\n",
      "Training loss:1.7261825799942017\n",
      "Training loss:1.6999849081039429\n",
      "Training loss:1.6687438488006592\n",
      "Training loss:1.7287923097610474\n",
      "Training loss:1.675270676612854\n",
      "Training loss:1.6788642406463623\n",
      "Training loss:1.6901843547821045\n",
      "Training loss:1.7079097032546997\n",
      "Training loss:1.6557518243789673\n",
      "Training loss:1.7061904668807983\n",
      "Training loss:1.6691077947616577\n",
      "Training loss:1.7370718717575073\n",
      "Training loss:1.7763277292251587\n",
      "Training loss:1.7607543468475342\n",
      "Training loss:1.6732161045074463\n",
      "Training loss:1.6039035320281982\n",
      "Training loss:1.685386061668396\n",
      "Training loss:1.7006828784942627\n",
      "Training loss:1.7700749635696411\n",
      "Training loss:1.6545486450195312\n",
      "Training loss:1.7294236421585083\n",
      "Training loss:1.7308168411254883\n",
      "Training loss:1.7509113550186157\n",
      "Training loss:1.6531147956848145\n",
      "Training loss:1.6219693422317505\n",
      "Training loss:1.6968921422958374\n",
      "Training loss:1.706149697303772\n",
      "Training loss:1.6729570627212524\n",
      "Training loss:1.7602512836456299\n",
      "Training loss:1.7090983390808105\n",
      "Training loss:1.8101160526275635\n",
      "Training loss:1.7505162954330444\n",
      "Training loss:1.6967154741287231\n",
      "Training loss:1.6774766445159912\n",
      "Training loss:1.709132194519043\n",
      "Training loss:1.667667031288147\n",
      "Training loss:1.6868910789489746\n",
      "Training loss:1.665989637374878\n",
      "Training loss:1.7075284719467163\n",
      "Training loss:1.6521577835083008\n",
      "Training loss:1.7220158576965332\n",
      "Training loss:1.6946293115615845\n",
      "Training loss:1.600468635559082\n",
      "Training loss:1.730904221534729\n",
      "Training loss:1.6873464584350586\n",
      "Training loss:1.7432045936584473\n",
      "Training loss:1.6585924625396729\n",
      "Training loss:1.6320984363555908\n",
      "Training loss:1.7285196781158447\n",
      "Training loss:1.6818041801452637\n",
      "Training loss:1.65220046043396\n",
      "Training loss:1.7240601778030396\n",
      "Training loss:1.6713207960128784\n",
      "Training loss:1.713004231452942\n",
      "Training loss:1.7054632902145386\n",
      "Training loss:1.7059743404388428\n",
      "Training loss:1.695176124572754\n",
      "Training loss:1.7401978969573975\n",
      "Training loss:1.6973271369934082\n",
      "Training loss:1.6767592430114746\n",
      "Training loss:1.6653498411178589\n",
      "Training loss:1.693769931793213\n",
      "Training loss:1.7699509859085083\n",
      "Training loss:1.6973457336425781\n",
      "Training loss:1.6805815696716309\n",
      "Training loss:1.686753749847412\n",
      "Training loss:1.6842955350875854\n",
      "Training loss:1.6747214794158936\n",
      "Training loss:1.7784550189971924\n",
      "Training loss:1.6954834461212158\n",
      "Training loss:1.7398312091827393\n",
      "Training loss:1.6742421388626099\n",
      "Training loss:1.6878869533538818\n",
      "Training loss:1.6830663681030273\n",
      "Training loss:1.6156855821609497\n",
      "Training loss:1.6815334558486938\n",
      "Training loss:1.7094100713729858\n",
      "Training loss:1.7142361402511597\n",
      "Training loss:1.693285346031189\n",
      "Training loss:1.8090085983276367\n",
      "Training loss:1.6771687269210815\n",
      "Training loss:1.706884741783142\n",
      "Training loss:1.7179391384124756\n",
      "Training loss:1.6760576963424683\n",
      "Training loss:1.730774164199829\n",
      "Training loss:1.7606743574142456\n",
      "Training loss:1.6498291492462158\n",
      "Training loss:1.699761986732483\n",
      "Training loss:1.7363998889923096\n",
      "Training loss:1.7244120836257935\n",
      "Training loss:1.721672534942627\n",
      "Training loss:1.593858003616333\n",
      "Training loss:1.7439708709716797\n",
      "Training loss:1.6550480127334595\n",
      "Epoch: 0058 loss_train: 159.715136 acc_train: 0.369458 loss_val: 20.241931 acc_val: 0.372333 time: 146921.559058s\n",
      "Training loss:1.6678669452667236\n",
      "Training loss:1.7078171968460083\n",
      "Training loss:1.721638798713684\n",
      "Training loss:1.6467913389205933\n",
      "Training loss:1.7104531526565552\n",
      "Training loss:1.6487194299697876\n",
      "Training loss:1.707342267036438\n",
      "Training loss:1.6580103635787964\n",
      "Training loss:1.6827831268310547\n",
      "Training loss:1.721146821975708\n",
      "Training loss:1.7024242877960205\n",
      "Training loss:1.6461199522018433\n",
      "Training loss:1.773700475692749\n",
      "Training loss:1.6868312358856201\n",
      "Training loss:1.6966687440872192\n",
      "Training loss:1.718064308166504\n",
      "Training loss:1.6876271963119507\n",
      "Training loss:1.6998140811920166\n",
      "Training loss:1.7068220376968384\n",
      "Training loss:1.7215970754623413\n",
      "Training loss:1.7613495588302612\n",
      "Training loss:1.658488154411316\n",
      "Training loss:1.6931244134902954\n",
      "Training loss:1.682320475578308\n",
      "Training loss:1.7192476987838745\n",
      "Training loss:1.7221848964691162\n",
      "Training loss:1.6638679504394531\n",
      "Training loss:1.7283459901809692\n",
      "Training loss:1.7274069786071777\n",
      "Training loss:1.6599959135055542\n",
      "Training loss:1.6878834962844849\n",
      "Training loss:1.656679630279541\n",
      "Training loss:1.6293009519577026\n",
      "Training loss:1.7544493675231934\n",
      "Training loss:1.6573052406311035\n",
      "Training loss:1.6622077226638794\n",
      "Training loss:1.734636664390564\n",
      "Training loss:1.6924147605895996\n",
      "Training loss:1.8124557733535767\n",
      "Training loss:1.728486180305481\n",
      "Training loss:1.713683843612671\n",
      "Training loss:1.601226806640625\n",
      "Training loss:1.6185280084609985\n",
      "Training loss:1.7656503915786743\n",
      "Training loss:1.6716097593307495\n",
      "Training loss:1.6771589517593384\n",
      "Training loss:1.6618216037750244\n",
      "Training loss:1.7268106937408447\n",
      "Training loss:1.6526356935501099\n",
      "Training loss:1.6927706003189087\n",
      "Training loss:1.6613496541976929\n",
      "Training loss:1.717072606086731\n",
      "Training loss:1.7016541957855225\n",
      "Training loss:1.7274748086929321\n",
      "Training loss:1.744762897491455\n",
      "Training loss:1.6796302795410156\n",
      "Training loss:1.7035796642303467\n",
      "Training loss:1.7248958349227905\n",
      "Training loss:1.6724679470062256\n",
      "Training loss:1.731144905090332\n",
      "Training loss:1.7920056581497192\n",
      "Training loss:1.667211890220642\n",
      "Training loss:1.7649385929107666\n",
      "Training loss:1.7354226112365723\n",
      "Training loss:1.7588911056518555\n",
      "Training loss:1.7059195041656494\n",
      "Training loss:1.7709810733795166\n",
      "Training loss:1.76479971408844\n",
      "Training loss:1.6819446086883545\n",
      "Training loss:1.684852957725525\n",
      "Training loss:1.7727599143981934\n",
      "Training loss:1.6712051630020142\n",
      "Training loss:1.6906017065048218\n",
      "Training loss:1.6615326404571533\n",
      "Training loss:1.6512218713760376\n",
      "Training loss:1.7003616094589233\n",
      "Training loss:1.6525695323944092\n",
      "Training loss:1.700899362564087\n",
      "Training loss:1.6940257549285889\n",
      "Training loss:1.752930760383606\n",
      "Training loss:1.740003228187561\n",
      "Training loss:1.7963987588882446\n",
      "Training loss:1.7133991718292236\n",
      "Training loss:1.7417662143707275\n",
      "Training loss:1.6812885999679565\n",
      "Training loss:1.744890570640564\n",
      "Training loss:1.7709534168243408\n",
      "Training loss:1.7126504182815552\n",
      "Training loss:1.6789443492889404\n",
      "Training loss:1.7427289485931396\n",
      "Training loss:1.752379059791565\n",
      "Training loss:1.648795485496521\n",
      "Training loss:1.670055627822876\n",
      "Training loss:1.7655935287475586\n",
      "Epoch: 0059 loss_train: 160.225243 acc_train: 0.369833 loss_val: 20.363770 acc_val: 0.366000 time: 149408.521757s\n",
      "Training loss:1.7585471868515015\n",
      "Training loss:1.7201948165893555\n",
      "Training loss:1.8310269117355347\n",
      "Training loss:1.7508760690689087\n",
      "Training loss:1.7180758714675903\n",
      "Training loss:1.7139182090759277\n",
      "Training loss:1.728278636932373\n",
      "Training loss:1.644753336906433\n",
      "Training loss:1.6197866201400757\n",
      "Training loss:1.778020977973938\n",
      "Training loss:1.6461642980575562\n",
      "Training loss:1.7102367877960205\n",
      "Training loss:1.6686091423034668\n",
      "Training loss:1.6572766304016113\n",
      "Training loss:1.6525155305862427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6735520362854004\n",
      "Training loss:1.678836464881897\n",
      "Training loss:1.6662566661834717\n",
      "Training loss:1.6927508115768433\n",
      "Training loss:1.6471083164215088\n",
      "Training loss:1.7764536142349243\n",
      "Training loss:1.632776141166687\n",
      "Training loss:1.7040280103683472\n",
      "Training loss:1.6764609813690186\n",
      "Training loss:1.716145396232605\n",
      "Training loss:1.6734739542007446\n",
      "Training loss:1.6881145238876343\n",
      "Training loss:1.729840636253357\n",
      "Training loss:1.7174636125564575\n",
      "Training loss:1.689283847808838\n",
      "Training loss:1.7780194282531738\n",
      "Training loss:1.7422237396240234\n",
      "Training loss:1.6505008935928345\n",
      "Training loss:1.683171033859253\n",
      "Training loss:1.6657806634902954\n",
      "Training loss:1.6975115537643433\n",
      "Training loss:1.6456034183502197\n",
      "Training loss:1.6979188919067383\n",
      "Training loss:1.6391575336456299\n",
      "Training loss:1.6357301473617554\n",
      "Training loss:1.6802486181259155\n",
      "Training loss:1.6920489072799683\n",
      "Training loss:1.701590895652771\n",
      "Training loss:1.7234848737716675\n",
      "Training loss:1.6797642707824707\n",
      "Training loss:1.776716947555542\n",
      "Training loss:1.622697353363037\n",
      "Training loss:1.6144208908081055\n",
      "Training loss:1.731329083442688\n",
      "Training loss:1.6485915184020996\n",
      "Training loss:1.6232889890670776\n",
      "Training loss:1.6814414262771606\n",
      "Training loss:1.6657004356384277\n",
      "Training loss:1.6637569665908813\n",
      "Training loss:1.662304162979126\n",
      "Training loss:1.6697028875350952\n",
      "Training loss:1.6756469011306763\n",
      "Training loss:1.6948844194412231\n",
      "Training loss:1.679929256439209\n",
      "Training loss:1.7275383472442627\n",
      "Training loss:1.8097785711288452\n",
      "Training loss:1.7340192794799805\n",
      "Training loss:1.6774399280548096\n",
      "Training loss:1.7415722608566284\n",
      "Training loss:1.6373496055603027\n",
      "Training loss:1.745068073272705\n",
      "Training loss:1.7094342708587646\n",
      "Training loss:1.7585527896881104\n",
      "Training loss:1.7389254570007324\n",
      "Training loss:1.707401156425476\n",
      "Training loss:1.7492462396621704\n",
      "Training loss:1.646558165550232\n",
      "Training loss:1.7053468227386475\n",
      "Training loss:1.6764631271362305\n",
      "Training loss:1.6894183158874512\n",
      "Training loss:1.728007435798645\n",
      "Training loss:1.7160807847976685\n",
      "Training loss:1.6407607793807983\n",
      "Training loss:1.692396640777588\n",
      "Training loss:1.6685833930969238\n",
      "Training loss:1.5836139917373657\n",
      "Training loss:1.6191009283065796\n",
      "Training loss:1.700506329536438\n",
      "Training loss:1.7004188299179077\n",
      "Training loss:1.7162543535232544\n",
      "Training loss:1.7054166793823242\n",
      "Training loss:1.6722943782806396\n",
      "Training loss:1.6659748554229736\n",
      "Training loss:1.6975605487823486\n",
      "Training loss:1.7124332189559937\n",
      "Training loss:1.7124437093734741\n",
      "Training loss:1.7071409225463867\n",
      "Training loss:1.6525543928146362\n",
      "Training loss:1.7278639078140259\n",
      "Epoch: 0060 loss_train: 159.157511 acc_train: 0.372083 loss_val: 20.172277 acc_val: 0.373167 time: 151898.938330s\n",
      "Training loss:1.697812795639038\n",
      "Training loss:1.6816785335540771\n",
      "Training loss:1.6021162271499634\n",
      "Training loss:1.7186880111694336\n",
      "Training loss:1.581160068511963\n",
      "Training loss:1.6697498559951782\n",
      "Training loss:1.6244553327560425\n",
      "Training loss:1.6577520370483398\n",
      "Training loss:1.6876742839813232\n",
      "Training loss:1.7244073152542114\n",
      "Training loss:1.6873489618301392\n",
      "Training loss:1.7159552574157715\n",
      "Training loss:1.618666172027588\n",
      "Training loss:1.7141106128692627\n",
      "Training loss:1.7380729913711548\n",
      "Training loss:1.705367088317871\n",
      "Training loss:1.7284241914749146\n",
      "Training loss:1.660579800605774\n",
      "Training loss:1.730808138847351\n",
      "Training loss:1.721534252166748\n",
      "Training loss:1.609035849571228\n",
      "Training loss:1.6703989505767822\n",
      "Training loss:1.7565451860427856\n",
      "Training loss:1.6266379356384277\n",
      "Training loss:1.6926497220993042\n",
      "Training loss:1.7333053350448608\n",
      "Training loss:1.7375295162200928\n",
      "Training loss:1.7527039051055908\n",
      "Training loss:1.7357885837554932\n",
      "Training loss:1.7297831773757935\n",
      "Training loss:1.7048511505126953\n",
      "Training loss:1.6974843740463257\n",
      "Training loss:1.6532440185546875\n",
      "Training loss:1.6311625242233276\n",
      "Training loss:1.7014994621276855\n",
      "Training loss:1.682931661605835\n",
      "Training loss:1.7022444009780884\n",
      "Training loss:1.6627048254013062\n",
      "Training loss:1.6991428136825562\n",
      "Training loss:1.6950477361679077\n",
      "Training loss:1.6392877101898193\n",
      "Training loss:1.6002014875411987\n",
      "Training loss:1.7562415599822998\n",
      "Training loss:1.6684225797653198\n",
      "Training loss:1.6680710315704346\n",
      "Training loss:1.6300785541534424\n",
      "Training loss:1.699662446975708\n",
      "Training loss:1.6924883127212524\n",
      "Training loss:1.713232159614563\n",
      "Training loss:1.6997426748275757\n",
      "Training loss:1.7045884132385254\n",
      "Training loss:1.6937648057937622\n",
      "Training loss:1.7055903673171997\n",
      "Training loss:1.7349486351013184\n",
      "Training loss:1.693410873413086\n",
      "Training loss:1.6344910860061646\n",
      "Training loss:1.6742253303527832\n",
      "Training loss:1.7375438213348389\n",
      "Training loss:1.6607282161712646\n",
      "Training loss:1.759460687637329\n",
      "Training loss:1.7535226345062256\n",
      "Training loss:1.6394264698028564\n",
      "Training loss:1.7206690311431885\n",
      "Training loss:1.6932085752487183\n",
      "Training loss:1.6953984498977661\n",
      "Training loss:1.7303645610809326\n",
      "Training loss:1.7258741855621338\n",
      "Training loss:1.6611400842666626\n",
      "Training loss:1.6660478115081787\n",
      "Training loss:1.6452895402908325\n",
      "Training loss:1.654818058013916\n",
      "Training loss:1.689174771308899\n",
      "Training loss:1.682110071182251\n",
      "Training loss:1.6627452373504639\n",
      "Training loss:1.7507884502410889\n",
      "Training loss:1.690509557723999\n",
      "Training loss:1.7462880611419678\n",
      "Training loss:1.590787649154663\n",
      "Training loss:1.6980780363082886\n",
      "Training loss:1.6453349590301514\n",
      "Training loss:1.6911723613739014\n",
      "Training loss:1.6513750553131104\n",
      "Training loss:1.7701375484466553\n",
      "Training loss:1.786348819732666\n",
      "Training loss:1.665517807006836\n",
      "Training loss:1.6643445491790771\n",
      "Training loss:1.721008062362671\n",
      "Training loss:1.6797548532485962\n",
      "Training loss:1.6401175260543823\n",
      "Training loss:1.6585543155670166\n",
      "Training loss:1.6159493923187256\n",
      "Training loss:1.7011168003082275\n",
      "Training loss:1.6894235610961914\n",
      "Training loss:1.720676302909851\n",
      "Epoch: 0061 loss_train: 158.704307 acc_train: 0.375063 loss_val: 20.281237 acc_val: 0.373500 time: 154384.192846s\n",
      "Training loss:1.6537526845932007\n",
      "Training loss:1.6805808544158936\n",
      "Training loss:1.7217469215393066\n",
      "Training loss:1.7251893281936646\n",
      "Training loss:1.667960286140442\n",
      "Training loss:1.769574522972107\n",
      "Training loss:1.7507857084274292\n",
      "Training loss:1.6779040098190308\n",
      "Training loss:1.719482183456421\n",
      "Training loss:1.7507761716842651\n",
      "Training loss:1.708980679512024\n",
      "Training loss:1.7959660291671753\n",
      "Training loss:1.7144112586975098\n",
      "Training loss:1.6870942115783691\n",
      "Training loss:1.642926812171936\n",
      "Training loss:1.6686443090438843\n",
      "Training loss:1.6705782413482666\n",
      "Training loss:1.6488083600997925\n",
      "Training loss:1.7166495323181152\n",
      "Training loss:1.6932111978530884\n",
      "Training loss:1.671242356300354\n",
      "Training loss:1.6258270740509033\n",
      "Training loss:1.6694176197052002\n",
      "Training loss:1.6165547370910645\n",
      "Training loss:1.6440269947052002\n",
      "Training loss:1.6952507495880127\n",
      "Training loss:1.6358559131622314\n",
      "Training loss:1.6915909051895142\n",
      "Training loss:1.635035753250122\n",
      "Training loss:1.6730618476867676\n",
      "Training loss:1.7609378099441528\n",
      "Training loss:1.6723709106445312\n",
      "Training loss:1.6940592527389526\n",
      "Training loss:1.6365517377853394\n",
      "Training loss:1.6286619901657104\n",
      "Training loss:1.7034897804260254\n",
      "Training loss:1.6616835594177246\n",
      "Training loss:1.7337162494659424\n",
      "Training loss:1.7495845556259155\n",
      "Training loss:1.6857656240463257\n",
      "Training loss:1.6730926036834717\n",
      "Training loss:1.6861902475357056\n",
      "Training loss:1.645469307899475\n",
      "Training loss:1.7620079517364502\n",
      "Training loss:1.7177624702453613\n",
      "Training loss:1.6785826683044434\n",
      "Training loss:1.7392115592956543\n",
      "Training loss:1.7217698097229004\n",
      "Training loss:1.7409473657608032\n",
      "Training loss:1.6924161911010742\n",
      "Training loss:1.6839395761489868\n",
      "Training loss:1.6720366477966309\n",
      "Training loss:1.6554450988769531\n",
      "Training loss:1.7185722589492798\n",
      "Training loss:1.708885908126831\n",
      "Training loss:1.7154029607772827\n",
      "Training loss:1.674975037574768\n",
      "Training loss:1.7258259057998657\n",
      "Training loss:1.6769508123397827\n",
      "Training loss:1.652283787727356\n",
      "Training loss:1.6623938083648682\n",
      "Training loss:1.7469004392623901\n",
      "Training loss:1.79751455783844\n",
      "Training loss:1.7103605270385742\n",
      "Training loss:1.7231311798095703\n",
      "Training loss:1.690613865852356\n",
      "Training loss:1.6520987749099731\n",
      "Training loss:1.6358739137649536\n",
      "Training loss:1.7092641592025757\n",
      "Training loss:1.6921489238739014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6847752332687378\n",
      "Training loss:1.6753449440002441\n",
      "Training loss:1.7398184537887573\n",
      "Training loss:1.7065150737762451\n",
      "Training loss:1.6971591711044312\n",
      "Training loss:1.6834887266159058\n",
      "Training loss:1.6564726829528809\n",
      "Training loss:1.7475612163543701\n",
      "Training loss:1.7264466285705566\n",
      "Training loss:1.7169703245162964\n",
      "Training loss:1.734816074371338\n",
      "Training loss:1.718396544456482\n",
      "Training loss:1.695862054824829\n",
      "Training loss:1.6800355911254883\n",
      "Training loss:1.7062605619430542\n",
      "Training loss:1.7357686758041382\n",
      "Training loss:1.7046136856079102\n",
      "Training loss:1.636581540107727\n",
      "Training loss:1.6311055421829224\n",
      "Training loss:1.7270220518112183\n",
      "Training loss:1.678183913230896\n",
      "Training loss:1.7206590175628662\n",
      "Training loss:1.7571611404418945\n",
      "Training loss:1.675811767578125\n",
      "Epoch: 0062 loss_train: 159.348608 acc_train: 0.371854 loss_val: 20.143506 acc_val: 0.375167 time: 156873.685330s\n",
      "Training loss:1.6724259853363037\n",
      "Training loss:1.688066005706787\n",
      "Training loss:1.6801738739013672\n",
      "Training loss:1.6127405166625977\n",
      "Training loss:1.7028682231903076\n",
      "Training loss:1.7062817811965942\n",
      "Training loss:1.6917743682861328\n",
      "Training loss:1.6725462675094604\n",
      "Training loss:1.6453325748443604\n",
      "Training loss:1.6895469427108765\n",
      "Training loss:1.750356912612915\n",
      "Training loss:1.6846193075180054\n",
      "Training loss:1.6811261177062988\n",
      "Training loss:1.7071353197097778\n",
      "Training loss:1.6632492542266846\n",
      "Training loss:1.7181801795959473\n",
      "Training loss:1.8598371744155884\n",
      "Training loss:1.6575229167938232\n",
      "Training loss:1.7684249877929688\n",
      "Training loss:1.68813157081604\n",
      "Training loss:1.7340328693389893\n",
      "Training loss:1.684598684310913\n",
      "Training loss:1.6198831796646118\n",
      "Training loss:1.738817572593689\n",
      "Training loss:1.6350511312484741\n",
      "Training loss:1.711568832397461\n",
      "Training loss:1.65900456905365\n",
      "Training loss:1.6681996583938599\n",
      "Training loss:1.6929105520248413\n",
      "Training loss:1.692816972732544\n",
      "Training loss:1.7671740055084229\n",
      "Training loss:1.7067290544509888\n",
      "Training loss:1.730898380279541\n",
      "Training loss:1.704650640487671\n",
      "Training loss:1.634512186050415\n",
      "Training loss:1.7939269542694092\n",
      "Training loss:1.678263783454895\n",
      "Training loss:1.6961381435394287\n",
      "Training loss:1.7214325666427612\n",
      "Training loss:1.605634331703186\n",
      "Training loss:1.7100419998168945\n",
      "Training loss:1.670753836631775\n",
      "Training loss:1.6422574520111084\n",
      "Training loss:1.6639561653137207\n",
      "Training loss:1.657347321510315\n",
      "Training loss:1.6158798933029175\n",
      "Training loss:1.6974014043807983\n",
      "Training loss:1.6968097686767578\n",
      "Training loss:1.7476215362548828\n",
      "Training loss:1.7061461210250854\n",
      "Training loss:1.6741944551467896\n",
      "Training loss:1.7048708200454712\n",
      "Training loss:1.6535581350326538\n",
      "Training loss:1.7173370122909546\n",
      "Training loss:1.7095171213150024\n",
      "Training loss:1.636520266532898\n",
      "Training loss:1.7619304656982422\n",
      "Training loss:1.6401026248931885\n",
      "Training loss:1.7443825006484985\n",
      "Training loss:1.6986883878707886\n",
      "Training loss:1.7109874486923218\n",
      "Training loss:1.7146005630493164\n",
      "Training loss:1.685105800628662\n",
      "Training loss:1.6819162368774414\n",
      "Training loss:1.6690475940704346\n",
      "Training loss:1.5956096649169922\n",
      "Training loss:1.690443515777588\n",
      "Training loss:1.7303647994995117\n",
      "Training loss:1.6787687540054321\n",
      "Training loss:1.6566170454025269\n",
      "Training loss:1.6368738412857056\n",
      "Training loss:1.6780210733413696\n",
      "Training loss:1.6996129751205444\n",
      "Training loss:1.7498219013214111\n",
      "Training loss:1.6816400289535522\n",
      "Training loss:1.629477858543396\n",
      "Training loss:1.7568912506103516\n",
      "Training loss:1.6285357475280762\n",
      "Training loss:1.6519582271575928\n",
      "Training loss:1.7046936750411987\n",
      "Training loss:1.6932411193847656\n",
      "Training loss:1.6563103199005127\n",
      "Training loss:1.6722135543823242\n",
      "Training loss:1.7016252279281616\n",
      "Training loss:1.714232087135315\n",
      "Training loss:1.6431407928466797\n",
      "Training loss:1.7260582447052002\n",
      "Training loss:1.715903401374817\n",
      "Training loss:1.7114137411117554\n",
      "Training loss:1.7386597394943237\n",
      "Training loss:1.6065753698349\n",
      "Training loss:1.6963460445404053\n",
      "Training loss:1.7201908826828003\n",
      "Training loss:1.7001277208328247\n",
      "Epoch: 0063 loss_train: 158.892928 acc_train: 0.372750 loss_val: 20.132487 acc_val: 0.375833 time: 159360.879815s\n",
      "Training loss:1.7125005722045898\n",
      "Training loss:1.693627119064331\n",
      "Training loss:1.6491267681121826\n",
      "Training loss:1.698346734046936\n",
      "Training loss:1.7320060729980469\n",
      "Training loss:1.6114394664764404\n",
      "Training loss:1.6551228761672974\n",
      "Training loss:1.7457436323165894\n",
      "Training loss:1.6942574977874756\n",
      "Training loss:1.7081488370895386\n",
      "Training loss:1.7492876052856445\n",
      "Training loss:1.65701425075531\n",
      "Training loss:1.5905410051345825\n",
      "Training loss:1.6998950242996216\n",
      "Training loss:1.68666410446167\n",
      "Training loss:1.6104236841201782\n",
      "Training loss:1.6743625402450562\n",
      "Training loss:1.703366756439209\n",
      "Training loss:1.6852353811264038\n",
      "Training loss:1.608064889907837\n",
      "Training loss:1.6804555654525757\n",
      "Training loss:1.6493674516677856\n",
      "Training loss:1.6840267181396484\n",
      "Training loss:1.702674388885498\n",
      "Training loss:1.6848922967910767\n",
      "Training loss:1.6400614976882935\n",
      "Training loss:1.6404774188995361\n",
      "Training loss:1.6271342039108276\n",
      "Training loss:1.6489919424057007\n",
      "Training loss:1.6940580606460571\n",
      "Training loss:1.7251628637313843\n",
      "Training loss:1.6586633920669556\n",
      "Training loss:1.6247491836547852\n",
      "Training loss:1.7247458696365356\n",
      "Training loss:1.7181057929992676\n",
      "Training loss:1.6486636400222778\n",
      "Training loss:1.7076202630996704\n",
      "Training loss:1.679496169090271\n",
      "Training loss:1.632276177406311\n",
      "Training loss:1.6785786151885986\n",
      "Training loss:1.6358399391174316\n",
      "Training loss:1.6894280910491943\n",
      "Training loss:1.7219229936599731\n",
      "Training loss:1.643508791923523\n",
      "Training loss:1.6934093236923218\n",
      "Training loss:1.786535382270813\n",
      "Training loss:1.7079477310180664\n",
      "Training loss:1.6766313314437866\n",
      "Training loss:1.7287458181381226\n",
      "Training loss:1.6683969497680664\n",
      "Training loss:1.7125564813613892\n",
      "Training loss:1.6639167070388794\n",
      "Training loss:1.7509722709655762\n",
      "Training loss:1.7363567352294922\n",
      "Training loss:1.7411292791366577\n",
      "Training loss:1.717379093170166\n",
      "Training loss:1.6564500331878662\n",
      "Training loss:1.6002596616744995\n",
      "Training loss:1.699306845664978\n",
      "Training loss:1.623570203781128\n",
      "Training loss:1.6706736087799072\n",
      "Training loss:1.740966558456421\n",
      "Training loss:1.65261971950531\n",
      "Training loss:1.6532480716705322\n",
      "Training loss:1.6448463201522827\n",
      "Training loss:1.6822582483291626\n",
      "Training loss:1.6182572841644287\n",
      "Training loss:1.740858793258667\n",
      "Training loss:1.7155863046646118\n",
      "Training loss:1.6745045185089111\n",
      "Training loss:1.6530802249908447\n",
      "Training loss:1.655846357345581\n",
      "Training loss:1.66221284866333\n",
      "Training loss:1.6612614393234253\n",
      "Training loss:1.6326353549957275\n",
      "Training loss:1.680798888206482\n",
      "Training loss:1.6430860757827759\n",
      "Training loss:1.6800320148468018\n",
      "Training loss:1.7335610389709473\n",
      "Training loss:1.714176893234253\n",
      "Training loss:1.6979347467422485\n",
      "Training loss:1.7445497512817383\n",
      "Training loss:1.690116047859192\n",
      "Training loss:1.6798467636108398\n",
      "Training loss:1.7317678928375244\n",
      "Training loss:1.6996957063674927\n",
      "Training loss:1.687269687652588\n",
      "Training loss:1.6463607549667358\n",
      "Training loss:1.6719228029251099\n",
      "Training loss:1.658125400543213\n",
      "Training loss:1.6628625392913818\n",
      "Training loss:1.7083146572113037\n",
      "Training loss:1.722265362739563\n",
      "Training loss:1.7054871320724487\n",
      "Epoch: 0064 loss_train: 158.086670 acc_train: 0.377958 loss_val: 20.019186 acc_val: 0.380667 time: 161903.465862s\n",
      "Training loss:1.6333191394805908\n",
      "Training loss:1.6528680324554443\n",
      "Training loss:1.7015190124511719\n",
      "Training loss:1.6999969482421875\n",
      "Training loss:1.831268072128296\n",
      "Training loss:1.674461841583252\n",
      "Training loss:1.6885566711425781\n",
      "Training loss:1.7872978448867798\n",
      "Training loss:1.6584795713424683\n",
      "Training loss:1.6716961860656738\n",
      "Training loss:1.6600227355957031\n",
      "Training loss:1.6533006429672241\n",
      "Training loss:1.7031474113464355\n",
      "Training loss:1.7007991075515747\n",
      "Training loss:1.7320704460144043\n",
      "Training loss:1.7057377099990845\n",
      "Training loss:1.6268810033798218\n",
      "Training loss:1.6712394952774048\n",
      "Training loss:1.6920462846755981\n",
      "Training loss:1.6830286979675293\n",
      "Training loss:1.6630979776382446\n",
      "Training loss:1.7293732166290283\n",
      "Training loss:1.674006700515747\n",
      "Training loss:1.638687252998352\n",
      "Training loss:1.777497410774231\n",
      "Training loss:1.570068597793579\n",
      "Training loss:1.6822327375411987\n",
      "Training loss:1.6543257236480713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.7080190181732178\n",
      "Training loss:1.662532091140747\n",
      "Training loss:1.6666314601898193\n",
      "Training loss:1.658951997756958\n",
      "Training loss:1.6700046062469482\n",
      "Training loss:1.6593228578567505\n",
      "Training loss:1.6320303678512573\n",
      "Training loss:1.5993120670318604\n",
      "Training loss:1.7279400825500488\n",
      "Training loss:1.7383074760437012\n",
      "Training loss:1.6322896480560303\n",
      "Training loss:1.7484102249145508\n",
      "Training loss:1.6364850997924805\n",
      "Training loss:1.7473195791244507\n",
      "Training loss:1.6722378730773926\n",
      "Training loss:1.7103530168533325\n",
      "Training loss:1.6913408041000366\n",
      "Training loss:1.61732017993927\n",
      "Training loss:1.700314998626709\n",
      "Training loss:1.7463526725769043\n",
      "Training loss:1.7079033851623535\n",
      "Training loss:1.6287137269973755\n",
      "Training loss:1.7533841133117676\n",
      "Training loss:1.7210211753845215\n",
      "Training loss:1.6313215494155884\n",
      "Training loss:1.6444767713546753\n",
      "Training loss:1.6676279306411743\n",
      "Training loss:1.634189248085022\n",
      "Training loss:1.6358145475387573\n",
      "Training loss:1.673757553100586\n",
      "Training loss:1.6738260984420776\n",
      "Training loss:1.6551716327667236\n",
      "Training loss:1.6219419240951538\n",
      "Training loss:1.6990877389907837\n",
      "Training loss:1.7027755975723267\n",
      "Training loss:1.6917816400527954\n",
      "Training loss:1.6315557956695557\n",
      "Training loss:1.6721888780593872\n",
      "Training loss:1.739880084991455\n",
      "Training loss:1.7254761457443237\n",
      "Training loss:1.6939257383346558\n",
      "Training loss:1.6824519634246826\n",
      "Training loss:1.6005449295043945\n",
      "Training loss:1.7510311603546143\n",
      "Training loss:1.7436859607696533\n",
      "Training loss:1.675362467765808\n",
      "Training loss:1.653730869293213\n",
      "Training loss:1.6616969108581543\n",
      "Training loss:1.6690024137496948\n",
      "Training loss:1.6911005973815918\n",
      "Training loss:1.7222024202346802\n",
      "Training loss:1.6462163925170898\n",
      "Training loss:1.6240154504776\n",
      "Training loss:1.6696772575378418\n",
      "Training loss:1.6389353275299072\n",
      "Training loss:1.6593682765960693\n",
      "Training loss:1.6358847618103027\n",
      "Training loss:1.6665611267089844\n",
      "Training loss:1.6642132997512817\n",
      "Training loss:1.7192364931106567\n",
      "Training loss:1.7516909837722778\n",
      "Training loss:1.6438745260238647\n",
      "Training loss:1.6380274295806885\n",
      "Training loss:1.7316443920135498\n",
      "Training loss:1.6291296482086182\n",
      "Training loss:1.7011953592300415\n",
      "Epoch: 0065 loss_train: 157.918804 acc_train: 0.379229 loss_val: 20.029178 acc_val: 0.378667 time: 164564.449108s\n",
      "Training loss:1.7115142345428467\n",
      "Training loss:1.624758005142212\n",
      "Training loss:1.6520534753799438\n",
      "Training loss:1.6897109746932983\n",
      "Training loss:1.6682212352752686\n",
      "Training loss:1.7013474702835083\n",
      "Training loss:1.652571439743042\n",
      "Training loss:1.6802639961242676\n",
      "Training loss:1.7288318872451782\n",
      "Training loss:1.773342251777649\n",
      "Training loss:1.66421639919281\n",
      "Training loss:1.70167875289917\n",
      "Training loss:1.598961591720581\n",
      "Training loss:1.6753591299057007\n",
      "Training loss:1.8141757249832153\n",
      "Training loss:1.7433371543884277\n",
      "Training loss:1.6093825101852417\n",
      "Training loss:1.68022620677948\n",
      "Training loss:1.717545509338379\n",
      "Training loss:1.6357773542404175\n",
      "Training loss:1.6765583753585815\n",
      "Training loss:1.7034133672714233\n",
      "Training loss:1.6771886348724365\n",
      "Training loss:1.6995160579681396\n",
      "Training loss:1.6896060705184937\n",
      "Training loss:1.6490687131881714\n",
      "Training loss:1.7137994766235352\n",
      "Training loss:1.637360692024231\n",
      "Training loss:1.7006251811981201\n",
      "Training loss:1.667431354522705\n",
      "Training loss:1.702041506767273\n",
      "Training loss:1.6897037029266357\n",
      "Training loss:1.6380774974822998\n",
      "Training loss:1.6942061185836792\n",
      "Training loss:1.6467478275299072\n",
      "Training loss:1.716845989227295\n",
      "Training loss:1.6122925281524658\n",
      "Training loss:1.663525104522705\n",
      "Training loss:1.690292477607727\n",
      "Training loss:1.6527245044708252\n",
      "Training loss:1.7585573196411133\n",
      "Training loss:1.7259961366653442\n",
      "Training loss:1.6387999057769775\n",
      "Training loss:1.6932616233825684\n",
      "Training loss:1.580911636352539\n",
      "Training loss:1.6583296060562134\n",
      "Training loss:1.720131278038025\n",
      "Training loss:1.7070693969726562\n",
      "Training loss:1.6645312309265137\n",
      "Training loss:1.6597785949707031\n",
      "Training loss:1.7051548957824707\n",
      "Training loss:1.6883400678634644\n",
      "Training loss:1.686968207359314\n",
      "Training loss:1.5981827974319458\n",
      "Training loss:1.6822513341903687\n",
      "Training loss:1.6501145362854004\n",
      "Training loss:1.659746766090393\n",
      "Training loss:1.6470075845718384\n",
      "Training loss:1.7461332082748413\n",
      "Training loss:1.6315624713897705\n",
      "Training loss:1.6102787256240845\n",
      "Training loss:1.7488811016082764\n",
      "Training loss:1.625171184539795\n",
      "Training loss:1.6818008422851562\n",
      "Training loss:1.709822416305542\n",
      "Training loss:1.6134581565856934\n",
      "Training loss:1.680212140083313\n",
      "Training loss:1.628621220588684\n",
      "Training loss:1.724788784980774\n",
      "Training loss:1.6572585105895996\n",
      "Training loss:1.6765965223312378\n",
      "Training loss:1.6340910196304321\n",
      "Training loss:1.6417609453201294\n",
      "Training loss:1.6586625576019287\n",
      "Training loss:1.7288302183151245\n",
      "Training loss:1.630654215812683\n",
      "Training loss:1.6564326286315918\n",
      "Training loss:1.6499733924865723\n",
      "Training loss:1.713196873664856\n",
      "Training loss:1.7047176361083984\n",
      "Training loss:1.7000874280929565\n",
      "Training loss:1.6723374128341675\n",
      "Training loss:1.6654082536697388\n",
      "Training loss:1.6577121019363403\n",
      "Training loss:1.6798453330993652\n",
      "Training loss:1.7468761205673218\n",
      "Training loss:1.6719725131988525\n",
      "Training loss:1.6532344818115234\n",
      "Training loss:1.675545334815979\n",
      "Training loss:1.7002134323120117\n",
      "Training loss:1.6231790781021118\n",
      "Training loss:1.6872992515563965\n",
      "Training loss:1.6883742809295654\n",
      "Training loss:1.6655055284500122\n",
      "Epoch: 0066 loss_train: 157.609929 acc_train: 0.379333 loss_val: 20.059049 acc_val: 0.380167 time: 167203.389784s\n",
      "Training loss:1.6820743083953857\n",
      "Training loss:1.6411104202270508\n",
      "Training loss:1.6568045616149902\n",
      "Training loss:1.6992031335830688\n",
      "Training loss:1.7064162492752075\n",
      "Training loss:1.651200294494629\n",
      "Training loss:1.7367593050003052\n",
      "Training loss:1.6371961832046509\n",
      "Training loss:1.6813441514968872\n",
      "Training loss:1.6509391069412231\n",
      "Training loss:1.643894076347351\n",
      "Training loss:1.733056902885437\n",
      "Training loss:1.6614930629730225\n",
      "Training loss:1.6547242403030396\n",
      "Training loss:1.7098593711853027\n",
      "Training loss:1.6732233762741089\n",
      "Training loss:1.7329461574554443\n",
      "Training loss:1.6791231632232666\n",
      "Training loss:1.7285172939300537\n",
      "Training loss:1.6709414720535278\n",
      "Training loss:1.6473898887634277\n",
      "Training loss:1.7142651081085205\n",
      "Training loss:1.641371726989746\n",
      "Training loss:1.6535199880599976\n",
      "Training loss:1.7185721397399902\n",
      "Training loss:1.7520087957382202\n",
      "Training loss:1.6596665382385254\n",
      "Training loss:1.708521842956543\n",
      "Training loss:1.691441535949707\n",
      "Training loss:1.7261080741882324\n",
      "Training loss:1.6161810159683228\n",
      "Training loss:1.7578693628311157\n",
      "Training loss:1.6552577018737793\n",
      "Training loss:1.6289530992507935\n",
      "Training loss:1.696162462234497\n",
      "Training loss:1.7552424669265747\n",
      "Training loss:1.6900913715362549\n",
      "Training loss:1.6565741300582886\n",
      "Training loss:1.6463818550109863\n",
      "Training loss:1.6700490713119507\n",
      "Training loss:1.7449660301208496\n",
      "Training loss:1.6725877523422241\n",
      "Training loss:1.6207107305526733\n",
      "Training loss:1.6591203212738037\n",
      "Training loss:1.6993343830108643\n",
      "Training loss:1.6508795022964478\n",
      "Training loss:1.6881386041641235\n",
      "Training loss:1.6279733180999756\n",
      "Training loss:1.665732741355896\n",
      "Training loss:1.635032296180725\n",
      "Training loss:1.5794845819473267\n",
      "Training loss:1.6470228433609009\n",
      "Training loss:1.66850745677948\n",
      "Training loss:1.740570068359375\n",
      "Training loss:1.6417452096939087\n",
      "Training loss:1.6739225387573242\n",
      "Training loss:1.59662663936615\n",
      "Training loss:1.6744544506072998\n",
      "Training loss:1.7363814115524292\n",
      "Training loss:1.6536779403686523\n",
      "Training loss:1.6572705507278442\n",
      "Training loss:1.7219585180282593\n",
      "Training loss:1.692726969718933\n",
      "Training loss:1.6941556930541992\n",
      "Training loss:1.670390248298645\n",
      "Training loss:1.6460484266281128\n",
      "Training loss:1.7381150722503662\n",
      "Training loss:1.597001075744629\n",
      "Training loss:1.7316659688949585\n",
      "Training loss:1.6901081800460815\n",
      "Training loss:1.6723709106445312\n",
      "Training loss:1.687463641166687\n",
      "Training loss:1.675941824913025\n",
      "Training loss:1.6743401288986206\n",
      "Training loss:1.6397473812103271\n",
      "Training loss:1.738639235496521\n",
      "Training loss:1.6674996614456177\n",
      "Training loss:1.5645445585250854\n",
      "Training loss:1.668408989906311\n",
      "Training loss:1.6599284410476685\n",
      "Training loss:1.6951013803482056\n",
      "Training loss:1.6844645738601685\n",
      "Training loss:1.6746989488601685\n",
      "Training loss:1.6995898485183716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6360740661621094\n",
      "Training loss:1.7093955278396606\n",
      "Training loss:1.701895833015442\n",
      "Training loss:1.680755376815796\n",
      "Training loss:1.6842329502105713\n",
      "Training loss:1.7036769390106201\n",
      "Training loss:1.629145622253418\n",
      "Training loss:1.7359322309494019\n",
      "Training loss:1.6487679481506348\n",
      "Training loss:1.6311620473861694\n",
      "Epoch: 0067 loss_train: 157.596543 acc_train: 0.380812 loss_val: 20.159313 acc_val: 0.378833 time: 169859.563283s\n",
      "Training loss:1.6496258974075317\n",
      "Training loss:1.7058686017990112\n",
      "Training loss:1.7121011018753052\n",
      "Training loss:1.67879319190979\n",
      "Training loss:1.6917414665222168\n",
      "Training loss:1.7124289274215698\n",
      "Training loss:1.6381396055221558\n",
      "Training loss:1.6930301189422607\n",
      "Training loss:1.6744354963302612\n",
      "Training loss:1.6721645593643188\n",
      "Training loss:1.763231635093689\n",
      "Training loss:1.723746418952942\n",
      "Training loss:1.7225087881088257\n",
      "Training loss:1.6477941274642944\n",
      "Training loss:1.6642102003097534\n",
      "Training loss:1.673992395401001\n",
      "Training loss:1.6779474020004272\n",
      "Training loss:1.664645791053772\n",
      "Training loss:1.687827229499817\n",
      "Training loss:1.612902045249939\n",
      "Training loss:1.6987580060958862\n",
      "Training loss:1.732947587966919\n",
      "Training loss:1.6308629512786865\n",
      "Training loss:1.6722763776779175\n",
      "Training loss:1.7077372074127197\n",
      "Training loss:1.641278624534607\n",
      "Training loss:1.6376601457595825\n",
      "Training loss:1.7045791149139404\n",
      "Training loss:1.7540496587753296\n",
      "Training loss:1.6697688102722168\n",
      "Training loss:1.602501392364502\n",
      "Training loss:1.6197729110717773\n",
      "Training loss:1.6239891052246094\n",
      "Training loss:1.6337764263153076\n",
      "Training loss:1.6763088703155518\n",
      "Training loss:1.6586123704910278\n",
      "Training loss:1.6584416627883911\n",
      "Training loss:1.649280309677124\n",
      "Training loss:1.6980143785476685\n",
      "Training loss:1.593208909034729\n",
      "Training loss:1.7140861749649048\n",
      "Training loss:1.6584396362304688\n",
      "Training loss:1.6648831367492676\n",
      "Training loss:1.6123472452163696\n",
      "Training loss:1.6441612243652344\n",
      "Training loss:1.6909379959106445\n",
      "Training loss:1.6514432430267334\n",
      "Training loss:1.7309376001358032\n",
      "Training loss:1.6577773094177246\n",
      "Training loss:1.6389431953430176\n",
      "Training loss:1.6477506160736084\n",
      "Training loss:1.7337064743041992\n",
      "Training loss:1.7023706436157227\n",
      "Training loss:1.6057497262954712\n",
      "Training loss:1.6716431379318237\n",
      "Training loss:1.6639337539672852\n",
      "Training loss:1.6965045928955078\n",
      "Training loss:1.6524252891540527\n",
      "Training loss:1.64046049118042\n",
      "Training loss:1.6847362518310547\n",
      "Training loss:1.6235712766647339\n",
      "Training loss:1.5939419269561768\n",
      "Training loss:1.6892597675323486\n",
      "Training loss:1.6678030490875244\n",
      "Training loss:1.638759732246399\n",
      "Training loss:1.6671782732009888\n",
      "Training loss:1.7558575868606567\n",
      "Training loss:1.6833350658416748\n",
      "Training loss:1.6813228130340576\n",
      "Training loss:1.6115038394927979\n",
      "Training loss:1.6212774515151978\n",
      "Training loss:1.7077059745788574\n",
      "Training loss:1.701515555381775\n",
      "Training loss:1.673896074295044\n",
      "Training loss:1.6504346132278442\n",
      "Training loss:1.674598217010498\n",
      "Training loss:1.6705783605575562\n",
      "Training loss:1.6760467290878296\n",
      "Training loss:1.6605324745178223\n",
      "Training loss:1.7123157978057861\n",
      "Training loss:1.7012883424758911\n",
      "Training loss:1.7265549898147583\n",
      "Training loss:1.6508203744888306\n",
      "Training loss:1.5811262130737305\n",
      "Training loss:1.7726165056228638\n",
      "Training loss:1.6755168437957764\n",
      "Training loss:1.6276506185531616\n",
      "Training loss:1.6278655529022217\n",
      "Training loss:1.6825544834136963\n",
      "Training loss:1.7458698749542236\n",
      "Training loss:1.6658713817596436\n",
      "Training loss:1.6882963180541992\n",
      "Training loss:1.73664128780365\n",
      "Training loss:1.6996604204177856\n",
      "Epoch: 0068 loss_train: 157.237965 acc_train: 0.381625 loss_val: 19.880770 acc_val: 0.385333 time: 172435.304405s\n",
      "Training loss:1.7409873008728027\n",
      "Training loss:1.6525764465332031\n",
      "Training loss:1.655157446861267\n",
      "Training loss:1.6410454511642456\n",
      "Training loss:1.646549105644226\n",
      "Training loss:1.621864676475525\n",
      "Training loss:1.6569175720214844\n",
      "Training loss:1.6603481769561768\n",
      "Training loss:1.6509528160095215\n",
      "Training loss:1.6735869646072388\n",
      "Training loss:1.6825590133666992\n",
      "Training loss:1.632015347480774\n",
      "Training loss:1.6476664543151855\n",
      "Training loss:1.6569960117340088\n",
      "Training loss:1.6753901243209839\n",
      "Training loss:1.7224222421646118\n",
      "Training loss:1.7022895812988281\n",
      "Training loss:1.6292309761047363\n",
      "Training loss:1.6366703510284424\n",
      "Training loss:1.6512954235076904\n",
      "Training loss:1.6263166666030884\n",
      "Training loss:1.7014122009277344\n",
      "Training loss:1.6118144989013672\n",
      "Training loss:1.714717984199524\n",
      "Training loss:1.6020658016204834\n",
      "Training loss:1.6745885610580444\n",
      "Training loss:1.657549500465393\n",
      "Training loss:1.689399242401123\n",
      "Training loss:1.6583267450332642\n",
      "Training loss:1.7748076915740967\n",
      "Training loss:1.6611411571502686\n",
      "Training loss:1.656205415725708\n",
      "Training loss:1.610367774963379\n",
      "Training loss:1.7391331195831299\n",
      "Training loss:1.6732090711593628\n",
      "Training loss:1.6322565078735352\n",
      "Training loss:1.7163673639297485\n",
      "Training loss:1.6685872077941895\n",
      "Training loss:1.652132272720337\n",
      "Training loss:1.7777824401855469\n",
      "Training loss:1.6276582479476929\n",
      "Training loss:1.5585840940475464\n",
      "Training loss:1.6130138635635376\n",
      "Training loss:1.6584243774414062\n",
      "Training loss:1.724995732307434\n",
      "Training loss:1.6321439743041992\n",
      "Training loss:1.6167489290237427\n",
      "Training loss:1.7291624546051025\n",
      "Training loss:1.6060895919799805\n",
      "Training loss:1.6881835460662842\n",
      "Training loss:1.6736375093460083\n",
      "Training loss:1.729965090751648\n",
      "Training loss:1.7503478527069092\n",
      "Training loss:1.7242473363876343\n",
      "Training loss:1.635953664779663\n",
      "Training loss:1.605136752128601\n",
      "Training loss:1.6587283611297607\n",
      "Training loss:1.6516728401184082\n",
      "Training loss:1.6718727350234985\n",
      "Training loss:1.6527303457260132\n",
      "Training loss:1.6482245922088623\n",
      "Training loss:1.7752606868743896\n",
      "Training loss:1.6197640895843506\n",
      "Training loss:1.6392605304718018\n",
      "Training loss:1.7414770126342773\n",
      "Training loss:1.6839549541473389\n",
      "Training loss:1.7433500289916992\n",
      "Training loss:1.7622673511505127\n",
      "Training loss:1.724855899810791\n",
      "Training loss:1.7101373672485352\n",
      "Training loss:1.636031150817871\n",
      "Training loss:1.7033859491348267\n",
      "Training loss:1.5649837255477905\n",
      "Training loss:1.63668954372406\n",
      "Training loss:1.7020995616912842\n",
      "Training loss:1.7290974855422974\n",
      "Training loss:1.619499683380127\n",
      "Training loss:1.6815812587738037\n",
      "Training loss:1.644742727279663\n",
      "Training loss:1.6421022415161133\n",
      "Training loss:1.72385835647583\n",
      "Training loss:1.7628931999206543\n",
      "Training loss:1.6536695957183838\n",
      "Training loss:1.6448030471801758\n",
      "Training loss:1.66336989402771\n",
      "Training loss:1.7065016031265259\n",
      "Training loss:1.6574056148529053\n",
      "Training loss:1.7308670282363892\n",
      "Training loss:1.6670624017715454\n",
      "Training loss:1.6487958431243896\n",
      "Training loss:1.648769736289978\n",
      "Training loss:1.7370929718017578\n",
      "Training loss:1.6593034267425537\n",
      "Training loss:1.6316885948181152\n",
      "Epoch: 0069 loss_train: 157.088847 acc_train: 0.382771 loss_val: 19.891413 acc_val: 0.383500 time: 175032.196336s\n",
      "Training loss:1.6710100173950195\n",
      "Training loss:1.6956065893173218\n",
      "Training loss:1.6165255308151245\n",
      "Training loss:1.595210075378418\n",
      "Training loss:1.6769742965698242\n",
      "Training loss:1.6760501861572266\n",
      "Training loss:1.6690285205841064\n",
      "Training loss:1.6476399898529053\n",
      "Training loss:1.6439218521118164\n",
      "Training loss:1.6629748344421387\n",
      "Training loss:1.660812497138977\n",
      "Training loss:1.686463475227356\n",
      "Training loss:1.6742898225784302\n",
      "Training loss:1.721753716468811\n",
      "Training loss:1.7075718641281128\n",
      "Training loss:1.6906377077102661\n",
      "Training loss:1.7161071300506592\n",
      "Training loss:1.6776677370071411\n",
      "Training loss:1.6914464235305786\n",
      "Training loss:1.7299556732177734\n",
      "Training loss:1.66765296459198\n",
      "Training loss:1.6619596481323242\n",
      "Training loss:1.6059792041778564\n",
      "Training loss:1.6649227142333984\n",
      "Training loss:1.6183198690414429\n",
      "Training loss:1.7067625522613525\n",
      "Training loss:1.7219961881637573\n",
      "Training loss:1.6227601766586304\n",
      "Training loss:1.6543256044387817\n",
      "Training loss:1.6255395412445068\n",
      "Training loss:1.5881047248840332\n",
      "Training loss:1.624106526374817\n",
      "Training loss:1.6448571681976318\n",
      "Training loss:1.6940299272537231\n",
      "Training loss:1.6105942726135254\n",
      "Training loss:1.6326282024383545\n",
      "Training loss:1.607285737991333\n",
      "Training loss:1.6629681587219238\n",
      "Training loss:1.6271578073501587\n",
      "Training loss:1.6686758995056152\n",
      "Training loss:1.6332157850265503\n",
      "Training loss:1.6802138090133667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6854207515716553\n",
      "Training loss:1.6144955158233643\n",
      "Training loss:1.692732572555542\n",
      "Training loss:1.6541593074798584\n",
      "Training loss:1.7033419609069824\n",
      "Training loss:1.6616475582122803\n",
      "Training loss:1.6143420934677124\n",
      "Training loss:1.6582534313201904\n",
      "Training loss:1.7604283094406128\n",
      "Training loss:1.6520380973815918\n",
      "Training loss:1.682186245918274\n",
      "Training loss:1.607456922531128\n",
      "Training loss:1.679369330406189\n",
      "Training loss:1.6639702320098877\n",
      "Training loss:1.6667206287384033\n",
      "Training loss:1.6666083335876465\n",
      "Training loss:1.6733403205871582\n",
      "Training loss:1.6358914375305176\n",
      "Training loss:1.6829396486282349\n",
      "Training loss:1.5909961462020874\n",
      "Training loss:1.6710041761398315\n",
      "Training loss:1.6805248260498047\n",
      "Training loss:1.6973559856414795\n",
      "Training loss:1.6685832738876343\n",
      "Training loss:1.6544256210327148\n",
      "Training loss:1.6740220785140991\n",
      "Training loss:1.6065632104873657\n",
      "Training loss:1.7206498384475708\n",
      "Training loss:1.6369597911834717\n",
      "Training loss:1.7146174907684326\n",
      "Training loss:1.6542270183563232\n",
      "Training loss:1.593618631362915\n",
      "Training loss:1.6480178833007812\n",
      "Training loss:1.6444522142410278\n",
      "Training loss:1.6666135787963867\n",
      "Training loss:1.689698338508606\n",
      "Training loss:1.6716477870941162\n",
      "Training loss:1.7000325918197632\n",
      "Training loss:1.6647034883499146\n",
      "Training loss:1.6326261758804321\n",
      "Training loss:1.662741780281067\n",
      "Training loss:1.5829578638076782\n",
      "Training loss:1.666951298713684\n",
      "Training loss:1.6279048919677734\n",
      "Training loss:1.7202119827270508\n",
      "Training loss:1.6689485311508179\n",
      "Training loss:1.7134666442871094\n",
      "Training loss:1.6888411045074463\n",
      "Training loss:1.7187881469726562\n",
      "Training loss:1.6920404434204102\n",
      "Training loss:1.652441143989563\n",
      "Training loss:1.6993480920791626\n",
      "Epoch: 0070 loss_train: 156.367029 acc_train: 0.387708 loss_val: 19.788561 acc_val: 0.387833 time: 177594.534150s\n",
      "Training loss:1.615522861480713\n",
      "Training loss:1.6774659156799316\n",
      "Training loss:1.642080545425415\n",
      "Training loss:1.620682716369629\n",
      "Training loss:1.6232715845108032\n",
      "Training loss:1.6854852437973022\n",
      "Training loss:1.6164875030517578\n",
      "Training loss:1.6546943187713623\n",
      "Training loss:1.6128801107406616\n",
      "Training loss:1.6273856163024902\n",
      "Training loss:1.6690155267715454\n",
      "Training loss:1.6684350967407227\n",
      "Training loss:1.7325067520141602\n",
      "Training loss:1.647606611251831\n",
      "Training loss:1.6748638153076172\n",
      "Training loss:1.6560258865356445\n",
      "Training loss:1.716774821281433\n",
      "Training loss:1.7583823204040527\n",
      "Training loss:1.6917455196380615\n",
      "Training loss:1.5625715255737305\n",
      "Training loss:1.7400792837142944\n",
      "Training loss:1.6233205795288086\n",
      "Training loss:1.5872565507888794\n",
      "Training loss:1.702997088432312\n",
      "Training loss:1.7031211853027344\n",
      "Training loss:1.6927629709243774\n",
      "Training loss:1.7469691038131714\n",
      "Training loss:1.595462441444397\n",
      "Training loss:1.6057966947555542\n",
      "Training loss:1.5869518518447876\n",
      "Training loss:1.6571846008300781\n",
      "Training loss:1.7461857795715332\n",
      "Training loss:1.7162648439407349\n",
      "Training loss:1.6796823740005493\n",
      "Training loss:1.6531721353530884\n",
      "Training loss:1.6964155435562134\n",
      "Training loss:1.700662612915039\n",
      "Training loss:1.6698564291000366\n",
      "Training loss:1.640333890914917\n",
      "Training loss:1.7243084907531738\n",
      "Training loss:1.724758505821228\n",
      "Training loss:1.6251704692840576\n",
      "Training loss:1.6458323001861572\n",
      "Training loss:1.6741645336151123\n",
      "Training loss:1.6060950756072998\n",
      "Training loss:1.6654008626937866\n",
      "Training loss:1.6171969175338745\n",
      "Training loss:1.6555315256118774\n",
      "Training loss:1.6532806158065796\n",
      "Training loss:1.7102316617965698\n",
      "Training loss:1.6942951679229736\n",
      "Training loss:1.6481026411056519\n",
      "Training loss:1.6882202625274658\n",
      "Training loss:1.685258388519287\n",
      "Training loss:1.634094476699829\n",
      "Training loss:1.6720389127731323\n",
      "Training loss:1.7027382850646973\n",
      "Training loss:1.6005964279174805\n",
      "Training loss:1.6948546171188354\n",
      "Training loss:1.585843563079834\n",
      "Training loss:1.7289677858352661\n",
      "Training loss:1.7145471572875977\n",
      "Training loss:1.6742199659347534\n",
      "Training loss:1.6492594480514526\n",
      "Training loss:1.6550558805465698\n",
      "Training loss:1.6671043634414673\n",
      "Training loss:1.6091705560684204\n",
      "Training loss:1.6738816499710083\n",
      "Training loss:1.6403584480285645\n",
      "Training loss:1.6389766931533813\n",
      "Training loss:1.63028883934021\n",
      "Training loss:1.6646339893341064\n",
      "Training loss:1.6793781518936157\n",
      "Training loss:1.6487566232681274\n",
      "Training loss:1.6380847692489624\n",
      "Training loss:1.7172034978866577\n",
      "Training loss:1.6859124898910522\n",
      "Training loss:1.7027660608291626\n",
      "Training loss:1.6425902843475342\n",
      "Training loss:1.7484208345413208\n",
      "Training loss:1.7413424253463745\n",
      "Training loss:1.679669737815857\n",
      "Training loss:1.6008410453796387\n",
      "Training loss:1.6773433685302734\n",
      "Training loss:1.6351172924041748\n",
      "Training loss:1.6928893327713013\n",
      "Training loss:1.6943702697753906\n",
      "Training loss:1.6767972707748413\n",
      "Training loss:1.7070218324661255\n",
      "Training loss:1.657378911972046\n",
      "Training loss:1.6748788356781006\n",
      "Training loss:1.5809335708618164\n",
      "Training loss:1.6192536354064941\n",
      "Training loss:1.6087146997451782\n",
      "Epoch: 0071 loss_train: 156.462504 acc_train: 0.386104 loss_val: 19.793586 acc_val: 0.388000 time: 180127.935071s\n",
      "Training loss:1.6587435007095337\n",
      "Training loss:1.7277075052261353\n",
      "Training loss:1.7326860427856445\n",
      "Training loss:1.6084719896316528\n",
      "Training loss:1.685454249382019\n",
      "Training loss:1.6556589603424072\n",
      "Training loss:1.676917552947998\n",
      "Training loss:1.5795236825942993\n",
      "Training loss:1.6127774715423584\n",
      "Training loss:1.712438941001892\n",
      "Training loss:1.7100995779037476\n",
      "Training loss:1.6575050354003906\n",
      "Training loss:1.7321807146072388\n",
      "Training loss:1.7205753326416016\n",
      "Training loss:1.6627943515777588\n",
      "Training loss:1.5846593379974365\n",
      "Training loss:1.6474874019622803\n",
      "Training loss:1.7042313814163208\n",
      "Training loss:1.6543693542480469\n",
      "Training loss:1.68845796585083\n",
      "Training loss:1.6796464920043945\n",
      "Training loss:1.6431269645690918\n",
      "Training loss:1.6583161354064941\n",
      "Training loss:1.6371082067489624\n",
      "Training loss:1.6540828943252563\n",
      "Training loss:1.6702512502670288\n",
      "Training loss:1.644334077835083\n",
      "Training loss:1.6000776290893555\n",
      "Training loss:1.6926926374435425\n",
      "Training loss:1.6684638261795044\n",
      "Training loss:1.6288286447525024\n",
      "Training loss:1.6965296268463135\n",
      "Training loss:1.6541187763214111\n",
      "Training loss:1.750878930091858\n",
      "Training loss:1.640084147453308\n",
      "Training loss:1.6517812013626099\n",
      "Training loss:1.647067904472351\n",
      "Training loss:1.6919976472854614\n",
      "Training loss:1.6212842464447021\n",
      "Training loss:1.629912257194519\n",
      "Training loss:1.6135272979736328\n",
      "Training loss:1.660108208656311\n",
      "Training loss:1.6444047689437866\n",
      "Training loss:1.6376636028289795\n",
      "Training loss:1.6939300298690796\n",
      "Training loss:1.658888816833496\n",
      "Training loss:1.712708830833435\n",
      "Training loss:1.684740662574768\n",
      "Training loss:1.7029176950454712\n",
      "Training loss:1.6082031726837158\n",
      "Training loss:1.7149039506912231\n",
      "Training loss:1.6273629665374756\n",
      "Training loss:1.651580572128296\n",
      "Training loss:1.6497775316238403\n",
      "Training loss:1.5809663534164429\n",
      "Training loss:1.6841418743133545\n",
      "Training loss:1.6937777996063232\n",
      "Training loss:1.737128734588623\n",
      "Training loss:1.6587496995925903\n",
      "Training loss:1.6861408948898315\n",
      "Training loss:1.6814736127853394\n",
      "Training loss:1.6644229888916016\n",
      "Training loss:1.6148360967636108\n",
      "Training loss:1.6423287391662598\n",
      "Training loss:1.6750234365463257\n",
      "Training loss:1.7074965238571167\n",
      "Training loss:1.688989520072937\n",
      "Training loss:1.7100334167480469\n",
      "Training loss:1.6972602605819702\n",
      "Training loss:1.627220630645752\n",
      "Training loss:1.6561721563339233\n",
      "Training loss:1.6669517755508423\n",
      "Training loss:1.6614432334899902\n",
      "Training loss:1.6084191799163818\n",
      "Training loss:1.6734145879745483\n",
      "Training loss:1.6471525430679321\n",
      "Training loss:1.6235653162002563\n",
      "Training loss:1.6450302600860596\n",
      "Training loss:1.585352897644043\n",
      "Training loss:1.6718262434005737\n",
      "Training loss:1.6728676557540894\n",
      "Training loss:1.644844651222229\n",
      "Training loss:1.6066139936447144\n",
      "Training loss:1.661766767501831\n",
      "Training loss:1.6737263202667236\n",
      "Training loss:1.6124659776687622\n",
      "Training loss:1.5924419164657593\n",
      "Training loss:1.6912546157836914\n",
      "Training loss:1.63744056224823\n",
      "Training loss:1.7048780918121338\n",
      "Training loss:1.62675142288208\n",
      "Training loss:1.5968477725982666\n",
      "Training loss:1.6583693027496338\n",
      "Training loss:1.6808232069015503\n",
      "Epoch: 0072 loss_train: 156.084453 acc_train: 0.389208 loss_val: 19.802157 acc_val: 0.390667 time: 182754.660953s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6661646366119385\n",
      "Training loss:1.676279902458191\n",
      "Training loss:1.619396686553955\n",
      "Training loss:1.637069582939148\n",
      "Training loss:1.6947932243347168\n",
      "Training loss:1.6359639167785645\n",
      "Training loss:1.703751564025879\n",
      "Training loss:1.6667392253875732\n",
      "Training loss:1.6913026571273804\n",
      "Training loss:1.6466741561889648\n",
      "Training loss:1.6937954425811768\n",
      "Training loss:1.6820048093795776\n",
      "Training loss:1.6233162879943848\n",
      "Training loss:1.6004823446273804\n",
      "Training loss:1.6475480794906616\n",
      "Training loss:1.6322909593582153\n",
      "Training loss:1.6300160884857178\n",
      "Training loss:1.6861066818237305\n",
      "Training loss:1.7488044500350952\n",
      "Training loss:1.6411203145980835\n",
      "Training loss:1.670583963394165\n",
      "Training loss:1.7256895303726196\n",
      "Training loss:1.6885790824890137\n",
      "Training loss:1.5691664218902588\n",
      "Training loss:1.6897220611572266\n",
      "Training loss:1.6083484888076782\n",
      "Training loss:1.6925013065338135\n",
      "Training loss:1.6944694519042969\n",
      "Training loss:1.671974539756775\n",
      "Training loss:1.661782145500183\n",
      "Training loss:1.6300996541976929\n",
      "Training loss:1.664592981338501\n",
      "Training loss:1.6673482656478882\n",
      "Training loss:1.5876739025115967\n",
      "Training loss:1.6468584537506104\n",
      "Training loss:1.695188045501709\n",
      "Training loss:1.6715567111968994\n",
      "Training loss:1.6649885177612305\n",
      "Training loss:1.6407946348190308\n",
      "Training loss:1.5655357837677002\n",
      "Training loss:1.6439180374145508\n",
      "Training loss:1.5918585062026978\n",
      "Training loss:1.6100655794143677\n",
      "Training loss:1.718351125717163\n",
      "Training loss:1.697554349899292\n",
      "Training loss:1.6821892261505127\n",
      "Training loss:1.6551580429077148\n",
      "Training loss:1.6391713619232178\n",
      "Training loss:1.57687509059906\n",
      "Training loss:1.6178425550460815\n",
      "Training loss:1.6277915239334106\n",
      "Training loss:1.6023019552230835\n",
      "Training loss:1.621634602546692\n",
      "Training loss:1.686326503753662\n",
      "Training loss:1.756248116493225\n",
      "Training loss:1.706695318222046\n",
      "Training loss:1.6657731533050537\n",
      "Training loss:1.7225780487060547\n",
      "Training loss:1.6332459449768066\n",
      "Training loss:1.6840016841888428\n",
      "Training loss:1.6891498565673828\n",
      "Training loss:1.635413646697998\n",
      "Training loss:1.5646395683288574\n",
      "Training loss:1.6585760116577148\n",
      "Training loss:1.5841549634933472\n",
      "Training loss:1.6082003116607666\n",
      "Training loss:1.6583997011184692\n",
      "Training loss:1.6428427696228027\n",
      "Training loss:1.693433165550232\n",
      "Training loss:1.7399277687072754\n",
      "Training loss:1.7091318368911743\n",
      "Training loss:1.6137346029281616\n",
      "Training loss:1.7744989395141602\n",
      "Training loss:1.6640994548797607\n",
      "Training loss:1.6263102293014526\n",
      "Training loss:1.6656367778778076\n",
      "Training loss:1.6234331130981445\n",
      "Training loss:1.6897821426391602\n",
      "Training loss:1.656469464302063\n",
      "Training loss:1.622306227684021\n",
      "Training loss:1.6653151512145996\n",
      "Training loss:1.6952112913131714\n",
      "Training loss:1.621497631072998\n",
      "Training loss:1.7077199220657349\n",
      "Training loss:1.6955314874649048\n",
      "Training loss:1.6788523197174072\n",
      "Training loss:1.7041943073272705\n",
      "Training loss:1.6901077032089233\n",
      "Training loss:1.65549635887146\n",
      "Training loss:1.597907304763794\n",
      "Training loss:1.7246811389923096\n",
      "Training loss:1.615165114402771\n",
      "Training loss:1.6014575958251953\n",
      "Training loss:1.6978415250778198\n",
      "Epoch: 0073 loss_train: 155.941775 acc_train: 0.388792 loss_val: 19.880267 acc_val: 0.386000 time: 185518.925749s\n",
      "Training loss:1.6606520414352417\n",
      "Training loss:1.6261534690856934\n",
      "Training loss:1.753831148147583\n",
      "Training loss:1.6892262697219849\n",
      "Training loss:1.6534373760223389\n",
      "Training loss:1.641501545906067\n",
      "Training loss:1.6954374313354492\n",
      "Training loss:1.6696088314056396\n",
      "Training loss:1.7070982456207275\n",
      "Training loss:1.6777095794677734\n",
      "Training loss:1.6879889965057373\n",
      "Training loss:1.6332850456237793\n",
      "Training loss:1.5931576490402222\n",
      "Training loss:1.6440085172653198\n",
      "Training loss:1.6240049600601196\n",
      "Training loss:1.579546570777893\n",
      "Training loss:1.716329574584961\n",
      "Training loss:1.660049319267273\n",
      "Training loss:1.6631256341934204\n",
      "Training loss:1.6635721921920776\n",
      "Training loss:1.6417624950408936\n",
      "Training loss:1.682070016860962\n",
      "Training loss:1.6604359149932861\n",
      "Training loss:1.59462308883667\n",
      "Training loss:1.6702361106872559\n",
      "Training loss:1.715012788772583\n",
      "Training loss:1.5979586839675903\n",
      "Training loss:1.6553130149841309\n",
      "Training loss:1.613602638244629\n",
      "Training loss:1.71806001663208\n",
      "Training loss:1.6974644660949707\n",
      "Training loss:1.627020239830017\n",
      "Training loss:1.6135613918304443\n",
      "Training loss:1.6288095712661743\n",
      "Training loss:1.6068553924560547\n",
      "Training loss:1.6824800968170166\n",
      "Training loss:1.6953177452087402\n",
      "Training loss:1.6451650857925415\n",
      "Training loss:1.6834821701049805\n",
      "Training loss:1.6616485118865967\n",
      "Training loss:1.6704862117767334\n",
      "Training loss:1.6417412757873535\n",
      "Training loss:1.6687198877334595\n",
      "Training loss:1.5845662355422974\n",
      "Training loss:1.6467887163162231\n",
      "Training loss:1.6754974126815796\n",
      "Training loss:1.6158767938613892\n",
      "Training loss:1.665765404701233\n",
      "Training loss:1.6767386198043823\n",
      "Training loss:1.6023727655410767\n",
      "Training loss:1.6348769664764404\n",
      "Training loss:1.7059084177017212\n",
      "Training loss:1.759613037109375\n",
      "Training loss:1.6838653087615967\n",
      "Training loss:1.6922202110290527\n",
      "Training loss:1.604771375656128\n",
      "Training loss:1.6342746019363403\n",
      "Training loss:1.6346627473831177\n",
      "Training loss:1.6618897914886475\n",
      "Training loss:1.583024501800537\n",
      "Training loss:1.6789535284042358\n",
      "Training loss:1.693687915802002\n",
      "Training loss:1.610723853111267\n",
      "Training loss:1.6478028297424316\n",
      "Training loss:1.6401727199554443\n",
      "Training loss:1.611586570739746\n",
      "Training loss:1.5900832414627075\n",
      "Training loss:1.707932472229004\n",
      "Training loss:1.714036464691162\n",
      "Training loss:1.622908592224121\n",
      "Training loss:1.6578893661499023\n",
      "Training loss:1.607839822769165\n",
      "Training loss:1.6129660606384277\n",
      "Training loss:1.6457968950271606\n",
      "Training loss:1.6124533414840698\n",
      "Training loss:1.6169308423995972\n",
      "Training loss:1.7191203832626343\n",
      "Training loss:1.668340802192688\n",
      "Training loss:1.5701502561569214\n",
      "Training loss:1.6858242750167847\n",
      "Training loss:1.6169846057891846\n",
      "Training loss:1.5984688997268677\n",
      "Training loss:1.5675761699676514\n",
      "Training loss:1.5549736022949219\n",
      "Training loss:1.6966016292572021\n",
      "Training loss:1.6804295778274536\n",
      "Training loss:1.5731704235076904\n",
      "Training loss:1.6372838020324707\n",
      "Training loss:1.6489801406860352\n",
      "Training loss:1.662355661392212\n",
      "Training loss:1.6700341701507568\n",
      "Training loss:1.655317783355713\n",
      "Training loss:1.698276400566101\n",
      "Training loss:1.6400352716445923\n",
      "Epoch: 0074 loss_train: 155.189952 acc_train: 0.393958 loss_val: 19.665411 acc_val: 0.393500 time: 188160.387720s\n",
      "Training loss:1.6040141582489014\n",
      "Training loss:1.6790014505386353\n",
      "Training loss:1.6475390195846558\n",
      "Training loss:1.5526918172836304\n",
      "Training loss:1.6205999851226807\n",
      "Training loss:1.5924420356750488\n",
      "Training loss:1.6556545495986938\n",
      "Training loss:1.6259359121322632\n",
      "Training loss:1.6338595151901245\n",
      "Training loss:1.6855032444000244\n",
      "Training loss:1.681545376777649\n",
      "Training loss:1.6265722513198853\n",
      "Training loss:1.6613126993179321\n",
      "Training loss:1.6042932271957397\n",
      "Training loss:1.6491503715515137\n",
      "Training loss:1.6661806106567383\n",
      "Training loss:1.6286839246749878\n",
      "Training loss:1.6903430223464966\n",
      "Training loss:1.6943421363830566\n",
      "Training loss:1.6729642152786255\n",
      "Training loss:1.6979061365127563\n",
      "Training loss:1.6413495540618896\n",
      "Training loss:1.7625129222869873\n",
      "Training loss:1.7043360471725464\n",
      "Training loss:1.7548396587371826\n",
      "Training loss:1.7207016944885254\n",
      "Training loss:1.690730333328247\n",
      "Training loss:1.6488641500473022\n",
      "Training loss:1.6493655443191528\n",
      "Training loss:1.631364345550537\n",
      "Training loss:1.6490520238876343\n",
      "Training loss:1.6413997411727905\n",
      "Training loss:1.6344176530838013\n",
      "Training loss:1.6108365058898926\n",
      "Training loss:1.6508327722549438\n",
      "Training loss:1.6343052387237549\n",
      "Training loss:1.6046043634414673\n",
      "Training loss:1.6976820230484009\n",
      "Training loss:1.7164361476898193\n",
      "Training loss:1.5764793157577515\n",
      "Training loss:1.6557430028915405\n",
      "Training loss:1.700382113456726\n",
      "Training loss:1.6535426378250122\n",
      "Training loss:1.6627287864685059\n",
      "Training loss:1.6649709939956665\n",
      "Training loss:1.6492438316345215\n",
      "Training loss:1.6157134771347046\n",
      "Training loss:1.6860812902450562\n",
      "Training loss:1.6180654764175415\n",
      "Training loss:1.6082632541656494\n",
      "Training loss:1.658239722251892\n",
      "Training loss:1.6387947797775269\n",
      "Training loss:1.53694748878479\n",
      "Training loss:1.6231611967086792\n",
      "Training loss:1.6117148399353027\n",
      "Training loss:1.6404544115066528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.602673888206482\n",
      "Training loss:1.7174485921859741\n",
      "Training loss:1.643619418144226\n",
      "Training loss:1.5923022031784058\n",
      "Training loss:1.7343591451644897\n",
      "Training loss:1.5746334791183472\n",
      "Training loss:1.7333890199661255\n",
      "Training loss:1.6282051801681519\n",
      "Training loss:1.7631709575653076\n",
      "Training loss:1.6729202270507812\n",
      "Training loss:1.6628700494766235\n",
      "Training loss:1.6910552978515625\n",
      "Training loss:1.680946946144104\n",
      "Training loss:1.6762278079986572\n",
      "Training loss:1.656798005104065\n",
      "Training loss:1.6152421236038208\n",
      "Training loss:1.652421474456787\n",
      "Training loss:1.602384328842163\n",
      "Training loss:1.680816650390625\n",
      "Training loss:1.6269631385803223\n",
      "Training loss:1.56474769115448\n",
      "Training loss:1.5834532976150513\n",
      "Training loss:1.6328108310699463\n",
      "Training loss:1.6261578798294067\n",
      "Training loss:1.6622838973999023\n",
      "Training loss:1.6712337732315063\n",
      "Training loss:1.6813702583312988\n",
      "Training loss:1.6604971885681152\n",
      "Training loss:1.5593513250350952\n",
      "Training loss:1.6723055839538574\n",
      "Training loss:1.6673054695129395\n",
      "Training loss:1.680521845817566\n",
      "Training loss:1.687457799911499\n",
      "Training loss:1.6494067907333374\n",
      "Training loss:1.7053035497665405\n",
      "Training loss:1.6400524377822876\n",
      "Training loss:1.6794885396957397\n",
      "Training loss:1.6444849967956543\n",
      "Epoch: 0075 loss_train: 155.291374 acc_train: 0.390979 loss_val: 19.528892 acc_val: 0.394000 time: 190722.619549s\n",
      "Training loss:1.6268693208694458\n",
      "Training loss:1.6545308828353882\n",
      "Training loss:1.6348326206207275\n",
      "Training loss:1.6048665046691895\n",
      "Training loss:1.6023144721984863\n",
      "Training loss:1.6713242530822754\n",
      "Training loss:1.6261305809020996\n",
      "Training loss:1.6607944965362549\n",
      "Training loss:1.6381862163543701\n",
      "Training loss:1.6343588829040527\n",
      "Training loss:1.7297236919403076\n",
      "Training loss:1.5709933042526245\n",
      "Training loss:1.6691819429397583\n",
      "Training loss:1.6035614013671875\n",
      "Training loss:1.5874509811401367\n",
      "Training loss:1.6251494884490967\n",
      "Training loss:1.733665108680725\n",
      "Training loss:1.6802417039871216\n",
      "Training loss:1.6404471397399902\n",
      "Training loss:1.6509069204330444\n",
      "Training loss:1.611738681793213\n",
      "Training loss:1.6389503479003906\n",
      "Training loss:1.6098966598510742\n",
      "Training loss:1.662528395652771\n",
      "Training loss:1.6053212881088257\n",
      "Training loss:1.6404852867126465\n",
      "Training loss:1.6665416955947876\n",
      "Training loss:1.6475958824157715\n",
      "Training loss:1.6055785417556763\n",
      "Training loss:1.6666005849838257\n",
      "Training loss:1.6226165294647217\n",
      "Training loss:1.6189149618148804\n",
      "Training loss:1.6476130485534668\n",
      "Training loss:1.6321035623550415\n",
      "Training loss:1.6510509252548218\n",
      "Training loss:1.5775948762893677\n",
      "Training loss:1.6201701164245605\n",
      "Training loss:1.6455518007278442\n",
      "Training loss:1.7433031797409058\n",
      "Training loss:1.6451845169067383\n",
      "Training loss:1.5931161642074585\n",
      "Training loss:1.731398344039917\n",
      "Training loss:1.6961737871170044\n",
      "Training loss:1.6081407070159912\n",
      "Training loss:1.6785645484924316\n",
      "Training loss:1.6502262353897095\n",
      "Training loss:1.6130326986312866\n",
      "Training loss:1.7346577644348145\n",
      "Training loss:1.6822340488433838\n",
      "Training loss:1.644095778465271\n",
      "Training loss:1.62774658203125\n",
      "Training loss:1.7755558490753174\n",
      "Training loss:1.7021960020065308\n",
      "Training loss:1.624172329902649\n",
      "Training loss:1.680540680885315\n",
      "Training loss:1.6736607551574707\n",
      "Training loss:1.666772484779358\n",
      "Training loss:1.7242655754089355\n",
      "Training loss:1.5938162803649902\n",
      "Training loss:1.654698133468628\n",
      "Training loss:1.679790735244751\n",
      "Training loss:1.739944338798523\n",
      "Training loss:1.6189476251602173\n",
      "Training loss:1.6095800399780273\n",
      "Training loss:1.662104845046997\n",
      "Training loss:1.642701268196106\n",
      "Training loss:1.6328622102737427\n",
      "Training loss:1.6141597032546997\n",
      "Training loss:1.7243192195892334\n",
      "Training loss:1.6257908344268799\n",
      "Training loss:1.5861228704452515\n",
      "Training loss:1.5969280004501343\n",
      "Training loss:1.6645575761795044\n",
      "Training loss:1.6164634227752686\n",
      "Training loss:1.6397184133529663\n",
      "Training loss:1.6870050430297852\n",
      "Training loss:1.6869683265686035\n",
      "Training loss:1.608233094215393\n",
      "Training loss:1.7311208248138428\n",
      "Training loss:1.643462896347046\n",
      "Training loss:1.671237587928772\n",
      "Training loss:1.6548240184783936\n",
      "Training loss:1.5662918090820312\n",
      "Training loss:1.6392070055007935\n",
      "Training loss:1.7044596672058105\n",
      "Training loss:1.6427547931671143\n",
      "Training loss:1.6396489143371582\n",
      "Training loss:1.6130805015563965\n",
      "Training loss:1.7292733192443848\n",
      "Training loss:1.6090037822723389\n",
      "Training loss:1.6367788314819336\n",
      "Training loss:1.572525143623352\n",
      "Training loss:1.628333568572998\n",
      "Training loss:1.6685857772827148\n",
      "Epoch: 0076 loss_train: 154.946722 acc_train: 0.395104 loss_val: 19.660976 acc_val: 0.390000 time: 193507.256513s\n",
      "Training loss:1.6499743461608887\n",
      "Training loss:1.6142492294311523\n",
      "Training loss:1.6135090589523315\n",
      "Training loss:1.6877961158752441\n",
      "Training loss:1.678518533706665\n",
      "Training loss:1.5823487043380737\n",
      "Training loss:1.5988377332687378\n",
      "Training loss:1.613031268119812\n",
      "Training loss:1.669914722442627\n",
      "Training loss:1.6245732307434082\n",
      "Training loss:1.6316994428634644\n",
      "Training loss:1.5884863138198853\n",
      "Training loss:1.6008548736572266\n",
      "Training loss:1.6482897996902466\n",
      "Training loss:1.591529130935669\n",
      "Training loss:1.6674730777740479\n",
      "Training loss:1.6020153760910034\n",
      "Training loss:1.687796711921692\n",
      "Training loss:1.6533734798431396\n",
      "Training loss:1.6633872985839844\n",
      "Training loss:1.5914698839187622\n",
      "Training loss:1.6706290245056152\n",
      "Training loss:1.6262582540512085\n",
      "Training loss:1.6665315628051758\n",
      "Training loss:1.6856493949890137\n",
      "Training loss:1.5881012678146362\n",
      "Training loss:1.6568500995635986\n",
      "Training loss:1.664233922958374\n",
      "Training loss:1.6051430702209473\n",
      "Training loss:1.686680555343628\n",
      "Training loss:1.620933175086975\n",
      "Training loss:1.6214206218719482\n",
      "Training loss:1.650747537612915\n",
      "Training loss:1.5496803522109985\n",
      "Training loss:1.6543011665344238\n",
      "Training loss:1.5935286283493042\n",
      "Training loss:1.6553552150726318\n",
      "Training loss:1.624841332435608\n",
      "Training loss:1.6467268466949463\n",
      "Training loss:1.667391061782837\n",
      "Training loss:1.6896603107452393\n",
      "Training loss:1.6452429294586182\n",
      "Training loss:1.5514514446258545\n",
      "Training loss:1.6323270797729492\n",
      "Training loss:1.6623303890228271\n",
      "Training loss:1.6736546754837036\n",
      "Training loss:1.5871917009353638\n",
      "Training loss:1.6312894821166992\n",
      "Training loss:1.6391750574111938\n",
      "Training loss:1.6113418340682983\n",
      "Training loss:1.6641347408294678\n",
      "Training loss:1.6893595457077026\n",
      "Training loss:1.628165364265442\n",
      "Training loss:1.7458876371383667\n",
      "Training loss:1.7074964046478271\n",
      "Training loss:1.6387845277786255\n",
      "Training loss:1.5796098709106445\n",
      "Training loss:1.7158586978912354\n",
      "Training loss:1.6247326135635376\n",
      "Training loss:1.6564480066299438\n",
      "Training loss:1.6573283672332764\n",
      "Training loss:1.6465740203857422\n",
      "Training loss:1.6995218992233276\n",
      "Training loss:1.67427396774292\n",
      "Training loss:1.6421353816986084\n",
      "Training loss:1.7270632982254028\n",
      "Training loss:1.67874014377594\n",
      "Training loss:1.6476167440414429\n",
      "Training loss:1.6350027322769165\n",
      "Training loss:1.6175233125686646\n",
      "Training loss:1.6936237812042236\n",
      "Training loss:1.6076340675354004\n",
      "Training loss:1.6538699865341187\n",
      "Training loss:1.6265841722488403\n",
      "Training loss:1.6504682302474976\n",
      "Training loss:1.6873453855514526\n",
      "Training loss:1.6508764028549194\n",
      "Training loss:1.5904951095581055\n",
      "Training loss:1.6630891561508179\n",
      "Training loss:1.5992796421051025\n",
      "Training loss:1.628061294555664\n",
      "Training loss:1.6711982488632202\n",
      "Training loss:1.6102023124694824\n",
      "Training loss:1.6314557790756226\n",
      "Training loss:1.6227295398712158\n",
      "Training loss:1.6955653429031372\n",
      "Training loss:1.5941545963287354\n",
      "Training loss:1.6801106929779053\n",
      "Training loss:1.7157810926437378\n",
      "Training loss:1.5991625785827637\n",
      "Training loss:1.6308765411376953\n",
      "Training loss:1.6460442543029785\n",
      "Training loss:1.672465443611145\n",
      "Training loss:1.688959002494812\n",
      "Epoch: 0077 loss_train: 154.504086 acc_train: 0.396479 loss_val: 19.684255 acc_val: 0.398667 time: 196253.091710s\n",
      "Training loss:1.6488113403320312\n",
      "Training loss:1.7102199792861938\n",
      "Training loss:1.7023621797561646\n",
      "Training loss:1.6190515756607056\n",
      "Training loss:1.598084568977356\n",
      "Training loss:1.5142368078231812\n",
      "Training loss:1.5547208786010742\n",
      "Training loss:1.7668145895004272\n",
      "Training loss:1.6702301502227783\n",
      "Training loss:1.6881968975067139\n",
      "Training loss:1.668662428855896\n",
      "Training loss:1.673518180847168\n",
      "Training loss:1.6128968000411987\n",
      "Training loss:1.7141166925430298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6805188655853271\n",
      "Training loss:1.674601674079895\n",
      "Training loss:1.594306468963623\n",
      "Training loss:1.6684074401855469\n",
      "Training loss:1.6352524757385254\n",
      "Training loss:1.664634108543396\n",
      "Training loss:1.7323193550109863\n",
      "Training loss:1.6937204599380493\n",
      "Training loss:1.6545017957687378\n",
      "Training loss:1.6719309091567993\n",
      "Training loss:1.6478556394577026\n",
      "Training loss:1.6763737201690674\n",
      "Training loss:1.6412931680679321\n",
      "Training loss:1.675521731376648\n",
      "Training loss:1.5961391925811768\n",
      "Training loss:1.6308737993240356\n",
      "Training loss:1.705184817314148\n",
      "Training loss:1.6552348136901855\n",
      "Training loss:1.6515274047851562\n",
      "Training loss:1.7088899612426758\n",
      "Training loss:1.6332134008407593\n",
      "Training loss:1.720211148262024\n",
      "Training loss:1.761884093284607\n",
      "Training loss:1.6962603330612183\n",
      "Training loss:1.6906497478485107\n",
      "Training loss:1.6856340169906616\n",
      "Training loss:1.693687915802002\n",
      "Training loss:1.6992747783660889\n",
      "Training loss:1.6174787282943726\n",
      "Training loss:1.6054214239120483\n",
      "Training loss:1.639858365058899\n",
      "Training loss:1.748170018196106\n",
      "Training loss:1.622159719467163\n",
      "Training loss:1.6608331203460693\n",
      "Training loss:1.6246875524520874\n",
      "Training loss:1.5859512090682983\n",
      "Training loss:1.666899561882019\n",
      "Training loss:1.6765055656433105\n",
      "Training loss:1.7046611309051514\n",
      "Training loss:1.6087148189544678\n",
      "Training loss:1.6922004222869873\n",
      "Training loss:1.5935505628585815\n",
      "Training loss:1.6161224842071533\n",
      "Training loss:1.710961103439331\n",
      "Training loss:1.5855965614318848\n",
      "Training loss:1.6673883199691772\n",
      "Training loss:1.6575623750686646\n",
      "Training loss:1.6900968551635742\n",
      "Training loss:1.6067850589752197\n",
      "Training loss:1.689125895500183\n",
      "Training loss:1.7031859159469604\n",
      "Training loss:1.6205295324325562\n",
      "Training loss:1.6657406091690063\n",
      "Training loss:1.643584966659546\n",
      "Training loss:1.6028989553451538\n",
      "Training loss:1.6372407674789429\n",
      "Training loss:1.6878447532653809\n",
      "Training loss:1.7777918577194214\n",
      "Training loss:1.5958542823791504\n",
      "Training loss:1.6970828771591187\n",
      "Training loss:1.6946539878845215\n",
      "Training loss:1.6305490732192993\n",
      "Training loss:1.6298304796218872\n",
      "Training loss:1.6777732372283936\n",
      "Training loss:1.6233091354370117\n",
      "Training loss:1.6342459917068481\n",
      "Training loss:1.6747372150421143\n",
      "Training loss:1.7260416746139526\n",
      "Training loss:1.7070313692092896\n",
      "Training loss:1.6422404050827026\n",
      "Training loss:1.5587271451950073\n",
      "Training loss:1.7031807899475098\n",
      "Training loss:1.6931227445602417\n",
      "Training loss:1.698649287223816\n",
      "Training loss:1.6245211362838745\n",
      "Training loss:1.5114185810089111\n",
      "Training loss:1.6585733890533447\n",
      "Training loss:1.592765212059021\n",
      "Training loss:1.5612130165100098\n",
      "Training loss:1.762229561805725\n",
      "Epoch: 0078 loss_train: 155.889125 acc_train: 0.392021 loss_val: 19.850064 acc_val: 0.388500 time: 198879.565613s\n",
      "Training loss:1.6843481063842773\n",
      "Training loss:1.6184844970703125\n",
      "Training loss:1.606481909751892\n",
      "Training loss:1.6125189065933228\n",
      "Training loss:1.6030688285827637\n",
      "Training loss:1.6528761386871338\n",
      "Training loss:1.6191571950912476\n",
      "Training loss:1.5909993648529053\n",
      "Training loss:1.5772148370742798\n",
      "Training loss:1.6309658288955688\n",
      "Training loss:1.6391700506210327\n",
      "Training loss:1.6226848363876343\n",
      "Training loss:1.654837965965271\n",
      "Training loss:1.6433566808700562\n",
      "Training loss:1.618933916091919\n",
      "Training loss:1.6179035902023315\n",
      "Training loss:1.6399054527282715\n",
      "Training loss:1.6149067878723145\n",
      "Training loss:1.6185907125473022\n",
      "Training loss:1.6344916820526123\n",
      "Training loss:1.6164604425430298\n",
      "Training loss:1.6441104412078857\n",
      "Training loss:1.619033932685852\n",
      "Training loss:1.72150719165802\n",
      "Training loss:1.64948570728302\n",
      "Training loss:1.631000280380249\n",
      "Training loss:1.6215107440948486\n",
      "Training loss:1.6766414642333984\n",
      "Training loss:1.618456482887268\n",
      "Training loss:1.636610984802246\n",
      "Training loss:1.5744789838790894\n",
      "Training loss:1.650284767150879\n",
      "Training loss:1.574599027633667\n",
      "Training loss:1.645445466041565\n",
      "Training loss:1.634666085243225\n",
      "Training loss:1.6278740167617798\n",
      "Training loss:1.6939160823822021\n",
      "Training loss:1.6440565586090088\n",
      "Training loss:1.7225112915039062\n",
      "Training loss:1.7170484066009521\n",
      "Training loss:1.6927227973937988\n",
      "Training loss:1.6559975147247314\n",
      "Training loss:1.622063159942627\n",
      "Training loss:1.6696943044662476\n",
      "Training loss:1.6285927295684814\n",
      "Training loss:1.6313934326171875\n",
      "Training loss:1.5876789093017578\n",
      "Training loss:1.6730048656463623\n",
      "Training loss:1.6001454591751099\n",
      "Training loss:1.6930711269378662\n",
      "Training loss:1.6232473850250244\n",
      "Training loss:1.6774474382400513\n",
      "Training loss:1.5871782302856445\n",
      "Training loss:1.6869155168533325\n",
      "Training loss:1.656309962272644\n",
      "Training loss:1.5915522575378418\n",
      "Training loss:1.6242198944091797\n",
      "Training loss:1.688026785850525\n",
      "Training loss:1.6020320653915405\n",
      "Training loss:1.6478135585784912\n",
      "Training loss:1.7270547151565552\n",
      "Training loss:1.5900315046310425\n",
      "Training loss:1.6401804685592651\n",
      "Training loss:1.7127755880355835\n",
      "Training loss:1.6328341960906982\n",
      "Training loss:1.6581616401672363\n",
      "Training loss:1.6326936483383179\n",
      "Training loss:1.6271220445632935\n",
      "Training loss:1.5668445825576782\n",
      "Training loss:1.6691157817840576\n",
      "Training loss:1.582056999206543\n",
      "Training loss:1.6850981712341309\n",
      "Training loss:1.6552386283874512\n",
      "Training loss:1.649201512336731\n",
      "Training loss:1.6181695461273193\n",
      "Training loss:1.539270043373108\n",
      "Training loss:1.5811957120895386\n",
      "Training loss:1.5686489343643188\n",
      "Training loss:1.6330420970916748\n",
      "Training loss:1.6361569166183472\n",
      "Training loss:1.6692423820495605\n",
      "Training loss:1.6277236938476562\n",
      "Training loss:1.6550707817077637\n",
      "Training loss:1.5932050943374634\n",
      "Training loss:1.6356256008148193\n",
      "Training loss:1.6849714517593384\n",
      "Training loss:1.6246321201324463\n",
      "Training loss:1.6983791589736938\n",
      "Training loss:1.5777679681777954\n",
      "Training loss:1.6494159698486328\n",
      "Training loss:1.5984432697296143\n",
      "Training loss:1.6652145385742188\n",
      "Training loss:1.6053178310394287\n",
      "Training loss:1.607837200164795\n",
      "Epoch: 0079 loss_train: 153.757443 acc_train: 0.399062 loss_val: 19.511584 acc_val: 0.401500 time: 201736.361066s\n",
      "Training loss:1.7005414962768555\n",
      "Training loss:1.5879499912261963\n",
      "Training loss:1.645851492881775\n",
      "Training loss:1.605787754058838\n",
      "Training loss:1.6197052001953125\n",
      "Training loss:1.6245609521865845\n",
      "Training loss:1.5545152425765991\n",
      "Training loss:1.6472382545471191\n",
      "Training loss:1.6301226615905762\n",
      "Training loss:1.7038335800170898\n",
      "Training loss:1.7252862453460693\n",
      "Training loss:1.6047711372375488\n",
      "Training loss:1.6568180322647095\n",
      "Training loss:1.6107416152954102\n",
      "Training loss:1.6680573225021362\n",
      "Training loss:1.5849418640136719\n",
      "Training loss:1.6582891941070557\n",
      "Training loss:1.6056149005889893\n",
      "Training loss:1.7001487016677856\n",
      "Training loss:1.5866094827651978\n",
      "Training loss:1.5886967182159424\n",
      "Training loss:1.6124460697174072\n",
      "Training loss:1.5531896352767944\n",
      "Training loss:1.663694143295288\n",
      "Training loss:1.6726828813552856\n",
      "Training loss:1.6604002714157104\n",
      "Training loss:1.6593972444534302\n",
      "Training loss:1.7116791009902954\n",
      "Training loss:1.6046377420425415\n",
      "Training loss:1.5985661745071411\n",
      "Training loss:1.5591208934783936\n",
      "Training loss:1.6033296585083008\n",
      "Training loss:1.6692640781402588\n",
      "Training loss:1.611512541770935\n",
      "Training loss:1.555497407913208\n",
      "Training loss:1.5754956007003784\n",
      "Training loss:1.6358895301818848\n",
      "Training loss:1.6318333148956299\n",
      "Training loss:1.6635684967041016\n",
      "Training loss:1.6544550657272339\n",
      "Training loss:1.6117732524871826\n",
      "Training loss:1.5537835359573364\n",
      "Training loss:1.6182492971420288\n",
      "Training loss:1.6242083311080933\n",
      "Training loss:1.6115223169326782\n",
      "Training loss:1.5809606313705444\n",
      "Training loss:1.6488927602767944\n",
      "Training loss:1.6840206384658813\n",
      "Training loss:1.5914543867111206\n",
      "Training loss:1.6513023376464844\n",
      "Training loss:1.6237305402755737\n",
      "Training loss:1.673884391784668\n",
      "Training loss:1.6761856079101562\n",
      "Training loss:1.6368975639343262\n",
      "Training loss:1.5586366653442383\n",
      "Training loss:1.7308179140090942\n",
      "Training loss:1.5983281135559082\n",
      "Training loss:1.65900719165802\n",
      "Training loss:1.595538854598999\n",
      "Training loss:1.6785516738891602\n",
      "Training loss:1.5445263385772705\n",
      "Training loss:1.5378503799438477\n",
      "Training loss:1.6579197645187378\n",
      "Training loss:1.623376488685608\n",
      "Training loss:1.5603870153427124\n",
      "Training loss:1.6694047451019287\n",
      "Training loss:1.6435445547103882\n",
      "Training loss:1.6100327968597412\n",
      "Training loss:1.5871492624282837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6457469463348389\n",
      "Training loss:1.5963155031204224\n",
      "Training loss:1.6379731893539429\n",
      "Training loss:1.6415032148361206\n",
      "Training loss:1.6322259902954102\n",
      "Training loss:1.6668198108673096\n",
      "Training loss:1.677803635597229\n",
      "Training loss:1.6520867347717285\n",
      "Training loss:1.6244244575500488\n",
      "Training loss:1.6175035238265991\n",
      "Training loss:1.6801828145980835\n",
      "Training loss:1.691431999206543\n",
      "Training loss:1.6674443483352661\n",
      "Training loss:1.5809699296951294\n",
      "Training loss:1.6216964721679688\n",
      "Training loss:1.7059366703033447\n",
      "Training loss:1.623520851135254\n",
      "Training loss:1.6816952228546143\n",
      "Training loss:1.629023551940918\n",
      "Training loss:1.6744226217269897\n",
      "Training loss:1.682366132736206\n",
      "Training loss:1.6035398244857788\n",
      "Training loss:1.6642332077026367\n",
      "Training loss:1.681632161140442\n",
      "Training loss:1.516453742980957\n",
      "Epoch: 0080 loss_train: 153.345632 acc_train: 0.402583 loss_val: 19.378590 acc_val: 0.412667 time: 204320.798298s\n",
      "Training loss:1.630454421043396\n",
      "Training loss:1.6336250305175781\n",
      "Training loss:1.6726248264312744\n",
      "Training loss:1.6864752769470215\n",
      "Training loss:1.5819621086120605\n",
      "Training loss:1.6033467054367065\n",
      "Training loss:1.6385211944580078\n",
      "Training loss:1.660354495048523\n",
      "Training loss:1.6518932580947876\n",
      "Training loss:1.6231484413146973\n",
      "Training loss:1.691698431968689\n",
      "Training loss:1.647855520248413\n",
      "Training loss:1.5952091217041016\n",
      "Training loss:1.599855899810791\n",
      "Training loss:1.6762359142303467\n",
      "Training loss:1.6460593938827515\n",
      "Training loss:1.6607872247695923\n",
      "Training loss:1.5857882499694824\n",
      "Training loss:1.6460182666778564\n",
      "Training loss:1.6363354921340942\n",
      "Training loss:1.5853785276412964\n",
      "Training loss:1.5586328506469727\n",
      "Training loss:1.6678544282913208\n",
      "Training loss:1.5676833391189575\n",
      "Training loss:1.5751981735229492\n",
      "Training loss:1.6788249015808105\n",
      "Training loss:1.6469955444335938\n",
      "Training loss:1.6476324796676636\n",
      "Training loss:1.6305804252624512\n",
      "Training loss:1.6258379220962524\n",
      "Training loss:1.628751516342163\n",
      "Training loss:1.5561119318008423\n",
      "Training loss:1.6367850303649902\n",
      "Training loss:1.5648871660232544\n",
      "Training loss:1.6352198123931885\n",
      "Training loss:1.6079744100570679\n",
      "Training loss:1.6097261905670166\n",
      "Training loss:1.618991494178772\n",
      "Training loss:1.5946460962295532\n",
      "Training loss:1.679645299911499\n",
      "Training loss:1.6214245557785034\n",
      "Training loss:1.6070544719696045\n",
      "Training loss:1.625422477722168\n",
      "Training loss:1.6318117380142212\n",
      "Training loss:1.6560437679290771\n",
      "Training loss:1.6289758682250977\n",
      "Training loss:1.6423650979995728\n",
      "Training loss:1.65543794631958\n",
      "Training loss:1.662275791168213\n",
      "Training loss:1.6728092432022095\n",
      "Training loss:1.6648387908935547\n",
      "Training loss:1.6937202215194702\n",
      "Training loss:1.6488631963729858\n",
      "Training loss:1.6187572479248047\n",
      "Training loss:1.5949571132659912\n",
      "Training loss:1.695128083229065\n",
      "Training loss:1.671057939529419\n",
      "Training loss:1.6695795059204102\n",
      "Training loss:1.5855655670166016\n",
      "Training loss:1.6341227293014526\n",
      "Training loss:1.661346673965454\n",
      "Training loss:1.725024700164795\n",
      "Training loss:1.6479312181472778\n",
      "Training loss:1.6161367893218994\n",
      "Training loss:1.6104443073272705\n",
      "Training loss:1.6457879543304443\n",
      "Training loss:1.661536693572998\n",
      "Training loss:1.6688945293426514\n",
      "Training loss:1.6548584699630737\n",
      "Training loss:1.6126536130905151\n",
      "Training loss:1.6503052711486816\n",
      "Training loss:1.662335991859436\n",
      "Training loss:1.693801999092102\n",
      "Training loss:1.685245394706726\n",
      "Training loss:1.575077772140503\n",
      "Training loss:1.6484118700027466\n",
      "Training loss:1.647810459136963\n",
      "Training loss:1.6374504566192627\n",
      "Training loss:1.570268154144287\n",
      "Training loss:1.5915573835372925\n",
      "Training loss:1.605823040008545\n",
      "Training loss:1.5035878419876099\n",
      "Training loss:1.6060678958892822\n",
      "Training loss:1.5730141401290894\n",
      "Training loss:1.6192411184310913\n",
      "Training loss:1.6492772102355957\n",
      "Training loss:1.6918696165084839\n",
      "Training loss:1.5835003852844238\n",
      "Training loss:1.6648492813110352\n",
      "Training loss:1.6793277263641357\n",
      "Training loss:1.5877584218978882\n",
      "Training loss:1.593923568725586\n",
      "Training loss:1.676534652709961\n",
      "Training loss:1.624168872833252\n",
      "Epoch: 0081 loss_train: 153.491642 acc_train: 0.400062 loss_val: 19.473772 acc_val: 0.405000 time: 206909.058850s\n",
      "Training loss:1.6534693241119385\n",
      "Training loss:1.6055351495742798\n",
      "Training loss:1.557047963142395\n",
      "Training loss:1.6711053848266602\n",
      "Training loss:1.6010956764221191\n",
      "Training loss:1.5798622369766235\n",
      "Training loss:1.6726555824279785\n",
      "Training loss:1.6401910781860352\n",
      "Training loss:1.6516971588134766\n",
      "Training loss:1.717180609703064\n",
      "Training loss:1.59833824634552\n",
      "Training loss:1.5962445735931396\n",
      "Training loss:1.6743648052215576\n",
      "Training loss:1.6607006788253784\n",
      "Training loss:1.6514489650726318\n",
      "Training loss:1.6074072122573853\n",
      "Training loss:1.6302297115325928\n",
      "Training loss:1.6458693742752075\n",
      "Training loss:1.6508574485778809\n",
      "Training loss:1.6415139436721802\n",
      "Training loss:1.5507866144180298\n",
      "Training loss:1.6133805513381958\n",
      "Training loss:1.686734914779663\n",
      "Training loss:1.6028525829315186\n",
      "Training loss:1.6132385730743408\n",
      "Training loss:1.647250771522522\n",
      "Training loss:1.6432762145996094\n",
      "Training loss:1.612074375152588\n",
      "Training loss:1.5931174755096436\n",
      "Training loss:1.727910041809082\n",
      "Training loss:1.655595064163208\n",
      "Training loss:1.5991582870483398\n",
      "Training loss:1.6065860986709595\n",
      "Training loss:1.7357219457626343\n",
      "Training loss:1.6386182308197021\n",
      "Training loss:1.599892497062683\n",
      "Training loss:1.5984119176864624\n",
      "Training loss:1.6416776180267334\n",
      "Training loss:1.6128336191177368\n",
      "Training loss:1.6247624158859253\n",
      "Training loss:1.62783944606781\n",
      "Training loss:1.6210505962371826\n",
      "Training loss:1.5978131294250488\n",
      "Training loss:1.6480549573898315\n",
      "Training loss:1.5858235359191895\n",
      "Training loss:1.6832278966903687\n",
      "Training loss:1.6262201070785522\n",
      "Training loss:1.6241995096206665\n",
      "Training loss:1.5773298740386963\n",
      "Training loss:1.6546728610992432\n",
      "Training loss:1.643591046333313\n",
      "Training loss:1.5781521797180176\n",
      "Training loss:1.6803311109542847\n",
      "Training loss:1.6482564210891724\n",
      "Training loss:1.6929903030395508\n",
      "Training loss:1.6391812562942505\n",
      "Training loss:1.6411601305007935\n",
      "Training loss:1.6658906936645508\n",
      "Training loss:1.5858864784240723\n",
      "Training loss:1.6691114902496338\n",
      "Training loss:1.7275489568710327\n",
      "Training loss:1.654049038887024\n",
      "Training loss:1.599319577217102\n",
      "Training loss:1.7043626308441162\n",
      "Training loss:1.6317116022109985\n",
      "Training loss:1.6843701601028442\n",
      "Training loss:1.5822324752807617\n",
      "Training loss:1.6485135555267334\n",
      "Training loss:1.5835070610046387\n",
      "Training loss:1.685704231262207\n",
      "Training loss:1.612082600593567\n",
      "Training loss:1.5791637897491455\n",
      "Training loss:1.5307656526565552\n",
      "Training loss:1.624029278755188\n",
      "Training loss:1.5756181478500366\n",
      "Training loss:1.6172900199890137\n",
      "Training loss:1.6370630264282227\n",
      "Training loss:1.6515531539916992\n",
      "Training loss:1.6197322607040405\n",
      "Training loss:1.6565322875976562\n",
      "Training loss:1.6841380596160889\n",
      "Training loss:1.5614956617355347\n",
      "Training loss:1.4733619689941406\n",
      "Training loss:1.553536295890808\n",
      "Training loss:1.5947024822235107\n",
      "Training loss:1.6097314357757568\n",
      "Training loss:1.5852410793304443\n",
      "Training loss:1.593635082244873\n",
      "Training loss:1.6162121295928955\n",
      "Training loss:1.5937381982803345\n",
      "Training loss:1.6195203065872192\n",
      "Training loss:1.596047043800354\n",
      "Training loss:1.631827712059021\n",
      "Training loss:1.6526198387145996\n",
      "Epoch: 0082 loss_train: 152.972435 acc_train: 0.401896 loss_val: 19.405141 acc_val: 0.400333 time: 209468.723735s\n",
      "Training loss:1.664588451385498\n",
      "Training loss:1.6464422941207886\n",
      "Training loss:1.6333506107330322\n",
      "Training loss:1.6606454849243164\n",
      "Training loss:1.5934661626815796\n",
      "Training loss:1.5887031555175781\n",
      "Training loss:1.6046987771987915\n",
      "Training loss:1.6032570600509644\n",
      "Training loss:1.6603202819824219\n",
      "Training loss:1.6133694648742676\n",
      "Training loss:1.5499341487884521\n",
      "Training loss:1.7373837232589722\n",
      "Training loss:1.6069408655166626\n",
      "Training loss:1.5532842874526978\n",
      "Training loss:1.5773509740829468\n",
      "Training loss:1.6397558450698853\n",
      "Training loss:1.5648506879806519\n",
      "Training loss:1.6236052513122559\n",
      "Training loss:1.7078251838684082\n",
      "Training loss:1.644545316696167\n",
      "Training loss:1.6086210012435913\n",
      "Training loss:1.6511845588684082\n",
      "Training loss:1.6641980409622192\n",
      "Training loss:1.589188814163208\n",
      "Training loss:1.5545856952667236\n",
      "Training loss:1.6224637031555176\n",
      "Training loss:1.62844979763031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6765187978744507\n",
      "Training loss:1.6607184410095215\n",
      "Training loss:1.6117831468582153\n",
      "Training loss:1.550856351852417\n",
      "Training loss:1.542779564857483\n",
      "Training loss:1.6212369203567505\n",
      "Training loss:1.6364903450012207\n",
      "Training loss:1.6041570901870728\n",
      "Training loss:1.6316431760787964\n",
      "Training loss:1.5751374959945679\n",
      "Training loss:1.709229588508606\n",
      "Training loss:1.620943307876587\n",
      "Training loss:1.5789663791656494\n",
      "Training loss:1.6634325981140137\n",
      "Training loss:1.6607284545898438\n",
      "Training loss:1.6878068447113037\n",
      "Training loss:1.6285337209701538\n",
      "Training loss:1.6301186084747314\n",
      "Training loss:1.6378892660140991\n",
      "Training loss:1.6309983730316162\n",
      "Training loss:1.6284208297729492\n",
      "Training loss:1.6180779933929443\n",
      "Training loss:1.6659878492355347\n",
      "Training loss:1.6148203611373901\n",
      "Training loss:1.617375135421753\n",
      "Training loss:1.7087574005126953\n",
      "Training loss:1.6347943544387817\n",
      "Training loss:1.6117643117904663\n",
      "Training loss:1.6395784616470337\n",
      "Training loss:1.6671745777130127\n",
      "Training loss:1.6260976791381836\n",
      "Training loss:1.6210954189300537\n",
      "Training loss:1.6631557941436768\n",
      "Training loss:1.6199326515197754\n",
      "Training loss:1.5748755931854248\n",
      "Training loss:1.6529042720794678\n",
      "Training loss:1.6325879096984863\n",
      "Training loss:1.5269546508789062\n",
      "Training loss:1.6310946941375732\n",
      "Training loss:1.5933951139450073\n",
      "Training loss:1.595335841178894\n",
      "Training loss:1.617679238319397\n",
      "Training loss:1.6614521741867065\n",
      "Training loss:1.5893462896347046\n",
      "Training loss:1.577606439590454\n",
      "Training loss:1.6016792058944702\n",
      "Training loss:1.6094797849655151\n",
      "Training loss:1.5729129314422607\n",
      "Training loss:1.6277529001235962\n",
      "Training loss:1.6392216682434082\n",
      "Training loss:1.7337207794189453\n",
      "Training loss:1.6723785400390625\n",
      "Training loss:1.6630040407180786\n",
      "Training loss:1.5893099308013916\n",
      "Training loss:1.571571707725525\n",
      "Training loss:1.5974948406219482\n",
      "Training loss:1.6043872833251953\n",
      "Training loss:1.5958423614501953\n",
      "Training loss:1.6613966226577759\n",
      "Training loss:1.5798602104187012\n",
      "Training loss:1.6577279567718506\n",
      "Training loss:1.638928771018982\n",
      "Training loss:1.6259095668792725\n",
      "Training loss:1.5563485622406006\n",
      "Training loss:1.6187666654586792\n",
      "Training loss:1.573024034500122\n",
      "Training loss:1.5712318420410156\n",
      "Epoch: 0083 loss_train: 152.505193 acc_train: 0.404521 loss_val: 19.323727 acc_val: 0.405667 time: 211946.512544s\n",
      "Training loss:1.6281880140304565\n",
      "Training loss:1.5555280447006226\n",
      "Training loss:1.644785761833191\n",
      "Training loss:1.589093804359436\n",
      "Training loss:1.5881493091583252\n",
      "Training loss:1.5931326150894165\n",
      "Training loss:1.6471651792526245\n",
      "Training loss:1.615346908569336\n",
      "Training loss:1.573053240776062\n",
      "Training loss:1.6351088285446167\n",
      "Training loss:1.5619373321533203\n",
      "Training loss:1.580191731452942\n",
      "Training loss:1.5980415344238281\n",
      "Training loss:1.6320174932479858\n",
      "Training loss:1.5181444883346558\n",
      "Training loss:1.7206121683120728\n",
      "Training loss:1.6172128915786743\n",
      "Training loss:1.5897332429885864\n",
      "Training loss:1.5832390785217285\n",
      "Training loss:1.601464867591858\n",
      "Training loss:1.623108983039856\n",
      "Training loss:1.575743317604065\n",
      "Training loss:1.531304955482483\n",
      "Training loss:1.59489905834198\n",
      "Training loss:1.6662328243255615\n",
      "Training loss:1.6292203664779663\n",
      "Training loss:1.6075457334518433\n",
      "Training loss:1.672116994857788\n",
      "Training loss:1.6632696390151978\n",
      "Training loss:1.6247371435165405\n",
      "Training loss:1.5664169788360596\n",
      "Training loss:1.6621750593185425\n",
      "Training loss:1.7048940658569336\n",
      "Training loss:1.6611127853393555\n",
      "Training loss:1.668491244316101\n",
      "Training loss:1.687322974205017\n",
      "Training loss:1.6258585453033447\n",
      "Training loss:1.5839465856552124\n",
      "Training loss:1.591078758239746\n",
      "Training loss:1.5976635217666626\n",
      "Training loss:1.5945982933044434\n",
      "Training loss:1.6491843461990356\n",
      "Training loss:1.6130573749542236\n",
      "Training loss:1.5191805362701416\n",
      "Training loss:1.5996270179748535\n",
      "Training loss:1.643347978591919\n",
      "Training loss:1.6276435852050781\n",
      "Training loss:1.6224849224090576\n",
      "Training loss:1.5874909162521362\n",
      "Training loss:1.6172692775726318\n",
      "Training loss:1.663033127784729\n",
      "Training loss:1.6478683948516846\n",
      "Training loss:1.573123574256897\n",
      "Training loss:1.637768030166626\n",
      "Training loss:1.6524913311004639\n",
      "Training loss:1.5667392015457153\n",
      "Training loss:1.6652437448501587\n",
      "Training loss:1.7110984325408936\n",
      "Training loss:1.5874876976013184\n",
      "Training loss:1.6556702852249146\n",
      "Training loss:1.63034987449646\n",
      "Training loss:1.6480796337127686\n",
      "Training loss:1.632968783378601\n",
      "Training loss:1.6243356466293335\n",
      "Training loss:1.598293423652649\n",
      "Training loss:1.662793517112732\n",
      "Training loss:1.5769984722137451\n",
      "Training loss:1.6522562503814697\n",
      "Training loss:1.6389033794403076\n",
      "Training loss:1.5891069173812866\n",
      "Training loss:1.6170778274536133\n",
      "Training loss:1.5904648303985596\n",
      "Training loss:1.6169061660766602\n",
      "Training loss:1.6486352682113647\n",
      "Training loss:1.613983392715454\n",
      "Training loss:1.6143574714660645\n",
      "Training loss:1.567521333694458\n",
      "Training loss:1.6554088592529297\n",
      "Training loss:1.6750432252883911\n",
      "Training loss:1.6185942888259888\n",
      "Training loss:1.6191084384918213\n",
      "Training loss:1.6645547151565552\n",
      "Training loss:1.7237365245819092\n",
      "Training loss:1.684970736503601\n",
      "Training loss:1.646979570388794\n",
      "Training loss:1.6118897199630737\n",
      "Training loss:1.5913656949996948\n",
      "Training loss:1.5548087358474731\n",
      "Training loss:1.6492451429367065\n",
      "Training loss:1.5767873525619507\n",
      "Training loss:1.591733694076538\n",
      "Training loss:1.6254255771636963\n",
      "Training loss:1.5614573955535889\n",
      "Training loss:1.5831284523010254\n",
      "Epoch: 0084 loss_train: 152.200964 acc_train: 0.405313 loss_val: 19.342221 acc_val: 0.401833 time: 214422.569483s\n",
      "Training loss:1.61513352394104\n",
      "Training loss:1.6321558952331543\n",
      "Training loss:1.530307412147522\n",
      "Training loss:1.6447168588638306\n",
      "Training loss:1.5923781394958496\n",
      "Training loss:1.6943413019180298\n",
      "Training loss:1.5928899049758911\n",
      "Training loss:1.6582626104354858\n",
      "Training loss:1.6830276250839233\n",
      "Training loss:1.64262855052948\n",
      "Training loss:1.6732985973358154\n",
      "Training loss:1.5837143659591675\n",
      "Training loss:1.5565646886825562\n",
      "Training loss:1.6251658201217651\n",
      "Training loss:1.5824987888336182\n",
      "Training loss:1.5953508615493774\n",
      "Training loss:1.6071789264678955\n",
      "Training loss:1.5873006582260132\n",
      "Training loss:1.6060510873794556\n",
      "Training loss:1.601090431213379\n",
      "Training loss:1.6231180429458618\n",
      "Training loss:1.7201539278030396\n",
      "Training loss:1.6436398029327393\n",
      "Training loss:1.6395416259765625\n",
      "Training loss:1.6875693798065186\n",
      "Training loss:1.6324197053909302\n",
      "Training loss:1.6463340520858765\n",
      "Training loss:1.666277527809143\n",
      "Training loss:1.610106348991394\n",
      "Training loss:1.6888686418533325\n",
      "Training loss:1.591615915298462\n",
      "Training loss:1.6426177024841309\n",
      "Training loss:1.6450659036636353\n",
      "Training loss:1.5782346725463867\n",
      "Training loss:1.6477936506271362\n",
      "Training loss:1.6972209215164185\n",
      "Training loss:1.678584098815918\n",
      "Training loss:1.6238924264907837\n",
      "Training loss:1.6886961460113525\n",
      "Training loss:1.6636871099472046\n",
      "Training loss:1.6182119846343994\n",
      "Training loss:1.6710222959518433\n",
      "Training loss:1.6320266723632812\n",
      "Training loss:1.5642451047897339\n",
      "Training loss:1.6716622114181519\n",
      "Training loss:1.676936149597168\n",
      "Training loss:1.6269805431365967\n",
      "Training loss:1.652332067489624\n",
      "Training loss:1.6217260360717773\n",
      "Training loss:1.5808738470077515\n",
      "Training loss:1.5905627012252808\n",
      "Training loss:1.6088194847106934\n",
      "Training loss:1.6042741537094116\n",
      "Training loss:1.610200047492981\n",
      "Training loss:1.6420345306396484\n",
      "Training loss:1.6352936029434204\n",
      "Training loss:1.5717097520828247\n",
      "Training loss:1.6065245866775513\n",
      "Training loss:1.5688118934631348\n",
      "Training loss:1.5862473249435425\n",
      "Training loss:1.5558676719665527\n",
      "Training loss:1.6314966678619385\n",
      "Training loss:1.644649624824524\n",
      "Training loss:1.6311874389648438\n",
      "Training loss:1.5420814752578735\n",
      "Training loss:1.5739762783050537\n",
      "Training loss:1.5717828273773193\n",
      "Training loss:1.6434593200683594\n",
      "Training loss:1.4871073961257935\n",
      "Training loss:1.530571460723877\n",
      "Training loss:1.6751865148544312\n",
      "Training loss:1.561263918876648\n",
      "Training loss:1.6478185653686523\n",
      "Training loss:1.5466442108154297\n",
      "Training loss:1.576840877532959\n",
      "Training loss:1.609136939048767\n",
      "Training loss:1.6052746772766113\n",
      "Training loss:1.7080192565917969\n",
      "Training loss:1.6597987413406372\n",
      "Training loss:1.6592233180999756\n",
      "Training loss:1.6524187326431274\n",
      "Training loss:1.6441073417663574\n",
      "Training loss:1.5549490451812744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5700172185897827\n",
      "Training loss:1.5701982975006104\n",
      "Training loss:1.6154383420944214\n",
      "Training loss:1.5686341524124146\n",
      "Training loss:1.6030921936035156\n",
      "Training loss:1.5697572231292725\n",
      "Training loss:1.6581172943115234\n",
      "Training loss:1.5839521884918213\n",
      "Training loss:1.568428635597229\n",
      "Training loss:1.6310254335403442\n",
      "Training loss:1.5877524614334106\n",
      "Epoch: 0085 loss_train: 152.099264 acc_train: 0.407458 loss_val: 19.476000 acc_val: 0.408167 time: 216889.564827s\n",
      "Training loss:1.6710081100463867\n",
      "Training loss:1.5896806716918945\n",
      "Training loss:1.5890593528747559\n",
      "Training loss:1.660466194152832\n",
      "Training loss:1.6545485258102417\n",
      "Training loss:1.5951632261276245\n",
      "Training loss:1.6425267457962036\n",
      "Training loss:1.7155100107192993\n",
      "Training loss:1.6100207567214966\n",
      "Training loss:1.6111565828323364\n",
      "Training loss:1.6153414249420166\n",
      "Training loss:1.5546578168869019\n",
      "Training loss:1.6744250059127808\n",
      "Training loss:1.6980712413787842\n",
      "Training loss:1.6092396974563599\n",
      "Training loss:1.6115227937698364\n",
      "Training loss:1.591620922088623\n",
      "Training loss:1.6024168729782104\n",
      "Training loss:1.611606478691101\n",
      "Training loss:1.5939457416534424\n",
      "Training loss:1.7041133642196655\n",
      "Training loss:1.572855830192566\n",
      "Training loss:1.6477049589157104\n",
      "Training loss:1.6425377130508423\n",
      "Training loss:1.5821706056594849\n",
      "Training loss:1.5744842290878296\n",
      "Training loss:1.6386774778366089\n",
      "Training loss:1.5883923768997192\n",
      "Training loss:1.583329677581787\n",
      "Training loss:1.648271918296814\n",
      "Training loss:1.6594698429107666\n",
      "Training loss:1.6144115924835205\n",
      "Training loss:1.626578450202942\n",
      "Training loss:1.6229199171066284\n",
      "Training loss:1.54193913936615\n",
      "Training loss:1.6308292150497437\n",
      "Training loss:1.6554886102676392\n",
      "Training loss:1.5094281435012817\n",
      "Training loss:1.6076775789260864\n",
      "Training loss:1.66156804561615\n",
      "Training loss:1.6704528331756592\n",
      "Training loss:1.535247802734375\n",
      "Training loss:1.6891790628433228\n",
      "Training loss:1.6327550411224365\n",
      "Training loss:1.6634501218795776\n",
      "Training loss:1.633631944656372\n",
      "Training loss:1.5528836250305176\n",
      "Training loss:1.6311078071594238\n",
      "Training loss:1.628491997718811\n",
      "Training loss:1.61099112033844\n",
      "Training loss:1.6272759437561035\n",
      "Training loss:1.6117119789123535\n",
      "Training loss:1.6778062582015991\n",
      "Training loss:1.5845484733581543\n",
      "Training loss:1.5729074478149414\n",
      "Training loss:1.664151668548584\n",
      "Training loss:1.6070624589920044\n",
      "Training loss:1.6597893238067627\n",
      "Training loss:1.5847938060760498\n",
      "Training loss:1.6188091039657593\n",
      "Training loss:1.6171222925186157\n",
      "Training loss:1.613156795501709\n",
      "Training loss:1.7440909147262573\n",
      "Training loss:1.614563226699829\n",
      "Training loss:1.5400431156158447\n",
      "Training loss:1.6414731740951538\n",
      "Training loss:1.650586485862732\n",
      "Training loss:1.614574670791626\n",
      "Training loss:1.6544166803359985\n",
      "Training loss:1.6155169010162354\n",
      "Training loss:1.646911382675171\n",
      "Training loss:1.5798472166061401\n",
      "Training loss:1.6411787271499634\n",
      "Training loss:1.5601987838745117\n",
      "Training loss:1.6109538078308105\n",
      "Training loss:1.593838095664978\n",
      "Training loss:1.5705305337905884\n",
      "Training loss:1.671103596687317\n",
      "Training loss:1.536008596420288\n",
      "Training loss:1.6470876932144165\n",
      "Training loss:1.6438360214233398\n",
      "Training loss:1.5402523279190063\n",
      "Training loss:1.5971107482910156\n",
      "Training loss:1.625152349472046\n",
      "Training loss:1.5917657613754272\n",
      "Training loss:1.6692416667938232\n",
      "Training loss:1.6472111940383911\n",
      "Training loss:1.568878412246704\n",
      "Training loss:1.6180760860443115\n",
      "Training loss:1.5719680786132812\n",
      "Training loss:1.6245841979980469\n",
      "Training loss:1.609123945236206\n",
      "Training loss:1.5761468410491943\n",
      "Training loss:1.584161400794983\n",
      "Epoch: 0086 loss_train: 152.102596 acc_train: 0.405687 loss_val: 19.248929 acc_val: 0.413500 time: 219364.705097s\n",
      "Training loss:1.6251736879348755\n",
      "Training loss:1.581174612045288\n",
      "Training loss:1.5358283519744873\n",
      "Training loss:1.6078462600708008\n",
      "Training loss:1.5764707326889038\n",
      "Training loss:1.5934617519378662\n",
      "Training loss:1.5786874294281006\n",
      "Training loss:1.5894254446029663\n",
      "Training loss:1.6757469177246094\n",
      "Training loss:1.6336380243301392\n",
      "Training loss:1.6002033948898315\n",
      "Training loss:1.5748422145843506\n",
      "Training loss:1.6435686349868774\n",
      "Training loss:1.6753989458084106\n",
      "Training loss:1.5498104095458984\n",
      "Training loss:1.5636425018310547\n",
      "Training loss:1.6794289350509644\n",
      "Training loss:1.617751121520996\n",
      "Training loss:1.5806655883789062\n",
      "Training loss:1.700208067893982\n",
      "Training loss:1.6026822328567505\n",
      "Training loss:1.6447577476501465\n",
      "Training loss:1.5752129554748535\n",
      "Training loss:1.630922555923462\n",
      "Training loss:1.5997868776321411\n",
      "Training loss:1.6205257177352905\n",
      "Training loss:1.6186227798461914\n",
      "Training loss:1.5385292768478394\n",
      "Training loss:1.619731068611145\n",
      "Training loss:1.6382747888565063\n",
      "Training loss:1.5990248918533325\n",
      "Training loss:1.6405614614486694\n",
      "Training loss:1.6781821250915527\n",
      "Training loss:1.6172657012939453\n",
      "Training loss:1.5712342262268066\n",
      "Training loss:1.6149383783340454\n",
      "Training loss:1.649517297744751\n",
      "Training loss:1.5595108270645142\n",
      "Training loss:1.5813730955123901\n",
      "Training loss:1.609115719795227\n",
      "Training loss:1.5854098796844482\n",
      "Training loss:1.6973106861114502\n",
      "Training loss:1.6604509353637695\n",
      "Training loss:1.6623793840408325\n",
      "Training loss:1.5809444189071655\n",
      "Training loss:1.621192455291748\n",
      "Training loss:1.6815956830978394\n",
      "Training loss:1.6739853620529175\n",
      "Training loss:1.5802112817764282\n",
      "Training loss:1.709656834602356\n",
      "Training loss:1.6758134365081787\n",
      "Training loss:1.6231478452682495\n",
      "Training loss:1.657681941986084\n",
      "Training loss:1.6013267040252686\n",
      "Training loss:1.7401516437530518\n",
      "Training loss:1.6495492458343506\n",
      "Training loss:1.6628628969192505\n",
      "Training loss:1.5691169500350952\n",
      "Training loss:1.6333096027374268\n",
      "Training loss:1.5998343229293823\n",
      "Training loss:1.5743480920791626\n",
      "Training loss:1.5541105270385742\n",
      "Training loss:1.6463466882705688\n",
      "Training loss:1.6133136749267578\n",
      "Training loss:1.6224688291549683\n",
      "Training loss:1.6411367654800415\n",
      "Training loss:1.6784381866455078\n",
      "Training loss:1.5922794342041016\n",
      "Training loss:1.634325623512268\n",
      "Training loss:1.5701853036880493\n",
      "Training loss:1.5956122875213623\n",
      "Training loss:1.6125051975250244\n",
      "Training loss:1.6236234903335571\n",
      "Training loss:1.5550252199172974\n",
      "Training loss:1.551883339881897\n",
      "Training loss:1.6534773111343384\n",
      "Training loss:1.5913357734680176\n",
      "Training loss:1.5790427923202515\n",
      "Training loss:1.5844842195510864\n",
      "Training loss:1.6647050380706787\n",
      "Training loss:1.6058204174041748\n",
      "Training loss:1.5872420072555542\n",
      "Training loss:1.6488851308822632\n",
      "Training loss:1.6409393548965454\n",
      "Training loss:1.6290212869644165\n",
      "Training loss:1.5982822179794312\n",
      "Training loss:1.5638445615768433\n",
      "Training loss:1.5480152368545532\n",
      "Training loss:1.6211875677108765\n",
      "Training loss:1.5706042051315308\n",
      "Training loss:1.5682249069213867\n",
      "Training loss:1.578094482421875\n",
      "Training loss:1.5847275257110596\n",
      "Training loss:1.628108024597168\n",
      "Epoch: 0087 loss_train: 151.770319 acc_train: 0.406729 loss_val: 19.353910 acc_val: 0.406000 time: 221847.592596s\n",
      "Training loss:1.561335563659668\n",
      "Training loss:1.7022888660430908\n",
      "Training loss:1.5863573551177979\n",
      "Training loss:1.5654350519180298\n",
      "Training loss:1.5799083709716797\n",
      "Training loss:1.619901180267334\n",
      "Training loss:1.608108401298523\n",
      "Training loss:1.6088007688522339\n",
      "Training loss:1.5389384031295776\n",
      "Training loss:1.6437867879867554\n",
      "Training loss:1.7444403171539307\n",
      "Training loss:1.6411325931549072\n",
      "Training loss:1.684952735900879\n",
      "Training loss:1.6748120784759521\n",
      "Training loss:1.628508448600769\n",
      "Training loss:1.5388480424880981\n",
      "Training loss:1.6498732566833496\n",
      "Training loss:1.5813027620315552\n",
      "Training loss:1.6436935663223267\n",
      "Training loss:1.5374568700790405\n",
      "Training loss:1.6001055240631104\n",
      "Training loss:1.6251312494277954\n",
      "Training loss:1.6266624927520752\n",
      "Training loss:1.6014137268066406\n",
      "Training loss:1.5727522373199463\n",
      "Training loss:1.5555261373519897\n",
      "Training loss:1.5669716596603394\n",
      "Training loss:1.5867804288864136\n",
      "Training loss:1.6315163373947144\n",
      "Training loss:1.582790493965149\n",
      "Training loss:1.5708541870117188\n",
      "Training loss:1.6223978996276855\n",
      "Training loss:1.6415550708770752\n",
      "Training loss:1.664867639541626\n",
      "Training loss:1.6101248264312744\n",
      "Training loss:1.6068851947784424\n",
      "Training loss:1.6793673038482666\n",
      "Training loss:1.6018575429916382\n",
      "Training loss:1.5850117206573486\n",
      "Training loss:1.66647207736969\n",
      "Training loss:1.559444785118103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5982812643051147\n",
      "Training loss:1.607418417930603\n",
      "Training loss:1.5674668550491333\n",
      "Training loss:1.6390068531036377\n",
      "Training loss:1.6770871877670288\n",
      "Training loss:1.614829421043396\n",
      "Training loss:1.528931975364685\n",
      "Training loss:1.6534093618392944\n",
      "Training loss:1.5268019437789917\n",
      "Training loss:1.7290780544281006\n",
      "Training loss:1.5681440830230713\n",
      "Training loss:1.6352479457855225\n",
      "Training loss:1.5953998565673828\n",
      "Training loss:1.6928285360336304\n",
      "Training loss:1.621260643005371\n",
      "Training loss:1.6151361465454102\n",
      "Training loss:1.6601868867874146\n",
      "Training loss:1.6623351573944092\n",
      "Training loss:1.6365361213684082\n",
      "Training loss:1.6599034070968628\n",
      "Training loss:1.5252050161361694\n",
      "Training loss:1.637042760848999\n",
      "Training loss:1.5862390995025635\n",
      "Training loss:1.6035757064819336\n",
      "Training loss:1.702541708946228\n",
      "Training loss:1.561646580696106\n",
      "Training loss:1.611121654510498\n",
      "Training loss:1.6192072629928589\n",
      "Training loss:1.5611402988433838\n",
      "Training loss:1.6125298738479614\n",
      "Training loss:1.6637853384017944\n",
      "Training loss:1.566927433013916\n",
      "Training loss:1.6304408311843872\n",
      "Training loss:1.6941535472869873\n",
      "Training loss:1.66350257396698\n",
      "Training loss:1.6356393098831177\n",
      "Training loss:1.6596143245697021\n",
      "Training loss:1.6364846229553223\n",
      "Training loss:1.6575069427490234\n",
      "Training loss:1.5360931158065796\n",
      "Training loss:1.689918875694275\n",
      "Training loss:1.6304233074188232\n",
      "Training loss:1.5440407991409302\n",
      "Training loss:1.6009219884872437\n",
      "Training loss:1.6564477682113647\n",
      "Training loss:1.6328028440475464\n",
      "Training loss:1.6236391067504883\n",
      "Training loss:1.6436028480529785\n",
      "Training loss:1.6319546699523926\n",
      "Training loss:1.616672158241272\n",
      "Training loss:1.591978907585144\n",
      "Training loss:1.6571930646896362\n",
      "Training loss:1.613479733467102\n",
      "Epoch: 0088 loss_train: 152.085134 acc_train: 0.407458 loss_val: 19.229849 acc_val: 0.413000 time: 224314.248493s\n",
      "Training loss:1.635627031326294\n",
      "Training loss:1.607042670249939\n",
      "Training loss:1.6048864126205444\n",
      "Training loss:1.545410394668579\n",
      "Training loss:1.6110639572143555\n",
      "Training loss:1.6242883205413818\n",
      "Training loss:1.604595422744751\n",
      "Training loss:1.686315894126892\n",
      "Training loss:1.6302931308746338\n",
      "Training loss:1.581066608428955\n",
      "Training loss:1.6202956438064575\n",
      "Training loss:1.6744353771209717\n",
      "Training loss:1.6428709030151367\n",
      "Training loss:1.6559476852416992\n",
      "Training loss:1.567308783531189\n",
      "Training loss:1.5945994853973389\n",
      "Training loss:1.566603422164917\n",
      "Training loss:1.6029577255249023\n",
      "Training loss:1.6442220211029053\n",
      "Training loss:1.5718159675598145\n",
      "Training loss:1.6051982641220093\n",
      "Training loss:1.5690436363220215\n",
      "Training loss:1.6457841396331787\n",
      "Training loss:1.611984133720398\n",
      "Training loss:1.5575929880142212\n",
      "Training loss:1.6340633630752563\n",
      "Training loss:1.6420550346374512\n",
      "Training loss:1.620370864868164\n",
      "Training loss:1.5702403783798218\n",
      "Training loss:1.6079837083816528\n",
      "Training loss:1.607909083366394\n",
      "Training loss:1.523736596107483\n",
      "Training loss:1.6029906272888184\n",
      "Training loss:1.6198469400405884\n",
      "Training loss:1.6030285358428955\n",
      "Training loss:1.615936279296875\n",
      "Training loss:1.670845866203308\n",
      "Training loss:1.5807130336761475\n",
      "Training loss:1.6183462142944336\n",
      "Training loss:1.5854485034942627\n",
      "Training loss:1.6732171773910522\n",
      "Training loss:1.560118317604065\n",
      "Training loss:1.6101739406585693\n",
      "Training loss:1.5002154111862183\n",
      "Training loss:1.5458707809448242\n",
      "Training loss:1.6427457332611084\n",
      "Training loss:1.5802218914031982\n",
      "Training loss:1.6389596462249756\n",
      "Training loss:1.6235673427581787\n",
      "Training loss:1.5908589363098145\n",
      "Training loss:1.5606117248535156\n",
      "Training loss:1.6425676345825195\n",
      "Training loss:1.5940269231796265\n",
      "Training loss:1.5520309209823608\n",
      "Training loss:1.6076933145523071\n",
      "Training loss:1.592050552368164\n",
      "Training loss:1.606484293937683\n",
      "Training loss:1.6060601472854614\n",
      "Training loss:1.6376045942306519\n",
      "Training loss:1.6227473020553589\n",
      "Training loss:1.5985490083694458\n",
      "Training loss:1.6015229225158691\n",
      "Training loss:1.6205402612686157\n",
      "Training loss:1.599622130393982\n",
      "Training loss:1.6618788242340088\n",
      "Training loss:1.5326563119888306\n",
      "Training loss:1.6517219543457031\n",
      "Training loss:1.5780181884765625\n",
      "Training loss:1.6390990018844604\n",
      "Training loss:1.5663716793060303\n",
      "Training loss:1.5865110158920288\n",
      "Training loss:1.5811885595321655\n",
      "Training loss:1.5351163148880005\n",
      "Training loss:1.6631479263305664\n",
      "Training loss:1.7235852479934692\n",
      "Training loss:1.5606341361999512\n",
      "Training loss:1.6302616596221924\n",
      "Training loss:1.6426523923873901\n",
      "Training loss:1.5766100883483887\n",
      "Training loss:1.604182243347168\n",
      "Training loss:1.660513162612915\n",
      "Training loss:1.5955508947372437\n",
      "Training loss:1.5995932817459106\n",
      "Training loss:1.6681016683578491\n",
      "Training loss:1.627474069595337\n",
      "Training loss:1.622543215751648\n",
      "Training loss:1.6278023719787598\n",
      "Training loss:1.6428247690200806\n",
      "Training loss:1.595609188079834\n",
      "Training loss:1.6469802856445312\n",
      "Training loss:1.6000473499298096\n",
      "Training loss:1.570394515991211\n",
      "Training loss:1.5987002849578857\n",
      "Training loss:1.5985441207885742\n",
      "Epoch: 0089 loss_train: 151.161141 acc_train: 0.411687 loss_val: 19.188312 acc_val: 0.415000 time: 226791.572737s\n",
      "Training loss:1.602871060371399\n",
      "Training loss:1.7371207475662231\n",
      "Training loss:1.5320210456848145\n",
      "Training loss:1.585497260093689\n",
      "Training loss:1.6555482149124146\n",
      "Training loss:1.5134563446044922\n",
      "Training loss:1.6675857305526733\n",
      "Training loss:1.5979207754135132\n",
      "Training loss:1.6866590976715088\n",
      "Training loss:1.6377876996994019\n",
      "Training loss:1.6281943321228027\n",
      "Training loss:1.6159520149230957\n",
      "Training loss:1.6841890811920166\n",
      "Training loss:1.604116678237915\n",
      "Training loss:1.5679402351379395\n",
      "Training loss:1.5341912508010864\n",
      "Training loss:1.5950225591659546\n",
      "Training loss:1.6070393323898315\n",
      "Training loss:1.5594675540924072\n",
      "Training loss:1.607150912284851\n",
      "Training loss:1.5937119722366333\n",
      "Training loss:1.5725696086883545\n",
      "Training loss:1.5493680238723755\n",
      "Training loss:1.5925196409225464\n",
      "Training loss:1.6537251472473145\n",
      "Training loss:1.6487715244293213\n",
      "Training loss:1.6008415222167969\n",
      "Training loss:1.6215347051620483\n",
      "Training loss:1.5063103437423706\n",
      "Training loss:1.5507649183273315\n",
      "Training loss:1.6326597929000854\n",
      "Training loss:1.67233407497406\n",
      "Training loss:1.538922905921936\n",
      "Training loss:1.6106833219528198\n",
      "Training loss:1.551400899887085\n",
      "Training loss:1.6319000720977783\n",
      "Training loss:1.6265158653259277\n",
      "Training loss:1.6161260604858398\n",
      "Training loss:1.5985515117645264\n",
      "Training loss:1.5943753719329834\n",
      "Training loss:1.6280956268310547\n",
      "Training loss:1.5591002702713013\n",
      "Training loss:1.6263978481292725\n",
      "Training loss:1.6416261196136475\n",
      "Training loss:1.669750690460205\n",
      "Training loss:1.568136215209961\n",
      "Training loss:1.5745062828063965\n",
      "Training loss:1.5694700479507446\n",
      "Training loss:1.602584719657898\n",
      "Training loss:1.6325161457061768\n",
      "Training loss:1.6145087480545044\n",
      "Training loss:1.6372628211975098\n",
      "Training loss:1.5704622268676758\n",
      "Training loss:1.625116229057312\n",
      "Training loss:1.6288248300552368\n",
      "Training loss:1.6322880983352661\n",
      "Training loss:1.6393280029296875\n",
      "Training loss:1.6669903993606567\n",
      "Training loss:1.7007704973220825\n",
      "Training loss:1.543800711631775\n",
      "Training loss:1.647971510887146\n",
      "Training loss:1.633590817451477\n",
      "Training loss:1.7035351991653442\n",
      "Training loss:1.6653528213500977\n",
      "Training loss:1.6255890130996704\n",
      "Training loss:1.608648657798767\n",
      "Training loss:1.5939500331878662\n",
      "Training loss:1.6324845552444458\n",
      "Training loss:1.5776084661483765\n",
      "Training loss:1.5753079652786255\n",
      "Training loss:1.5594947338104248\n",
      "Training loss:1.614297866821289\n",
      "Training loss:1.5295358896255493\n",
      "Training loss:1.5680615901947021\n",
      "Training loss:1.620295524597168\n",
      "Training loss:1.5907140970230103\n",
      "Training loss:1.5830957889556885\n",
      "Training loss:1.5827354192733765\n",
      "Training loss:1.5854252576828003\n",
      "Training loss:1.5817060470581055\n",
      "Training loss:1.6270238161087036\n",
      "Training loss:1.572681188583374\n",
      "Training loss:1.5957177877426147\n",
      "Training loss:1.691061019897461\n",
      "Training loss:1.5794042348861694\n",
      "Training loss:1.615043044090271\n",
      "Training loss:1.650229811668396\n",
      "Training loss:1.5153067111968994\n",
      "Training loss:1.580819010734558\n",
      "Training loss:1.637500524520874\n",
      "Training loss:1.6543015241622925\n",
      "Training loss:1.6279594898223877\n",
      "Training loss:1.6189132928848267\n",
      "Training loss:1.6299055814743042\n",
      "Epoch: 0090 loss_train: 151.190124 acc_train: 0.412458 loss_val: 19.406521 acc_val: 0.406667 time: 229263.896382s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6845695972442627\n",
      "Training loss:1.6599481105804443\n",
      "Training loss:1.6275475025177002\n",
      "Training loss:1.5475871562957764\n",
      "Training loss:1.6117932796478271\n",
      "Training loss:1.6527787446975708\n",
      "Training loss:1.6019906997680664\n",
      "Training loss:1.6684386730194092\n",
      "Training loss:1.621759295463562\n",
      "Training loss:1.5874477624893188\n",
      "Training loss:1.6489380598068237\n",
      "Training loss:1.5247427225112915\n",
      "Training loss:1.612203598022461\n",
      "Training loss:1.539977788925171\n",
      "Training loss:1.613882303237915\n",
      "Training loss:1.589679479598999\n",
      "Training loss:1.596556305885315\n",
      "Training loss:1.578670620918274\n",
      "Training loss:1.6539337635040283\n",
      "Training loss:1.5705304145812988\n",
      "Training loss:1.588035225868225\n",
      "Training loss:1.6204224824905396\n",
      "Training loss:1.6094423532485962\n",
      "Training loss:1.6619466543197632\n",
      "Training loss:1.580000638961792\n",
      "Training loss:1.6654112339019775\n",
      "Training loss:1.570264458656311\n",
      "Training loss:1.545711636543274\n",
      "Training loss:1.6226561069488525\n",
      "Training loss:1.6217458248138428\n",
      "Training loss:1.6373471021652222\n",
      "Training loss:1.5804270505905151\n",
      "Training loss:1.5642789602279663\n",
      "Training loss:1.6061949729919434\n",
      "Training loss:1.5866137742996216\n",
      "Training loss:1.6698204278945923\n",
      "Training loss:1.5477114915847778\n",
      "Training loss:1.565870761871338\n",
      "Training loss:1.6071829795837402\n",
      "Training loss:1.6195762157440186\n",
      "Training loss:1.5894598960876465\n",
      "Training loss:1.6704648733139038\n",
      "Training loss:1.6635090112686157\n",
      "Training loss:1.619789958000183\n",
      "Training loss:1.6349283456802368\n",
      "Training loss:1.6193104982376099\n",
      "Training loss:1.6031986474990845\n",
      "Training loss:1.683610439300537\n",
      "Training loss:1.6233350038528442\n",
      "Training loss:1.615043044090271\n",
      "Training loss:1.619767427444458\n",
      "Training loss:1.5790013074874878\n",
      "Training loss:1.612153172492981\n",
      "Training loss:1.646457314491272\n",
      "Training loss:1.5934313535690308\n",
      "Training loss:1.6441810131072998\n",
      "Training loss:1.5642073154449463\n",
      "Training loss:1.620673656463623\n",
      "Training loss:1.539670705795288\n",
      "Training loss:1.6163586378097534\n",
      "Training loss:1.6303843259811401\n",
      "Training loss:1.6105070114135742\n",
      "Training loss:1.5462747812271118\n",
      "Training loss:1.6103408336639404\n",
      "Training loss:1.6203815937042236\n",
      "Training loss:1.5653948783874512\n",
      "Training loss:1.5267711877822876\n",
      "Training loss:1.5642714500427246\n",
      "Training loss:1.635403037071228\n",
      "Training loss:1.5724985599517822\n",
      "Training loss:1.624587893486023\n",
      "Training loss:1.6054401397705078\n",
      "Training loss:1.5678629875183105\n",
      "Training loss:1.6184319257736206\n",
      "Training loss:1.6104286909103394\n",
      "Training loss:1.57697331905365\n",
      "Training loss:1.5525511503219604\n",
      "Training loss:1.613116979598999\n",
      "Training loss:1.548595905303955\n",
      "Training loss:1.6445643901824951\n",
      "Training loss:1.585564374923706\n",
      "Training loss:1.603925347328186\n",
      "Training loss:1.6339006423950195\n",
      "Training loss:1.6356667280197144\n",
      "Training loss:1.5570791959762573\n",
      "Training loss:1.606232762336731\n",
      "Training loss:1.5974245071411133\n",
      "Training loss:1.5693422555923462\n",
      "Training loss:1.6148756742477417\n",
      "Training loss:1.6120637655258179\n",
      "Training loss:1.6365944147109985\n",
      "Training loss:1.51962411403656\n",
      "Training loss:1.6584925651550293\n",
      "Training loss:1.6450719833374023\n",
      "Epoch: 0091 loss_train: 150.942827 acc_train: 0.411292 loss_val: 19.028459 acc_val: 0.419167 time: 231752.154728s\n",
      "Training loss:1.569779872894287\n",
      "Training loss:1.6307494640350342\n",
      "Training loss:1.6204386949539185\n",
      "Training loss:1.5534141063690186\n",
      "Training loss:1.524277925491333\n",
      "Training loss:1.574203372001648\n",
      "Training loss:1.5875612497329712\n",
      "Training loss:1.57576322555542\n",
      "Training loss:1.59950852394104\n",
      "Training loss:1.6502273082733154\n",
      "Training loss:1.5448458194732666\n",
      "Training loss:1.5240099430084229\n",
      "Training loss:1.514938235282898\n",
      "Training loss:1.6315685510635376\n",
      "Training loss:1.5613806247711182\n",
      "Training loss:1.6422367095947266\n",
      "Training loss:1.6064528226852417\n",
      "Training loss:1.628793478012085\n",
      "Training loss:1.5182198286056519\n",
      "Training loss:1.6091786623001099\n",
      "Training loss:1.6021640300750732\n",
      "Training loss:1.5068469047546387\n",
      "Training loss:1.553525447845459\n",
      "Training loss:1.5129882097244263\n",
      "Training loss:1.6760119199752808\n",
      "Training loss:1.6571959257125854\n",
      "Training loss:1.5720289945602417\n",
      "Training loss:1.5933247804641724\n",
      "Training loss:1.6281359195709229\n",
      "Training loss:1.5858410596847534\n",
      "Training loss:1.7005703449249268\n",
      "Training loss:1.5218337774276733\n",
      "Training loss:1.6200143098831177\n",
      "Training loss:1.6038858890533447\n",
      "Training loss:1.6478540897369385\n",
      "Training loss:1.5973044633865356\n",
      "Training loss:1.5575188398361206\n",
      "Training loss:1.5297036170959473\n",
      "Training loss:1.5877350568771362\n",
      "Training loss:1.5349748134613037\n",
      "Training loss:1.6197963953018188\n",
      "Training loss:1.5310195684432983\n",
      "Training loss:1.5797659158706665\n",
      "Training loss:1.6549687385559082\n",
      "Training loss:1.617647409439087\n",
      "Training loss:1.610114336013794\n",
      "Training loss:1.5934102535247803\n",
      "Training loss:1.5770514011383057\n",
      "Training loss:1.652970552444458\n",
      "Training loss:1.6014176607131958\n",
      "Training loss:1.6468803882598877\n",
      "Training loss:1.5888795852661133\n",
      "Training loss:1.6229227781295776\n",
      "Training loss:1.6334013938903809\n",
      "Training loss:1.6735808849334717\n",
      "Training loss:1.6331192255020142\n",
      "Training loss:1.592460036277771\n",
      "Training loss:1.5911755561828613\n",
      "Training loss:1.6133166551589966\n",
      "Training loss:1.5829646587371826\n",
      "Training loss:1.5141875743865967\n",
      "Training loss:1.561446189880371\n",
      "Training loss:1.5914710760116577\n",
      "Training loss:1.6240477561950684\n",
      "Training loss:1.5878320932388306\n",
      "Training loss:1.6624393463134766\n",
      "Training loss:1.5720105171203613\n",
      "Training loss:1.6420488357543945\n",
      "Training loss:1.5540271997451782\n",
      "Training loss:1.5440332889556885\n",
      "Training loss:1.5183165073394775\n",
      "Training loss:1.6349128484725952\n",
      "Training loss:1.622918725013733\n",
      "Training loss:1.6776796579360962\n",
      "Training loss:1.62851881980896\n",
      "Training loss:1.5046011209487915\n",
      "Training loss:1.625362515449524\n",
      "Training loss:1.5473217964172363\n",
      "Training loss:1.5901665687561035\n",
      "Training loss:1.5888404846191406\n",
      "Training loss:1.630615234375\n",
      "Training loss:1.658612847328186\n",
      "Training loss:1.6786787509918213\n",
      "Training loss:1.6559746265411377\n",
      "Training loss:1.6967443227767944\n",
      "Training loss:1.6270086765289307\n",
      "Training loss:1.623190999031067\n",
      "Training loss:1.6724270582199097\n",
      "Training loss:1.5966238975524902\n",
      "Training loss:1.5701040029525757\n",
      "Training loss:1.6533491611480713\n",
      "Training loss:1.5664187669754028\n",
      "Training loss:1.6727662086486816\n",
      "Training loss:1.6130930185317993\n",
      "Epoch: 0092 loss_train: 150.381661 acc_train: 0.414521 loss_val: 19.259859 acc_val: 0.401667 time: 234228.264632s\n",
      "Training loss:1.5971176624298096\n",
      "Training loss:1.7277514934539795\n",
      "Training loss:1.6324723958969116\n",
      "Training loss:1.6429003477096558\n",
      "Training loss:1.63871431350708\n",
      "Training loss:1.5939704179763794\n",
      "Training loss:1.5454703569412231\n",
      "Training loss:1.6177258491516113\n",
      "Training loss:1.6232798099517822\n",
      "Training loss:1.6210891008377075\n",
      "Training loss:1.6338281631469727\n",
      "Training loss:1.5810457468032837\n",
      "Training loss:1.5569031238555908\n",
      "Training loss:1.6082046031951904\n",
      "Training loss:1.6363294124603271\n",
      "Training loss:1.6979671716690063\n",
      "Training loss:1.5480118989944458\n",
      "Training loss:1.6161940097808838\n",
      "Training loss:1.682059407234192\n",
      "Training loss:1.5704143047332764\n",
      "Training loss:1.6206071376800537\n",
      "Training loss:1.5875760316848755\n",
      "Training loss:1.596684455871582\n",
      "Training loss:1.5813939571380615\n",
      "Training loss:1.5837769508361816\n",
      "Training loss:1.652192234992981\n",
      "Training loss:1.5815359354019165\n",
      "Training loss:1.6362322568893433\n",
      "Training loss:1.5907220840454102\n",
      "Training loss:1.5823265314102173\n",
      "Training loss:1.5257207155227661\n",
      "Training loss:1.5908268690109253\n",
      "Training loss:1.6435927152633667\n",
      "Training loss:1.587876319885254\n",
      "Training loss:1.5922614336013794\n",
      "Training loss:1.6064280271530151\n",
      "Training loss:1.5760650634765625\n",
      "Training loss:1.6754802465438843\n",
      "Training loss:1.6038864850997925\n",
      "Training loss:1.5796159505844116\n",
      "Training loss:1.5892691612243652\n",
      "Training loss:1.580070972442627\n",
      "Training loss:1.608031988143921\n",
      "Training loss:1.5759776830673218\n",
      "Training loss:1.6164530515670776\n",
      "Training loss:1.621423363685608\n",
      "Training loss:1.6536027193069458\n",
      "Training loss:1.5736647844314575\n",
      "Training loss:1.6868318319320679\n",
      "Training loss:1.7255527973175049\n",
      "Training loss:1.613171935081482\n",
      "Training loss:1.61920964717865\n",
      "Training loss:1.6902341842651367\n",
      "Training loss:1.5520001649856567\n",
      "Training loss:1.551392674446106\n",
      "Training loss:1.5650221109390259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6578179597854614\n",
      "Training loss:1.6027451753616333\n",
      "Training loss:1.5758029222488403\n",
      "Training loss:1.5988849401474\n",
      "Training loss:1.5861318111419678\n",
      "Training loss:1.5684326887130737\n",
      "Training loss:1.5988410711288452\n",
      "Training loss:1.5626823902130127\n",
      "Training loss:1.6409780979156494\n",
      "Training loss:1.678772211074829\n",
      "Training loss:1.6159223318099976\n",
      "Training loss:1.6056270599365234\n",
      "Training loss:1.6063542366027832\n",
      "Training loss:1.6180109977722168\n",
      "Training loss:1.5564948320388794\n",
      "Training loss:1.6132802963256836\n",
      "Training loss:1.58140230178833\n",
      "Training loss:1.6065603494644165\n",
      "Training loss:1.6405551433563232\n",
      "Training loss:1.5813239812850952\n",
      "Training loss:1.5176922082901\n",
      "Training loss:1.5909332036972046\n",
      "Training loss:1.5827175378799438\n",
      "Training loss:1.5410584211349487\n",
      "Training loss:1.5978642702102661\n",
      "Training loss:1.5316189527511597\n",
      "Training loss:1.584062099456787\n",
      "Training loss:1.5982242822647095\n",
      "Training loss:1.590861439704895\n",
      "Training loss:1.5969148874282837\n",
      "Training loss:1.5114222764968872\n",
      "Training loss:1.581178069114685\n",
      "Training loss:1.6312706470489502\n",
      "Training loss:1.6103670597076416\n",
      "Training loss:1.5804357528686523\n",
      "Training loss:1.6566520929336548\n",
      "Training loss:1.7209981679916382\n",
      "Training loss:1.5858427286148071\n",
      "Epoch: 0093 loss_train: 150.894895 acc_train: 0.412562 loss_val: 19.020655 acc_val: 0.418833 time: 236722.934758s\n",
      "Training loss:1.6249090433120728\n",
      "Training loss:1.5049785375595093\n",
      "Training loss:1.690866231918335\n",
      "Training loss:1.511896014213562\n",
      "Training loss:1.6413286924362183\n",
      "Training loss:1.5588147640228271\n",
      "Training loss:1.6508519649505615\n",
      "Training loss:1.6897594928741455\n",
      "Training loss:1.560775876045227\n",
      "Training loss:1.631150484085083\n",
      "Training loss:1.685778260231018\n",
      "Training loss:1.601517677307129\n",
      "Training loss:1.609334111213684\n",
      "Training loss:1.6380034685134888\n",
      "Training loss:1.6118731498718262\n",
      "Training loss:1.6115822792053223\n",
      "Training loss:1.6708314418792725\n",
      "Training loss:1.5180444717407227\n",
      "Training loss:1.5929315090179443\n",
      "Training loss:1.5956906080245972\n",
      "Training loss:1.591752052307129\n",
      "Training loss:1.6428736448287964\n",
      "Training loss:1.6423627138137817\n",
      "Training loss:1.5943320989608765\n",
      "Training loss:1.6641987562179565\n",
      "Training loss:1.6074275970458984\n",
      "Training loss:1.5679248571395874\n",
      "Training loss:1.5851575136184692\n",
      "Training loss:1.579432725906372\n",
      "Training loss:1.6235347986221313\n",
      "Training loss:1.5956262350082397\n",
      "Training loss:1.531616449356079\n",
      "Training loss:1.5234652757644653\n",
      "Training loss:1.61838960647583\n",
      "Training loss:1.6079784631729126\n",
      "Training loss:1.5799812078475952\n",
      "Training loss:1.5929778814315796\n",
      "Training loss:1.5572844743728638\n",
      "Training loss:1.596523642539978\n",
      "Training loss:1.5681322813034058\n",
      "Training loss:1.576034426689148\n",
      "Training loss:1.61692214012146\n",
      "Training loss:1.557786226272583\n",
      "Training loss:1.5609129667282104\n",
      "Training loss:1.5712412595748901\n",
      "Training loss:1.6684355735778809\n",
      "Training loss:1.6068552732467651\n",
      "Training loss:1.5406742095947266\n",
      "Training loss:1.5433303117752075\n",
      "Training loss:1.5983877182006836\n",
      "Training loss:1.637778401374817\n",
      "Training loss:1.5988309383392334\n",
      "Training loss:1.5929850339889526\n",
      "Training loss:1.534243106842041\n",
      "Training loss:1.611140489578247\n",
      "Training loss:1.5939717292785645\n",
      "Training loss:1.6064717769622803\n",
      "Training loss:1.592490792274475\n",
      "Training loss:1.6270183324813843\n",
      "Training loss:1.5759087800979614\n",
      "Training loss:1.5404659509658813\n",
      "Training loss:1.63270103931427\n",
      "Training loss:1.6374436616897583\n",
      "Training loss:1.566108226776123\n",
      "Training loss:1.5621460676193237\n",
      "Training loss:1.64845609664917\n",
      "Training loss:1.6461464166641235\n",
      "Training loss:1.5902804136276245\n",
      "Training loss:1.6383520364761353\n",
      "Training loss:1.6118642091751099\n",
      "Training loss:1.6647976636886597\n",
      "Training loss:1.6183432340621948\n",
      "Training loss:1.5693784952163696\n",
      "Training loss:1.5565897226333618\n",
      "Training loss:1.6124274730682373\n",
      "Training loss:1.5532668828964233\n",
      "Training loss:1.6896394491195679\n",
      "Training loss:1.653367519378662\n",
      "Training loss:1.656002163887024\n",
      "Training loss:1.6124752759933472\n",
      "Training loss:1.5827338695526123\n",
      "Training loss:1.5892759561538696\n",
      "Training loss:1.508426308631897\n",
      "Training loss:1.5952558517456055\n",
      "Training loss:1.5745271444320679\n",
      "Training loss:1.6163578033447266\n",
      "Training loss:1.6206425428390503\n",
      "Training loss:1.5945512056350708\n",
      "Training loss:1.5746564865112305\n",
      "Training loss:1.5637743473052979\n",
      "Training loss:1.6127058267593384\n",
      "Training loss:1.6371616125106812\n",
      "Training loss:1.6233141422271729\n",
      "Training loss:1.601770043373108\n",
      "Epoch: 0094 loss_train: 150.438715 acc_train: 0.413354 loss_val: 19.189102 acc_val: 0.409667 time: 239231.791837s\n",
      "Training loss:1.6764549016952515\n",
      "Training loss:1.5719563961029053\n",
      "Training loss:1.5960711240768433\n",
      "Training loss:1.647179126739502\n",
      "Training loss:1.508715271949768\n",
      "Training loss:1.598451852798462\n",
      "Training loss:1.5428591966629028\n",
      "Training loss:1.5593303442001343\n",
      "Training loss:1.584797978401184\n",
      "Training loss:1.5567995309829712\n",
      "Training loss:1.593235969543457\n",
      "Training loss:1.6035466194152832\n",
      "Training loss:1.615712285041809\n",
      "Training loss:1.6356199979782104\n",
      "Training loss:1.5856608152389526\n",
      "Training loss:1.6133586168289185\n",
      "Training loss:1.6167869567871094\n",
      "Training loss:1.6779448986053467\n",
      "Training loss:1.5462795495986938\n",
      "Training loss:1.6908775568008423\n",
      "Training loss:1.6047651767730713\n",
      "Training loss:1.5379984378814697\n",
      "Training loss:1.6078770160675049\n",
      "Training loss:1.5246952772140503\n",
      "Training loss:1.6400126218795776\n",
      "Training loss:1.608979344367981\n",
      "Training loss:1.6083253622055054\n",
      "Training loss:1.5835869312286377\n",
      "Training loss:1.6089024543762207\n",
      "Training loss:1.573254942893982\n",
      "Training loss:1.5976651906967163\n",
      "Training loss:1.6465808153152466\n",
      "Training loss:1.5980803966522217\n",
      "Training loss:1.5819587707519531\n",
      "Training loss:1.5528630018234253\n",
      "Training loss:1.6088030338287354\n",
      "Training loss:1.5935165882110596\n",
      "Training loss:1.5989528894424438\n",
      "Training loss:1.6374967098236084\n",
      "Training loss:1.497092604637146\n",
      "Training loss:1.6477293968200684\n",
      "Training loss:1.5939074754714966\n",
      "Training loss:1.5576826333999634\n",
      "Training loss:1.5836334228515625\n",
      "Training loss:1.608957052230835\n",
      "Training loss:1.5170193910598755\n",
      "Training loss:1.558941125869751\n",
      "Training loss:1.6834622621536255\n",
      "Training loss:1.5947682857513428\n",
      "Training loss:1.5777617692947388\n",
      "Training loss:1.6259305477142334\n",
      "Training loss:1.5559521913528442\n",
      "Training loss:1.5292046070098877\n",
      "Training loss:1.6824482679367065\n",
      "Training loss:1.5925135612487793\n",
      "Training loss:1.624128818511963\n",
      "Training loss:1.6091034412384033\n",
      "Training loss:1.6278823614120483\n",
      "Training loss:1.5907456874847412\n",
      "Training loss:1.6348410844802856\n",
      "Training loss:1.5184205770492554\n",
      "Training loss:1.5610913038253784\n",
      "Training loss:1.5863463878631592\n",
      "Training loss:1.664707899093628\n",
      "Training loss:1.6897568702697754\n",
      "Training loss:1.591314435005188\n",
      "Training loss:1.5609380006790161\n",
      "Training loss:1.5514070987701416\n",
      "Training loss:1.574416160583496\n",
      "Training loss:1.551645040512085\n",
      "Training loss:1.56312096118927\n",
      "Training loss:1.627015471458435\n",
      "Training loss:1.6195926666259766\n",
      "Training loss:1.619282841682434\n",
      "Training loss:1.538758635520935\n",
      "Training loss:1.5496852397918701\n",
      "Training loss:1.6579002141952515\n",
      "Training loss:1.6052662134170532\n",
      "Training loss:1.6446245908737183\n",
      "Training loss:1.5973161458969116\n",
      "Training loss:1.598169207572937\n",
      "Training loss:1.5976537466049194\n",
      "Training loss:1.645182728767395\n",
      "Training loss:1.6026760339736938\n",
      "Training loss:1.6000454425811768\n",
      "Training loss:1.5685272216796875\n",
      "Training loss:1.563383936882019\n",
      "Training loss:1.5779186487197876\n",
      "Training loss:1.6285738945007324\n",
      "Training loss:1.5873130559921265\n",
      "Training loss:1.6212133169174194\n",
      "Training loss:1.5228828191757202\n",
      "Training loss:1.6035641431808472\n",
      "Training loss:1.6144331693649292\n",
      "Epoch: 0095 loss_train: 150.033804 acc_train: 0.416833 loss_val: 19.144932 acc_val: 0.417500 time: 241744.928336s\n",
      "Training loss:1.6524792909622192\n",
      "Training loss:1.6363961696624756\n",
      "Training loss:1.5105171203613281\n",
      "Training loss:1.6185760498046875\n",
      "Training loss:1.657426357269287\n",
      "Training loss:1.5460947751998901\n",
      "Training loss:1.5731803178787231\n",
      "Training loss:1.6448214054107666\n",
      "Training loss:1.5434952974319458\n",
      "Training loss:1.5680153369903564\n",
      "Training loss:1.5879091024398804\n",
      "Training loss:1.5528461933135986\n",
      "Training loss:1.5467617511749268\n",
      "Training loss:1.5503853559494019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5822627544403076\n",
      "Training loss:1.6349585056304932\n",
      "Training loss:1.6127454042434692\n",
      "Training loss:1.6847187280654907\n",
      "Training loss:1.5678833723068237\n",
      "Training loss:1.5977059602737427\n",
      "Training loss:1.6912281513214111\n",
      "Training loss:1.6002650260925293\n",
      "Training loss:1.542915940284729\n",
      "Training loss:1.6000808477401733\n",
      "Training loss:1.653927206993103\n",
      "Training loss:1.6399767398834229\n",
      "Training loss:1.5928921699523926\n",
      "Training loss:1.4967703819274902\n",
      "Training loss:1.6297369003295898\n",
      "Training loss:1.602610468864441\n",
      "Training loss:1.5913722515106201\n",
      "Training loss:1.5786036252975464\n",
      "Training loss:1.5918115377426147\n",
      "Training loss:1.6336314678192139\n",
      "Training loss:1.6247143745422363\n",
      "Training loss:1.5143803358078003\n",
      "Training loss:1.57509183883667\n",
      "Training loss:1.5575555562973022\n",
      "Training loss:1.5570369958877563\n",
      "Training loss:1.5946674346923828\n",
      "Training loss:1.5763823986053467\n",
      "Training loss:1.611464262008667\n",
      "Training loss:1.5524048805236816\n",
      "Training loss:1.6328215599060059\n",
      "Training loss:1.5807949304580688\n",
      "Training loss:1.6863175630569458\n",
      "Training loss:1.5745371580123901\n",
      "Training loss:1.5733355283737183\n",
      "Training loss:1.5517537593841553\n",
      "Training loss:1.539649248123169\n",
      "Training loss:1.5538357496261597\n",
      "Training loss:1.490673303604126\n",
      "Training loss:1.6081329584121704\n",
      "Training loss:1.5644406080245972\n",
      "Training loss:1.5751110315322876\n",
      "Training loss:1.6190298795700073\n",
      "Training loss:1.6239618062973022\n",
      "Training loss:1.5465203523635864\n",
      "Training loss:1.5787701606750488\n",
      "Training loss:1.568705677986145\n",
      "Training loss:1.5625790357589722\n",
      "Training loss:1.605110764503479\n",
      "Training loss:1.6097372770309448\n",
      "Training loss:1.5432153940200806\n",
      "Training loss:1.511481761932373\n",
      "Training loss:1.644803524017334\n",
      "Training loss:1.6436243057250977\n",
      "Training loss:1.6080639362335205\n",
      "Training loss:1.6217595338821411\n",
      "Training loss:1.6280094385147095\n",
      "Training loss:1.6568256616592407\n",
      "Training loss:1.622617483139038\n",
      "Training loss:1.6243137121200562\n",
      "Training loss:1.6119987964630127\n",
      "Training loss:1.6060819625854492\n",
      "Training loss:1.6389086246490479\n",
      "Training loss:1.643771767616272\n",
      "Training loss:1.6169408559799194\n",
      "Training loss:1.5551643371582031\n",
      "Training loss:1.622293472290039\n",
      "Training loss:1.6022381782531738\n",
      "Training loss:1.6600391864776611\n",
      "Training loss:1.5943360328674316\n",
      "Training loss:1.535064697265625\n",
      "Training loss:1.7222206592559814\n",
      "Training loss:1.5793180465698242\n",
      "Training loss:1.5532889366149902\n",
      "Training loss:1.5714329481124878\n",
      "Training loss:1.6046802997589111\n",
      "Training loss:1.6089532375335693\n",
      "Training loss:1.6347719430923462\n",
      "Training loss:1.6638295650482178\n",
      "Training loss:1.5484203100204468\n",
      "Training loss:1.5739860534667969\n",
      "Epoch: 0096 loss_train: 149.976973 acc_train: 0.417833 loss_val: 18.974161 acc_val: 0.422000 time: 244257.724115s\n",
      "Training loss:1.643622636795044\n",
      "Training loss:1.6279085874557495\n",
      "Training loss:1.6080238819122314\n",
      "Training loss:1.6137216091156006\n",
      "Training loss:1.584365725517273\n",
      "Training loss:1.5869557857513428\n",
      "Training loss:1.6137638092041016\n",
      "Training loss:1.6188435554504395\n",
      "Training loss:1.4982231855392456\n",
      "Training loss:1.6114614009857178\n",
      "Training loss:1.6384515762329102\n",
      "Training loss:1.646939754486084\n",
      "Training loss:1.6248574256896973\n",
      "Training loss:1.6152492761611938\n",
      "Training loss:1.6078380346298218\n",
      "Training loss:1.6360347270965576\n",
      "Training loss:1.5608861446380615\n",
      "Training loss:1.5554877519607544\n",
      "Training loss:1.4721664190292358\n",
      "Training loss:1.635765790939331\n",
      "Training loss:1.6348538398742676\n",
      "Training loss:1.5706136226654053\n",
      "Training loss:1.5560628175735474\n",
      "Training loss:1.6643425226211548\n",
      "Training loss:1.5606173276901245\n",
      "Training loss:1.5939868688583374\n",
      "Training loss:1.5524436235427856\n",
      "Training loss:1.6089974641799927\n",
      "Training loss:1.5330970287322998\n",
      "Training loss:1.5694286823272705\n",
      "Training loss:1.6267017126083374\n",
      "Training loss:1.6143946647644043\n",
      "Training loss:1.5830267667770386\n",
      "Training loss:1.5928727388381958\n",
      "Training loss:1.570894479751587\n",
      "Training loss:1.5891623497009277\n",
      "Training loss:1.6573787927627563\n",
      "Training loss:1.6251486539840698\n",
      "Training loss:1.5866073369979858\n",
      "Training loss:1.5776267051696777\n",
      "Training loss:1.5272083282470703\n",
      "Training loss:1.6062613725662231\n",
      "Training loss:1.604081392288208\n",
      "Training loss:1.4882724285125732\n",
      "Training loss:1.5320724248886108\n",
      "Training loss:1.5460962057113647\n",
      "Training loss:1.5934969186782837\n",
      "Training loss:1.6445276737213135\n",
      "Training loss:1.5099358558654785\n",
      "Training loss:1.5713573694229126\n",
      "Training loss:1.572253942489624\n",
      "Training loss:1.6593323945999146\n",
      "Training loss:1.5344078540802002\n",
      "Training loss:1.6125459671020508\n",
      "Training loss:1.6225749254226685\n",
      "Training loss:1.6448760032653809\n",
      "Training loss:1.6212228536605835\n",
      "Training loss:1.5511910915374756\n",
      "Training loss:1.5638320446014404\n",
      "Training loss:1.5894094705581665\n",
      "Training loss:1.5399225950241089\n",
      "Training loss:1.5795639753341675\n",
      "Training loss:1.5199065208435059\n",
      "Training loss:1.650728464126587\n",
      "Training loss:1.6098836660385132\n",
      "Training loss:1.580893874168396\n",
      "Training loss:1.5198383331298828\n",
      "Training loss:1.617310881614685\n",
      "Training loss:1.6606744527816772\n",
      "Training loss:1.5969737768173218\n",
      "Training loss:1.5974382162094116\n",
      "Training loss:1.507065773010254\n",
      "Training loss:1.5999606847763062\n",
      "Training loss:1.6229963302612305\n",
      "Training loss:1.542548418045044\n",
      "Training loss:1.6046801805496216\n",
      "Training loss:1.5717337131500244\n",
      "Training loss:1.5587520599365234\n",
      "Training loss:1.4891331195831299\n",
      "Training loss:1.545619010925293\n",
      "Training loss:1.6075005531311035\n",
      "Training loss:1.6049646139144897\n",
      "Training loss:1.5084763765335083\n",
      "Training loss:1.4750131368637085\n",
      "Training loss:1.655182957649231\n",
      "Training loss:1.572908878326416\n",
      "Training loss:1.5639809370040894\n",
      "Training loss:1.5855375528335571\n",
      "Training loss:1.5654419660568237\n",
      "Training loss:1.643741488456726\n",
      "Training loss:1.57120943069458\n",
      "Training loss:1.5918866395950317\n",
      "Training loss:1.5350950956344604\n",
      "Training loss:1.5903469324111938\n",
      "Epoch: 0097 loss_train: 149.050692 acc_train: 0.421292 loss_val: 18.968745 acc_val: 0.419500 time: 246782.065117s\n",
      "Training loss:1.5569270849227905\n",
      "Training loss:1.555270791053772\n",
      "Training loss:1.570264220237732\n",
      "Training loss:1.65336275100708\n",
      "Training loss:1.6420612335205078\n",
      "Training loss:1.5649068355560303\n",
      "Training loss:1.5760250091552734\n",
      "Training loss:1.557605504989624\n",
      "Training loss:1.6135354042053223\n",
      "Training loss:1.6215591430664062\n",
      "Training loss:1.5579067468643188\n",
      "Training loss:1.5754303932189941\n",
      "Training loss:1.6157245635986328\n",
      "Training loss:1.5974347591400146\n",
      "Training loss:1.5381940603256226\n",
      "Training loss:1.5224487781524658\n",
      "Training loss:1.5573015213012695\n",
      "Training loss:1.67457115650177\n",
      "Training loss:1.5838550329208374\n",
      "Training loss:1.6357194185256958\n",
      "Training loss:1.5619468688964844\n",
      "Training loss:1.5991336107254028\n",
      "Training loss:1.5306349992752075\n",
      "Training loss:1.636432409286499\n",
      "Training loss:1.5689290761947632\n",
      "Training loss:1.619214653968811\n",
      "Training loss:1.5774065256118774\n",
      "Training loss:1.5981251001358032\n",
      "Training loss:1.6006546020507812\n",
      "Training loss:1.5181026458740234\n",
      "Training loss:1.5532457828521729\n",
      "Training loss:1.601117491722107\n",
      "Training loss:1.6290340423583984\n",
      "Training loss:1.59735906124115\n",
      "Training loss:1.5746209621429443\n",
      "Training loss:1.5814213752746582\n",
      "Training loss:1.6390365362167358\n",
      "Training loss:1.4867383241653442\n",
      "Training loss:1.5820746421813965\n",
      "Training loss:1.5685956478118896\n",
      "Training loss:1.5386528968811035\n",
      "Training loss:1.592207431793213\n",
      "Training loss:1.5932310819625854\n",
      "Training loss:1.6379797458648682\n",
      "Training loss:1.6318734884262085\n",
      "Training loss:1.5727484226226807\n",
      "Training loss:1.5604522228240967\n",
      "Training loss:1.5852981805801392\n",
      "Training loss:1.6131019592285156\n",
      "Training loss:1.6758581399917603\n",
      "Training loss:1.5960631370544434\n",
      "Training loss:1.589680552482605\n",
      "Training loss:1.5682759284973145\n",
      "Training loss:1.6251049041748047\n",
      "Training loss:1.5915446281433105\n",
      "Training loss:1.572248101234436\n",
      "Training loss:1.5509980916976929\n",
      "Training loss:1.567610740661621\n",
      "Training loss:1.570343255996704\n",
      "Training loss:1.5115336179733276\n",
      "Training loss:1.67038893699646\n",
      "Training loss:1.516830325126648\n",
      "Training loss:1.564475417137146\n",
      "Training loss:1.608033299446106\n",
      "Training loss:1.6856826543807983\n",
      "Training loss:1.5628135204315186\n",
      "Training loss:1.625774621963501\n",
      "Training loss:1.5572032928466797\n",
      "Training loss:1.5526130199432373\n",
      "Training loss:1.5119411945343018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6329087018966675\n",
      "Training loss:1.6154805421829224\n",
      "Training loss:1.5657751560211182\n",
      "Training loss:1.6441553831100464\n",
      "Training loss:1.634644865989685\n",
      "Training loss:1.6100715398788452\n",
      "Training loss:1.6274590492248535\n",
      "Training loss:1.5920190811157227\n",
      "Training loss:1.5127054452896118\n",
      "Training loss:1.5832525491714478\n",
      "Training loss:1.5944947004318237\n",
      "Training loss:1.607334017753601\n",
      "Training loss:1.5497182607650757\n",
      "Training loss:1.5110116004943848\n",
      "Training loss:1.6937334537506104\n",
      "Training loss:1.5974520444869995\n",
      "Training loss:1.5925564765930176\n",
      "Training loss:1.5395482778549194\n",
      "Training loss:1.586302399635315\n",
      "Training loss:1.5842057466506958\n",
      "Training loss:1.488792061805725\n",
      "Training loss:1.6187655925750732\n",
      "Training loss:1.5797663927078247\n",
      "Training loss:1.59799063205719\n",
      "Epoch: 0098 loss_train: 149.154636 acc_train: 0.420438 loss_val: 18.868240 acc_val: 0.420333 time: 249350.833857s\n",
      "Training loss:1.635643720626831\n",
      "Training loss:1.5974714756011963\n",
      "Training loss:1.5915820598602295\n",
      "Training loss:1.6358921527862549\n",
      "Training loss:1.6307381391525269\n",
      "Training loss:1.6726073026657104\n",
      "Training loss:1.5750751495361328\n",
      "Training loss:1.5523560047149658\n",
      "Training loss:1.6070572137832642\n",
      "Training loss:1.5649335384368896\n",
      "Training loss:1.5769596099853516\n",
      "Training loss:1.5703849792480469\n",
      "Training loss:1.5568675994873047\n",
      "Training loss:1.6725255250930786\n",
      "Training loss:1.4711878299713135\n",
      "Training loss:1.585435390472412\n",
      "Training loss:1.5822397470474243\n",
      "Training loss:1.5937840938568115\n",
      "Training loss:1.5876420736312866\n",
      "Training loss:1.5513771772384644\n",
      "Training loss:1.5834025144577026\n",
      "Training loss:1.613176941871643\n",
      "Training loss:1.5649429559707642\n",
      "Training loss:1.6169519424438477\n",
      "Training loss:1.547662377357483\n",
      "Training loss:1.6083316802978516\n",
      "Training loss:1.5774996280670166\n",
      "Training loss:1.5528619289398193\n",
      "Training loss:1.633528709411621\n",
      "Training loss:1.660470724105835\n",
      "Training loss:1.592299461364746\n",
      "Training loss:1.5334618091583252\n",
      "Training loss:1.6558542251586914\n",
      "Training loss:1.582496166229248\n",
      "Training loss:1.600598692893982\n",
      "Training loss:1.633028268814087\n",
      "Training loss:1.5803450345993042\n",
      "Training loss:1.555185317993164\n",
      "Training loss:1.5953534841537476\n",
      "Training loss:1.6077640056610107\n",
      "Training loss:1.6254463195800781\n",
      "Training loss:1.5531378984451294\n",
      "Training loss:1.6218743324279785\n",
      "Training loss:1.5713136196136475\n",
      "Training loss:1.5765472650527954\n",
      "Training loss:1.6015024185180664\n",
      "Training loss:1.5918256044387817\n",
      "Training loss:1.6205334663391113\n",
      "Training loss:1.563496708869934\n",
      "Training loss:1.6455113887786865\n",
      "Training loss:1.6305551528930664\n",
      "Training loss:1.5883007049560547\n",
      "Training loss:1.5279061794281006\n",
      "Training loss:1.5374010801315308\n",
      "Training loss:1.5233538150787354\n",
      "Training loss:1.620546817779541\n",
      "Training loss:1.6071622371673584\n",
      "Training loss:1.5893425941467285\n",
      "Training loss:1.5261396169662476\n",
      "Training loss:1.4998054504394531\n",
      "Training loss:1.7076218128204346\n",
      "Training loss:1.5433794260025024\n",
      "Training loss:1.604066252708435\n",
      "Training loss:1.6068328619003296\n",
      "Training loss:1.5285810232162476\n",
      "Training loss:1.531306505203247\n",
      "Training loss:1.6141490936279297\n",
      "Training loss:1.5701560974121094\n",
      "Training loss:1.6776621341705322\n",
      "Training loss:1.5764225721359253\n",
      "Training loss:1.6082525253295898\n",
      "Training loss:1.6025633811950684\n",
      "Training loss:1.5452865362167358\n",
      "Training loss:1.6372867822647095\n",
      "Training loss:1.6247570514678955\n",
      "Training loss:1.5610226392745972\n",
      "Training loss:1.5779187679290771\n",
      "Training loss:1.6464406251907349\n",
      "Training loss:1.6379984617233276\n",
      "Training loss:1.6173063516616821\n",
      "Training loss:1.5342442989349365\n",
      "Training loss:1.5765405893325806\n",
      "Training loss:1.5918506383895874\n",
      "Training loss:1.6216728687286377\n",
      "Training loss:1.6243830919265747\n",
      "Training loss:1.5774171352386475\n",
      "Training loss:1.559594988822937\n",
      "Training loss:1.612351655960083\n",
      "Training loss:1.5834325551986694\n",
      "Training loss:1.5665910243988037\n",
      "Training loss:1.5830373764038086\n",
      "Training loss:1.5137609243392944\n",
      "Training loss:1.5733476877212524\n",
      "Training loss:1.5775586366653442\n",
      "Epoch: 0099 loss_train: 149.443474 acc_train: 0.419438 loss_val: 18.743294 acc_val: 0.426833 time: 251994.441237s\n",
      "Training loss:1.558605670928955\n",
      "Training loss:1.5903645753860474\n",
      "Training loss:1.5809777975082397\n",
      "Training loss:1.582629919052124\n",
      "Training loss:1.5434207916259766\n",
      "Training loss:1.6658592224121094\n",
      "Training loss:1.5629454851150513\n",
      "Training loss:1.6265732049942017\n",
      "Training loss:1.5932797193527222\n",
      "Training loss:1.5072413682937622\n",
      "Training loss:1.556836485862732\n",
      "Training loss:1.5937360525131226\n",
      "Training loss:1.589074730873108\n",
      "Training loss:1.5382475852966309\n",
      "Training loss:1.6379948854446411\n",
      "Training loss:1.6093066930770874\n",
      "Training loss:1.5905433893203735\n",
      "Training loss:1.5757689476013184\n",
      "Training loss:1.6046688556671143\n",
      "Training loss:1.544696569442749\n",
      "Training loss:1.5689852237701416\n",
      "Training loss:1.6139850616455078\n",
      "Training loss:1.5481185913085938\n",
      "Training loss:1.5157932043075562\n",
      "Training loss:1.5554518699645996\n",
      "Training loss:1.5556827783584595\n",
      "Training loss:1.5851235389709473\n",
      "Training loss:1.6704750061035156\n",
      "Training loss:1.5461828708648682\n",
      "Training loss:1.5953394174575806\n",
      "Training loss:1.6133629083633423\n",
      "Training loss:1.60306715965271\n",
      "Training loss:1.5573842525482178\n",
      "Training loss:1.6679720878601074\n",
      "Training loss:1.5345088243484497\n",
      "Training loss:1.6013656854629517\n",
      "Training loss:1.5476491451263428\n",
      "Training loss:1.5989768505096436\n",
      "Training loss:1.5664899349212646\n",
      "Training loss:1.5229899883270264\n",
      "Training loss:1.5682251453399658\n",
      "Training loss:1.5558278560638428\n",
      "Training loss:1.6996183395385742\n",
      "Training loss:1.651731252670288\n",
      "Training loss:1.635072112083435\n",
      "Training loss:1.633172869682312\n",
      "Training loss:1.5687389373779297\n",
      "Training loss:1.5831687450408936\n",
      "Training loss:1.6197123527526855\n",
      "Training loss:1.590080976486206\n",
      "Training loss:1.600748062133789\n",
      "Training loss:1.6250473260879517\n",
      "Training loss:1.6282533407211304\n",
      "Training loss:1.5448365211486816\n",
      "Training loss:1.6116220951080322\n",
      "Training loss:1.5998482704162598\n",
      "Training loss:1.4940720796585083\n",
      "Training loss:1.629376769065857\n",
      "Training loss:1.5904676914215088\n",
      "Training loss:1.5794793367385864\n",
      "Training loss:1.6115810871124268\n",
      "Training loss:1.6029019355773926\n",
      "Training loss:1.541792869567871\n",
      "Training loss:1.6145793199539185\n",
      "Training loss:1.5743287801742554\n",
      "Training loss:1.5534042119979858\n",
      "Training loss:1.622700810432434\n",
      "Training loss:1.630204200744629\n",
      "Training loss:1.5819765329360962\n",
      "Training loss:1.560542106628418\n",
      "Training loss:1.5109126567840576\n",
      "Training loss:1.6115509271621704\n",
      "Training loss:1.5458462238311768\n",
      "Training loss:1.5878840684890747\n",
      "Training loss:1.5834869146347046\n",
      "Training loss:1.568132996559143\n",
      "Training loss:1.6321593523025513\n",
      "Training loss:1.6298162937164307\n",
      "Training loss:1.6189961433410645\n",
      "Training loss:1.5849947929382324\n",
      "Training loss:1.5497782230377197\n",
      "Training loss:1.6898282766342163\n",
      "Training loss:1.5644499063491821\n",
      "Training loss:1.5750114917755127\n",
      "Training loss:1.4879615306854248\n",
      "Training loss:1.5402058362960815\n",
      "Training loss:1.5494807958602905\n",
      "Training loss:1.5770885944366455\n",
      "Training loss:1.6280450820922852\n",
      "Training loss:1.538670301437378\n",
      "Training loss:1.6116688251495361\n",
      "Training loss:1.5780503749847412\n",
      "Training loss:1.5009993314743042\n",
      "Training loss:1.5585845708847046\n",
      "Epoch: 0100 loss_train: 148.948392 acc_train: 0.420646 loss_val: 18.826133 acc_val: 0.420500 time: 254554.163573s\n",
      "Training loss:1.5846993923187256\n",
      "Training loss:1.615541696548462\n",
      "Training loss:1.6285457611083984\n",
      "Training loss:1.6108514070510864\n",
      "Training loss:1.634610652923584\n",
      "Training loss:1.576934814453125\n",
      "Training loss:1.5637946128845215\n",
      "Training loss:1.544282078742981\n",
      "Training loss:1.5405350923538208\n",
      "Training loss:1.6284339427947998\n",
      "Training loss:1.4540519714355469\n",
      "Training loss:1.5867674350738525\n",
      "Training loss:1.6181517839431763\n",
      "Training loss:1.6321988105773926\n",
      "Training loss:1.5175641775131226\n",
      "Training loss:1.6342021226882935\n",
      "Training loss:1.650745153427124\n",
      "Training loss:1.5766396522521973\n",
      "Training loss:1.5268816947937012\n",
      "Training loss:1.5897923707962036\n",
      "Training loss:1.5894715785980225\n",
      "Training loss:1.6077278852462769\n",
      "Training loss:1.6028586626052856\n",
      "Training loss:1.5877147912979126\n",
      "Training loss:1.5713615417480469\n",
      "Training loss:1.6053770780563354\n",
      "Training loss:1.5987482070922852\n",
      "Training loss:1.5865777730941772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5608066320419312\n",
      "Training loss:1.5306416749954224\n",
      "Training loss:1.5933456420898438\n",
      "Training loss:1.4968230724334717\n",
      "Training loss:1.583014965057373\n",
      "Training loss:1.6095503568649292\n",
      "Training loss:1.512221097946167\n",
      "Training loss:1.5805418491363525\n",
      "Training loss:1.5501710176467896\n",
      "Training loss:1.5131912231445312\n",
      "Training loss:1.6140493154525757\n",
      "Training loss:1.5866758823394775\n",
      "Training loss:1.6420749425888062\n",
      "Training loss:1.496851921081543\n",
      "Training loss:1.6327126026153564\n",
      "Training loss:1.6275691986083984\n",
      "Training loss:1.6056230068206787\n",
      "Training loss:1.5579988956451416\n",
      "Training loss:1.6122355461120605\n",
      "Training loss:1.6175835132598877\n",
      "Training loss:1.628650426864624\n",
      "Training loss:1.5406501293182373\n",
      "Training loss:1.5881068706512451\n",
      "Training loss:1.5852855443954468\n",
      "Training loss:1.6015013456344604\n",
      "Training loss:1.6310043334960938\n",
      "Training loss:1.5536081790924072\n",
      "Training loss:1.6036748886108398\n",
      "Training loss:1.5267664194107056\n",
      "Training loss:1.6012314558029175\n",
      "Training loss:1.6618367433547974\n",
      "Training loss:1.5335432291030884\n",
      "Training loss:1.6437897682189941\n",
      "Training loss:1.578413486480713\n",
      "Training loss:1.49679696559906\n",
      "Training loss:1.5901143550872803\n",
      "Training loss:1.6022459268569946\n",
      "Training loss:1.5798795223236084\n",
      "Training loss:1.654555082321167\n",
      "Training loss:1.5922564268112183\n",
      "Training loss:1.6040711402893066\n",
      "Training loss:1.630038857460022\n",
      "Training loss:1.5817011594772339\n",
      "Training loss:1.5969065427780151\n",
      "Training loss:1.57378351688385\n",
      "Training loss:1.635972261428833\n",
      "Training loss:1.6224788427352905\n",
      "Training loss:1.658372402191162\n",
      "Training loss:1.672487735748291\n",
      "Training loss:1.6209254264831543\n",
      "Training loss:1.532742977142334\n",
      "Training loss:1.6148042678833008\n",
      "Training loss:1.575434684753418\n",
      "Training loss:1.6333061456680298\n",
      "Training loss:1.6211826801300049\n",
      "Training loss:1.6688237190246582\n",
      "Training loss:1.6142382621765137\n",
      "Training loss:1.6462839841842651\n",
      "Training loss:1.6514534950256348\n",
      "Training loss:1.615563988685608\n",
      "Training loss:1.564965844154358\n",
      "Training loss:1.6670022010803223\n",
      "Training loss:1.596372365951538\n",
      "Training loss:1.5889451503753662\n",
      "Training loss:1.5644692182540894\n",
      "Training loss:1.649789810180664\n",
      "Epoch: 0101 loss_train: 149.784772 acc_train: 0.418333 loss_val: 19.040619 acc_val: 0.413500 time: 257228.650436s\n",
      "Training loss:1.5919256210327148\n",
      "Training loss:1.5594518184661865\n",
      "Training loss:1.5822356939315796\n",
      "Training loss:1.6333386898040771\n",
      "Training loss:1.4781020879745483\n",
      "Training loss:1.5100123882293701\n",
      "Training loss:1.6056857109069824\n",
      "Training loss:1.561111330986023\n",
      "Training loss:1.6166740655899048\n",
      "Training loss:1.5864156484603882\n",
      "Training loss:1.5567384958267212\n",
      "Training loss:1.5631262063980103\n",
      "Training loss:1.5464842319488525\n",
      "Training loss:1.5594013929367065\n",
      "Training loss:1.6229329109191895\n",
      "Training loss:1.5107433795928955\n",
      "Training loss:1.578594446182251\n",
      "Training loss:1.499194860458374\n",
      "Training loss:1.6186919212341309\n",
      "Training loss:1.6378532648086548\n",
      "Training loss:1.5527247190475464\n",
      "Training loss:1.4969967603683472\n",
      "Training loss:1.6059596538543701\n",
      "Training loss:1.5590938329696655\n",
      "Training loss:1.5670819282531738\n",
      "Training loss:1.658004879951477\n",
      "Training loss:1.6036978960037231\n",
      "Training loss:1.5699890851974487\n",
      "Training loss:1.6234878301620483\n",
      "Training loss:1.5968202352523804\n",
      "Training loss:1.610166072845459\n",
      "Training loss:1.6094566583633423\n",
      "Training loss:1.4646267890930176\n",
      "Training loss:1.61763596534729\n",
      "Training loss:1.6041830778121948\n",
      "Training loss:1.5998848676681519\n",
      "Training loss:1.599550724029541\n",
      "Training loss:1.6675359010696411\n",
      "Training loss:1.5858707427978516\n",
      "Training loss:1.6069824695587158\n",
      "Training loss:1.5617384910583496\n",
      "Training loss:1.5486396551132202\n",
      "Training loss:1.619745135307312\n",
      "Training loss:1.662247896194458\n",
      "Training loss:1.5854965448379517\n",
      "Training loss:1.626798391342163\n",
      "Training loss:1.5973800420761108\n",
      "Training loss:1.547798752784729\n",
      "Training loss:1.6115928888320923\n",
      "Training loss:1.605420470237732\n",
      "Training loss:1.5304460525512695\n",
      "Training loss:1.5494332313537598\n",
      "Training loss:1.6202138662338257\n",
      "Training loss:1.6169198751449585\n",
      "Training loss:1.623926043510437\n",
      "Training loss:1.563328742980957\n",
      "Training loss:1.5014649629592896\n",
      "Training loss:1.5448931455612183\n",
      "Training loss:1.5249992609024048\n",
      "Training loss:1.5334525108337402\n",
      "Training loss:1.5638316869735718\n",
      "Training loss:1.5845927000045776\n",
      "Training loss:1.6274361610412598\n",
      "Training loss:1.6514253616333008\n",
      "Training loss:1.5471001863479614\n",
      "Training loss:1.6297035217285156\n",
      "Training loss:1.5825271606445312\n",
      "Training loss:1.5504754781723022\n",
      "Training loss:1.6289094686508179\n",
      "Training loss:1.5189305543899536\n",
      "Training loss:1.5798759460449219\n",
      "Training loss:1.4762452840805054\n",
      "Training loss:1.5752607583999634\n",
      "Training loss:1.5531872510910034\n",
      "Training loss:1.5544241666793823\n",
      "Training loss:1.6152809858322144\n",
      "Training loss:1.565482258796692\n",
      "Training loss:1.5829970836639404\n",
      "Training loss:1.5049431324005127\n",
      "Training loss:1.5451788902282715\n",
      "Training loss:1.5566339492797852\n",
      "Training loss:1.5845705270767212\n",
      "Training loss:1.6175453662872314\n",
      "Training loss:1.5916306972503662\n",
      "Training loss:1.5439295768737793\n",
      "Training loss:1.609229564666748\n",
      "Training loss:1.6496765613555908\n",
      "Training loss:1.5585800409317017\n",
      "Training loss:1.5341870784759521\n",
      "Training loss:1.5554040670394897\n",
      "Training loss:1.5822651386260986\n",
      "Training loss:1.5865514278411865\n",
      "Training loss:1.5533360242843628\n",
      "Training loss:1.5877937078475952\n",
      "Epoch: 0102 loss_train: 148.343542 acc_train: 0.425063 loss_val: 18.740256 acc_val: 0.426333 time: 259809.502192s\n",
      "Training loss:1.5733555555343628\n",
      "Training loss:1.548406720161438\n",
      "Training loss:1.5471735000610352\n",
      "Training loss:1.5815742015838623\n",
      "Training loss:1.592410683631897\n",
      "Training loss:1.5673054456710815\n",
      "Training loss:1.532345175743103\n",
      "Training loss:1.586427092552185\n",
      "Training loss:1.5603740215301514\n",
      "Training loss:1.55575692653656\n",
      "Training loss:1.6190892457962036\n",
      "Training loss:1.5001283884048462\n",
      "Training loss:1.5554018020629883\n",
      "Training loss:1.576116681098938\n",
      "Training loss:1.5665268898010254\n",
      "Training loss:1.5128929615020752\n",
      "Training loss:1.5582263469696045\n",
      "Training loss:1.546893835067749\n",
      "Training loss:1.5640254020690918\n",
      "Training loss:1.6145379543304443\n",
      "Training loss:1.5710128545761108\n",
      "Training loss:1.5122209787368774\n",
      "Training loss:1.5665466785430908\n",
      "Training loss:1.5911129713058472\n",
      "Training loss:1.6485896110534668\n",
      "Training loss:1.5501494407653809\n",
      "Training loss:1.6002050638198853\n",
      "Training loss:1.5472179651260376\n",
      "Training loss:1.5588855743408203\n",
      "Training loss:1.6387344598770142\n",
      "Training loss:1.5581656694412231\n",
      "Training loss:1.5606220960617065\n",
      "Training loss:1.5728052854537964\n",
      "Training loss:1.6250050067901611\n",
      "Training loss:1.5583158731460571\n",
      "Training loss:1.5920580625534058\n",
      "Training loss:1.4928545951843262\n",
      "Training loss:1.6585992574691772\n",
      "Training loss:1.5761030912399292\n",
      "Training loss:1.6721346378326416\n",
      "Training loss:1.6371177434921265\n",
      "Training loss:1.5616626739501953\n",
      "Training loss:1.5868333578109741\n",
      "Training loss:1.5965681076049805\n",
      "Training loss:1.566845178604126\n",
      "Training loss:1.5506420135498047\n",
      "Training loss:1.5614986419677734\n",
      "Training loss:1.5907511711120605\n",
      "Training loss:1.4887338876724243\n",
      "Training loss:1.582054853439331\n",
      "Training loss:1.640834093093872\n",
      "Training loss:1.6380411386489868\n",
      "Training loss:1.519976258277893\n",
      "Training loss:1.6157546043395996\n",
      "Training loss:1.535840630531311\n",
      "Training loss:1.6196283102035522\n",
      "Training loss:1.5783042907714844\n",
      "Training loss:1.6288386583328247\n",
      "Training loss:1.464016318321228\n",
      "Training loss:1.5457578897476196\n",
      "Training loss:1.56680166721344\n",
      "Training loss:1.6384987831115723\n",
      "Training loss:1.5520371198654175\n",
      "Training loss:1.5383827686309814\n",
      "Training loss:1.5062848329544067\n",
      "Training loss:1.5193144083023071\n",
      "Training loss:1.5594618320465088\n",
      "Training loss:1.6135534048080444\n",
      "Training loss:1.5888051986694336\n",
      "Training loss:1.4999810457229614\n",
      "Training loss:1.5785934925079346\n",
      "Training loss:1.5840497016906738\n",
      "Training loss:1.5583723783493042\n",
      "Training loss:1.5496891736984253\n",
      "Training loss:1.512900471687317\n",
      "Training loss:1.6218161582946777\n",
      "Training loss:1.6257083415985107\n",
      "Training loss:1.6000202894210815\n",
      "Training loss:1.5020719766616821\n",
      "Training loss:1.6119608879089355\n",
      "Training loss:1.54753577709198\n",
      "Training loss:1.6364922523498535\n",
      "Training loss:1.599452257156372\n",
      "Training loss:1.6269874572753906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.6339606046676636\n",
      "Training loss:1.6197988986968994\n",
      "Training loss:1.6413377523422241\n",
      "Training loss:1.6460940837860107\n",
      "Training loss:1.513282060623169\n",
      "Training loss:1.5311375856399536\n",
      "Training loss:1.5742863416671753\n",
      "Training loss:1.6655268669128418\n",
      "Training loss:1.5734456777572632\n",
      "Training loss:1.5213302373886108\n",
      "Epoch: 0103 loss_train: 148.078980 acc_train: 0.425250 loss_val: 18.740335 acc_val: 0.430500 time: 262372.106199s\n",
      "Training loss:1.6239982843399048\n",
      "Training loss:1.5317233800888062\n",
      "Training loss:1.6124012470245361\n",
      "Training loss:1.5802695751190186\n",
      "Training loss:1.5675315856933594\n",
      "Training loss:1.5743073225021362\n",
      "Training loss:1.6215639114379883\n",
      "Training loss:1.6731094121932983\n",
      "Training loss:1.5419727563858032\n",
      "Training loss:1.5841645002365112\n",
      "Training loss:1.6086605787277222\n",
      "Training loss:1.5662291049957275\n",
      "Training loss:1.5801955461502075\n",
      "Training loss:1.5673211812973022\n",
      "Training loss:1.5762163400650024\n",
      "Training loss:1.5894863605499268\n",
      "Training loss:1.4978525638580322\n",
      "Training loss:1.6242307424545288\n",
      "Training loss:1.538581371307373\n",
      "Training loss:1.6005808115005493\n",
      "Training loss:1.5610575675964355\n",
      "Training loss:1.5204575061798096\n",
      "Training loss:1.5369161367416382\n",
      "Training loss:1.588545799255371\n",
      "Training loss:1.5660508871078491\n",
      "Training loss:1.62241530418396\n",
      "Training loss:1.5567270517349243\n",
      "Training loss:1.6064659357070923\n",
      "Training loss:1.5281739234924316\n",
      "Training loss:1.5700626373291016\n",
      "Training loss:1.5372660160064697\n",
      "Training loss:1.5724924802780151\n",
      "Training loss:1.6415642499923706\n",
      "Training loss:1.5570346117019653\n",
      "Training loss:1.6045793294906616\n",
      "Training loss:1.6058399677276611\n",
      "Training loss:1.586012601852417\n",
      "Training loss:1.6223914623260498\n",
      "Training loss:1.5287154912948608\n",
      "Training loss:1.5697442293167114\n",
      "Training loss:1.54378080368042\n",
      "Training loss:1.552493929862976\n",
      "Training loss:1.6086558103561401\n",
      "Training loss:1.572170615196228\n",
      "Training loss:1.5633530616760254\n",
      "Training loss:1.6461600065231323\n",
      "Training loss:1.5926165580749512\n",
      "Training loss:1.5598305463790894\n",
      "Training loss:1.5629595518112183\n",
      "Training loss:1.582095980644226\n",
      "Training loss:1.5767241716384888\n",
      "Training loss:1.6031925678253174\n",
      "Training loss:1.5964304208755493\n",
      "Training loss:1.554312825202942\n",
      "Training loss:1.5539084672927856\n",
      "Training loss:1.6117596626281738\n",
      "Training loss:1.6160778999328613\n",
      "Training loss:1.6241596937179565\n",
      "Training loss:1.5618300437927246\n",
      "Training loss:1.5922482013702393\n",
      "Training loss:1.5497710704803467\n",
      "Training loss:1.579945683479309\n",
      "Training loss:1.5210484266281128\n",
      "Training loss:1.5581635236740112\n",
      "Training loss:1.6026655435562134\n",
      "Training loss:1.6116218566894531\n",
      "Training loss:1.5977472066879272\n",
      "Training loss:1.62642240524292\n",
      "Training loss:1.6006890535354614\n",
      "Training loss:1.6487016677856445\n",
      "Training loss:1.5521165132522583\n",
      "Training loss:1.5754516124725342\n",
      "Training loss:1.5694129467010498\n",
      "Training loss:1.5254385471343994\n",
      "Training loss:1.5877939462661743\n",
      "Training loss:1.5072338581085205\n",
      "Training loss:1.577832579612732\n",
      "Training loss:1.576572299003601\n",
      "Training loss:1.5722243785858154\n",
      "Training loss:1.5799899101257324\n",
      "Training loss:1.6206731796264648\n",
      "Training loss:1.6353256702423096\n",
      "Training loss:1.559828281402588\n",
      "Training loss:1.560528039932251\n",
      "Training loss:1.4956914186477661\n",
      "Training loss:1.6325494050979614\n",
      "Training loss:1.5819759368896484\n",
      "Training loss:1.474998950958252\n",
      "Training loss:1.5950449705123901\n",
      "Training loss:1.5225379467010498\n",
      "Training loss:1.586403727531433\n",
      "Training loss:1.51735258102417\n",
      "Training loss:1.5783398151397705\n",
      "Training loss:1.6103887557983398\n",
      "Epoch: 0104 loss_train: 148.310152 acc_train: 0.426063 loss_val: 18.948259 acc_val: 0.418167 time: 264926.661339s\n",
      "Training loss:1.6093491315841675\n",
      "Training loss:1.5421868562698364\n",
      "Training loss:1.555252194404602\n",
      "Training loss:1.6524525880813599\n",
      "Training loss:1.573119878768921\n",
      "Training loss:1.4496233463287354\n",
      "Training loss:1.6148182153701782\n",
      "Training loss:1.5816667079925537\n",
      "Training loss:1.5134671926498413\n",
      "Training loss:1.614498257637024\n",
      "Training loss:1.541774868965149\n",
      "Training loss:1.584876298904419\n",
      "Training loss:1.6084586381912231\n",
      "Training loss:1.6144357919692993\n",
      "Training loss:1.6328823566436768\n",
      "Training loss:1.6651803255081177\n",
      "Training loss:1.5768591165542603\n",
      "Training loss:1.5691341161727905\n",
      "Training loss:1.5107120275497437\n",
      "Training loss:1.5923386812210083\n",
      "Training loss:1.5765361785888672\n",
      "Training loss:1.5885823965072632\n",
      "Training loss:1.6018822193145752\n",
      "Training loss:1.582723617553711\n",
      "Training loss:1.5335708856582642\n",
      "Training loss:1.5788333415985107\n",
      "Training loss:1.5853229761123657\n",
      "Training loss:1.6172157526016235\n",
      "Training loss:1.5709775686264038\n",
      "Training loss:1.521708607673645\n",
      "Training loss:1.6480371952056885\n",
      "Training loss:1.531618356704712\n",
      "Training loss:1.5818719863891602\n",
      "Training loss:1.5786181688308716\n",
      "Training loss:1.6315981149673462\n",
      "Training loss:1.5695663690567017\n",
      "Training loss:1.5969702005386353\n",
      "Training loss:1.5681148767471313\n",
      "Training loss:1.556281566619873\n",
      "Training loss:1.5013792514801025\n",
      "Training loss:1.569456696510315\n",
      "Training loss:1.5535063743591309\n",
      "Training loss:1.5788507461547852\n",
      "Training loss:1.565711259841919\n",
      "Training loss:1.5323480367660522\n",
      "Training loss:1.6114342212677002\n",
      "Training loss:1.603110909461975\n",
      "Training loss:1.5336226224899292\n",
      "Training loss:1.599858283996582\n",
      "Training loss:1.6174085140228271\n",
      "Training loss:1.622951865196228\n",
      "Training loss:1.5175395011901855\n",
      "Training loss:1.6349583864212036\n",
      "Training loss:1.507672667503357\n",
      "Training loss:1.587116003036499\n",
      "Training loss:1.6077245473861694\n",
      "Training loss:1.597812533378601\n",
      "Training loss:1.5535403490066528\n",
      "Training loss:1.6226142644882202\n",
      "Training loss:1.5859183073043823\n",
      "Training loss:1.540148138999939\n",
      "Training loss:1.570836067199707\n",
      "Training loss:1.5092195272445679\n",
      "Training loss:1.6190019845962524\n",
      "Training loss:1.6301758289337158\n",
      "Training loss:1.5775781869888306\n",
      "Training loss:1.5569967031478882\n",
      "Training loss:1.702867031097412\n",
      "Training loss:1.5685439109802246\n",
      "Training loss:1.6190017461776733\n",
      "Training loss:1.5099759101867676\n",
      "Training loss:1.5430786609649658\n",
      "Training loss:1.6287870407104492\n",
      "Training loss:1.5724695920944214\n",
      "Training loss:1.5487911701202393\n",
      "Training loss:1.6321276426315308\n",
      "Training loss:1.573509693145752\n",
      "Training loss:1.5852372646331787\n",
      "Training loss:1.6241298913955688\n",
      "Training loss:1.5869616270065308\n",
      "Training loss:1.5519678592681885\n",
      "Training loss:1.523856282234192\n",
      "Training loss:1.534989595413208\n",
      "Training loss:1.5818870067596436\n",
      "Training loss:1.4985339641571045\n",
      "Training loss:1.6597778797149658\n",
      "Training loss:1.6240397691726685\n",
      "Training loss:1.585200548171997\n",
      "Training loss:1.669226050376892\n",
      "Training loss:1.5786384344100952\n",
      "Training loss:1.6035828590393066\n",
      "Training loss:1.519464373588562\n",
      "Training loss:1.5339794158935547\n",
      "Training loss:1.5800275802612305\n",
      "Epoch: 0105 loss_train: 148.472262 acc_train: 0.423250 loss_val: 18.703802 acc_val: 0.426333 time: 267450.741148s\n",
      "Training loss:1.5954020023345947\n",
      "Training loss:1.4821569919586182\n",
      "Training loss:1.5742238759994507\n",
      "Training loss:1.5537734031677246\n",
      "Training loss:1.5732556581497192\n",
      "Training loss:1.5504192113876343\n",
      "Training loss:1.5216736793518066\n",
      "Training loss:1.6692233085632324\n",
      "Training loss:1.5590780973434448\n",
      "Training loss:1.5564625263214111\n",
      "Training loss:1.6281788349151611\n",
      "Training loss:1.4911203384399414\n",
      "Training loss:1.5371453762054443\n",
      "Training loss:1.5399779081344604\n",
      "Training loss:1.6896283626556396\n",
      "Training loss:1.6078717708587646\n",
      "Training loss:1.5505481958389282\n",
      "Training loss:1.5408034324645996\n",
      "Training loss:1.5104697942733765\n",
      "Training loss:1.6330777406692505\n",
      "Training loss:1.5723751783370972\n",
      "Training loss:1.6070338487625122\n",
      "Training loss:1.6057463884353638\n",
      "Training loss:1.5365349054336548\n",
      "Training loss:1.5245548486709595\n",
      "Training loss:1.5255502462387085\n",
      "Training loss:1.5663533210754395\n",
      "Training loss:1.5388038158416748\n",
      "Training loss:1.5659375190734863\n",
      "Training loss:1.6201626062393188\n",
      "Training loss:1.5377399921417236\n",
      "Training loss:1.6809483766555786\n",
      "Training loss:1.608985424041748\n",
      "Training loss:1.6453014612197876\n",
      "Training loss:1.6516950130462646\n",
      "Training loss:1.5899255275726318\n",
      "Training loss:1.675117015838623\n",
      "Training loss:1.5649319887161255\n",
      "Training loss:1.6090340614318848\n",
      "Training loss:1.5714863538742065\n",
      "Training loss:1.60897958278656\n",
      "Training loss:1.5821566581726074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.559993863105774\n",
      "Training loss:1.6020230054855347\n",
      "Training loss:1.506492018699646\n",
      "Training loss:1.5639070272445679\n",
      "Training loss:1.5068049430847168\n",
      "Training loss:1.6010078191757202\n",
      "Training loss:1.6763819456100464\n",
      "Training loss:1.5652271509170532\n",
      "Training loss:1.5346630811691284\n",
      "Training loss:1.6707987785339355\n",
      "Training loss:1.664366364479065\n",
      "Training loss:1.6000670194625854\n",
      "Training loss:1.6377809047698975\n",
      "Training loss:1.5634194612503052\n",
      "Training loss:1.5764602422714233\n",
      "Training loss:1.54568350315094\n",
      "Training loss:1.5083436965942383\n",
      "Training loss:1.5165935754776\n",
      "Training loss:1.5310488939285278\n",
      "Training loss:1.5603153705596924\n",
      "Training loss:1.5611320734024048\n",
      "Training loss:1.5428003072738647\n",
      "Training loss:1.548619270324707\n",
      "Training loss:1.6287976503372192\n",
      "Training loss:1.5874170064926147\n",
      "Training loss:1.5520877838134766\n",
      "Training loss:1.6222630739212036\n",
      "Training loss:1.5026159286499023\n",
      "Training loss:1.669266700744629\n",
      "Training loss:1.6296135187149048\n",
      "Training loss:1.5104522705078125\n",
      "Training loss:1.501139521598816\n",
      "Training loss:1.5842233896255493\n",
      "Training loss:1.5588666200637817\n",
      "Training loss:1.5388411283493042\n",
      "Training loss:1.567468285560608\n",
      "Training loss:1.5777798891067505\n",
      "Training loss:1.5539182424545288\n",
      "Training loss:1.5198582410812378\n",
      "Training loss:1.6342853307724\n",
      "Training loss:1.5255460739135742\n",
      "Training loss:1.539784550666809\n",
      "Training loss:1.6060383319854736\n",
      "Training loss:1.6104425191879272\n",
      "Training loss:1.5254483222961426\n",
      "Training loss:1.58268141746521\n",
      "Training loss:1.5358870029449463\n",
      "Training loss:1.4917699098587036\n",
      "Training loss:1.5603009462356567\n",
      "Training loss:1.573837399482727\n",
      "Training loss:1.567978858947754\n",
      "Training loss:1.611791729927063\n",
      "Epoch: 0106 loss_train: 147.968177 acc_train: 0.425500 loss_val: 18.815321 acc_val: 0.419333 time: 269976.348162s\n",
      "Training loss:1.542427659034729\n",
      "Training loss:1.6005924940109253\n",
      "Training loss:1.464813470840454\n",
      "Training loss:1.5843679904937744\n",
      "Training loss:1.546148419380188\n",
      "Training loss:1.5691462755203247\n",
      "Training loss:1.5227935314178467\n",
      "Training loss:1.4993077516555786\n",
      "Training loss:1.6398489475250244\n",
      "Training loss:1.5512663125991821\n",
      "Training loss:1.692678451538086\n",
      "Training loss:1.5960547924041748\n",
      "Training loss:1.5524171590805054\n",
      "Training loss:1.6416616439819336\n",
      "Training loss:1.5758038759231567\n",
      "Training loss:1.6155720949172974\n",
      "Training loss:1.5563443899154663\n",
      "Training loss:1.5794169902801514\n",
      "Training loss:1.6000968217849731\n",
      "Training loss:1.581511378288269\n",
      "Training loss:1.5105246305465698\n",
      "Training loss:1.589613914489746\n",
      "Training loss:1.5497084856033325\n",
      "Training loss:1.5662554502487183\n",
      "Training loss:1.544407606124878\n",
      "Training loss:1.5950912237167358\n",
      "Training loss:1.6160457134246826\n",
      "Training loss:1.5741623640060425\n",
      "Training loss:1.5705170631408691\n",
      "Training loss:1.6077195405960083\n",
      "Training loss:1.5242247581481934\n",
      "Training loss:1.593263030052185\n",
      "Training loss:1.4827096462249756\n",
      "Training loss:1.5844618082046509\n",
      "Training loss:1.549852967262268\n",
      "Training loss:1.5159027576446533\n",
      "Training loss:1.5812029838562012\n",
      "Training loss:1.5083918571472168\n",
      "Training loss:1.6334646940231323\n",
      "Training loss:1.5102026462554932\n",
      "Training loss:1.4975767135620117\n",
      "Training loss:1.6221038103103638\n",
      "Training loss:1.6067291498184204\n",
      "Training loss:1.545231819152832\n",
      "Training loss:1.5884408950805664\n",
      "Training loss:1.592938780784607\n",
      "Training loss:1.5888750553131104\n",
      "Training loss:1.6096081733703613\n",
      "Training loss:1.578873872756958\n",
      "Training loss:1.5776926279067993\n",
      "Training loss:1.6131947040557861\n",
      "Training loss:1.6434520483016968\n",
      "Training loss:1.5197688341140747\n",
      "Training loss:1.55465829372406\n",
      "Training loss:1.5474742650985718\n",
      "Training loss:1.487414836883545\n",
      "Training loss:1.6180366277694702\n",
      "Training loss:1.5655096769332886\n",
      "Training loss:1.5992990732192993\n",
      "Training loss:1.5615726709365845\n",
      "Training loss:1.5652996301651\n",
      "Training loss:1.5496306419372559\n",
      "Training loss:1.5331823825836182\n",
      "Training loss:1.5563749074935913\n",
      "Training loss:1.6148555278778076\n",
      "Training loss:1.6076613664627075\n",
      "Training loss:1.5923101902008057\n",
      "Training loss:1.5568346977233887\n",
      "Training loss:1.6487956047058105\n",
      "Training loss:1.542458176612854\n",
      "Training loss:1.5571892261505127\n",
      "Training loss:1.6420514583587646\n",
      "Training loss:1.56562340259552\n",
      "Training loss:1.564914584159851\n",
      "Training loss:1.5942604541778564\n",
      "Training loss:1.5685287714004517\n",
      "Training loss:1.5714454650878906\n",
      "Training loss:1.6024225950241089\n",
      "Training loss:1.5447123050689697\n",
      "Training loss:1.5376396179199219\n",
      "Training loss:1.5993760824203491\n",
      "Training loss:1.5958046913146973\n",
      "Training loss:1.5587736368179321\n",
      "Training loss:1.5395638942718506\n",
      "Training loss:1.6297695636749268\n",
      "Training loss:1.607926607131958\n",
      "Training loss:1.5957642793655396\n",
      "Training loss:1.583836555480957\n",
      "Training loss:1.5748188495635986\n",
      "Training loss:1.507639765739441\n",
      "Training loss:1.5239511728286743\n",
      "Training loss:1.5450016260147095\n",
      "Training loss:1.5783182382583618\n",
      "Training loss:1.616849422454834\n",
      "Epoch: 0107 loss_train: 147.812031 acc_train: 0.427979 loss_val: 18.713126 acc_val: 0.429000 time: 272505.087556s\n",
      "Training loss:1.5209126472473145\n",
      "Training loss:1.6319231986999512\n",
      "Training loss:1.590720772743225\n",
      "Training loss:1.5631459951400757\n",
      "Training loss:1.5810755491256714\n",
      "Training loss:1.5795235633850098\n",
      "Training loss:1.5551460981369019\n",
      "Training loss:1.4798271656036377\n",
      "Training loss:1.5871212482452393\n",
      "Training loss:1.506575584411621\n",
      "Training loss:1.6754378080368042\n",
      "Training loss:1.5801657438278198\n",
      "Training loss:1.4857746362686157\n",
      "Training loss:1.512930989265442\n",
      "Training loss:1.5686805248260498\n",
      "Training loss:1.4845271110534668\n",
      "Training loss:1.520769476890564\n",
      "Training loss:1.5433579683303833\n",
      "Training loss:1.5586518049240112\n",
      "Training loss:1.589069128036499\n",
      "Training loss:1.5481011867523193\n",
      "Training loss:1.5995147228240967\n",
      "Training loss:1.592660665512085\n",
      "Training loss:1.5597906112670898\n",
      "Training loss:1.5950549840927124\n",
      "Training loss:1.601900339126587\n",
      "Training loss:1.476233959197998\n",
      "Training loss:1.5340512990951538\n",
      "Training loss:1.546759843826294\n",
      "Training loss:1.5658069849014282\n",
      "Training loss:1.6053705215454102\n",
      "Training loss:1.615660548210144\n",
      "Training loss:1.6778006553649902\n",
      "Training loss:1.5602672100067139\n",
      "Training loss:1.5476031303405762\n",
      "Training loss:1.5627232789993286\n",
      "Training loss:1.5602781772613525\n",
      "Training loss:1.6283111572265625\n",
      "Training loss:1.4745339155197144\n",
      "Training loss:1.554887294769287\n",
      "Training loss:1.6161296367645264\n",
      "Training loss:1.5498270988464355\n",
      "Training loss:1.5077886581420898\n",
      "Training loss:1.5848368406295776\n",
      "Training loss:1.5904172658920288\n",
      "Training loss:1.561231255531311\n",
      "Training loss:1.6588177680969238\n",
      "Training loss:1.5795222520828247\n",
      "Training loss:1.5666298866271973\n",
      "Training loss:1.5560368299484253\n",
      "Training loss:1.6156750917434692\n",
      "Training loss:1.5973423719406128\n",
      "Training loss:1.6008186340332031\n",
      "Training loss:1.5828845500946045\n",
      "Training loss:1.5729494094848633\n",
      "Training loss:1.4971369504928589\n",
      "Training loss:1.5350781679153442\n",
      "Training loss:1.5384259223937988\n",
      "Training loss:1.6078323125839233\n",
      "Training loss:1.5314013957977295\n",
      "Training loss:1.5600141286849976\n",
      "Training loss:1.5461562871932983\n",
      "Training loss:1.5426828861236572\n",
      "Training loss:1.6844305992126465\n",
      "Training loss:1.5967193841934204\n",
      "Training loss:1.5953140258789062\n",
      "Training loss:1.6192160844802856\n",
      "Training loss:1.530787467956543\n",
      "Training loss:1.516486406326294\n",
      "Training loss:1.5224659442901611\n",
      "Training loss:1.5016682147979736\n",
      "Training loss:1.5102145671844482\n",
      "Training loss:1.609274983406067\n",
      "Training loss:1.573006272315979\n",
      "Training loss:1.5748087167739868\n",
      "Training loss:1.6431300640106201\n",
      "Training loss:1.6007797718048096\n",
      "Training loss:1.5228816270828247\n",
      "Training loss:1.585592269897461\n",
      "Training loss:1.5175302028656006\n",
      "Training loss:1.6158506870269775\n",
      "Training loss:1.6139885187149048\n",
      "Training loss:1.509577989578247\n",
      "Training loss:1.5581411123275757\n",
      "Training loss:1.5977085828781128\n",
      "Training loss:1.5661933422088623\n",
      "Training loss:1.5361374616622925\n",
      "Training loss:1.630856990814209\n",
      "Training loss:1.5284842252731323\n",
      "Training loss:1.5368846654891968\n",
      "Training loss:1.606866717338562\n",
      "Training loss:1.58474600315094\n",
      "Training loss:1.5600537061691284\n",
      "Training loss:1.570267677307129\n",
      "Epoch: 0108 loss_train: 147.342347 acc_train: 0.427146 loss_val: 18.643179 acc_val: 0.427667 time: 275019.300166s\n",
      "Training loss:1.578323483467102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5591319799423218\n",
      "Training loss:1.6034886837005615\n",
      "Training loss:1.4850150346755981\n",
      "Training loss:1.5402096509933472\n",
      "Training loss:1.5767828226089478\n",
      "Training loss:1.5703232288360596\n",
      "Training loss:1.5494177341461182\n",
      "Training loss:1.610176920890808\n",
      "Training loss:1.5343022346496582\n",
      "Training loss:1.5982738733291626\n",
      "Training loss:1.5911756753921509\n",
      "Training loss:1.5747699737548828\n",
      "Training loss:1.5435740947723389\n",
      "Training loss:1.5667887926101685\n",
      "Training loss:1.595766544342041\n",
      "Training loss:1.4824470281600952\n",
      "Training loss:1.5946170091629028\n",
      "Training loss:1.546149492263794\n",
      "Training loss:1.5643112659454346\n",
      "Training loss:1.562666654586792\n",
      "Training loss:1.5864169597625732\n",
      "Training loss:1.5418701171875\n",
      "Training loss:1.578547716140747\n",
      "Training loss:1.5520087480545044\n",
      "Training loss:1.5111844539642334\n",
      "Training loss:1.5668870210647583\n",
      "Training loss:1.5990220308303833\n",
      "Training loss:1.6408168077468872\n",
      "Training loss:1.5511232614517212\n",
      "Training loss:1.519755244255066\n",
      "Training loss:1.6343982219696045\n",
      "Training loss:1.586993932723999\n",
      "Training loss:1.5958744287490845\n",
      "Training loss:1.5411055088043213\n",
      "Training loss:1.5128716230392456\n",
      "Training loss:1.6367032527923584\n",
      "Training loss:1.5476486682891846\n",
      "Training loss:1.5509618520736694\n",
      "Training loss:1.6033413410186768\n",
      "Training loss:1.583032488822937\n",
      "Training loss:1.6619775295257568\n",
      "Training loss:1.5201870203018188\n",
      "Training loss:1.5679261684417725\n",
      "Training loss:1.5013315677642822\n",
      "Training loss:1.5670723915100098\n",
      "Training loss:1.5968843698501587\n",
      "Training loss:1.5991078615188599\n",
      "Training loss:1.5286375284194946\n",
      "Training loss:1.5965709686279297\n",
      "Training loss:1.6196274757385254\n",
      "Training loss:1.5825108289718628\n",
      "Training loss:1.622840166091919\n",
      "Training loss:1.5747064352035522\n",
      "Training loss:1.632714033126831\n",
      "Training loss:1.635233759880066\n",
      "Training loss:1.5879088640213013\n",
      "Training loss:1.579362392425537\n",
      "Training loss:1.561259150505066\n",
      "Training loss:1.658604621887207\n",
      "Training loss:1.6877802610397339\n",
      "Training loss:1.545238733291626\n",
      "Training loss:1.5470396280288696\n",
      "Training loss:1.6034228801727295\n",
      "Training loss:1.5791360139846802\n",
      "Training loss:1.4999886751174927\n",
      "Training loss:1.5838274955749512\n",
      "Training loss:1.5902034044265747\n",
      "Training loss:1.575705647468567\n",
      "Training loss:1.5510363578796387\n",
      "Training loss:1.5796433687210083\n",
      "Training loss:1.5533581972122192\n",
      "Training loss:1.486869215965271\n",
      "Training loss:1.5249786376953125\n",
      "Training loss:1.5370746850967407\n",
      "Training loss:1.5791349411010742\n",
      "Training loss:1.5125653743743896\n",
      "Training loss:1.5204839706420898\n",
      "Training loss:1.552990198135376\n",
      "Training loss:1.6033767461776733\n",
      "Training loss:1.6369225978851318\n",
      "Training loss:1.613239049911499\n",
      "Training loss:1.5833263397216797\n",
      "Training loss:1.5331110954284668\n",
      "Training loss:1.5518920421600342\n",
      "Training loss:1.5374470949172974\n",
      "Training loss:1.596800446510315\n",
      "Training loss:1.4981982707977295\n",
      "Training loss:1.5635746717453003\n",
      "Training loss:1.5821634531021118\n",
      "Training loss:1.6129207611083984\n",
      "Training loss:1.5737330913543701\n",
      "Training loss:1.5658659934997559\n",
      "Training loss:1.5963902473449707\n",
      "Epoch: 0109 loss_train: 147.722179 acc_train: 0.427708 loss_val: 18.678173 acc_val: 0.434000 time: 277578.721523s\n",
      "Training loss:1.5948395729064941\n",
      "Training loss:1.496962308883667\n",
      "Training loss:1.5471420288085938\n",
      "Training loss:1.5083509683609009\n",
      "Training loss:1.524451494216919\n",
      "Training loss:1.5712785720825195\n",
      "Training loss:1.5703258514404297\n",
      "Training loss:1.633823037147522\n",
      "Training loss:1.5204776525497437\n",
      "Training loss:1.5243107080459595\n",
      "Training loss:1.5206907987594604\n",
      "Training loss:1.5837985277175903\n",
      "Training loss:1.5477734804153442\n",
      "Training loss:1.5359035730361938\n",
      "Training loss:1.627633810043335\n",
      "Training loss:1.4775819778442383\n",
      "Training loss:1.5592955350875854\n",
      "Training loss:1.6085973978042603\n",
      "Training loss:1.5085272789001465\n",
      "Training loss:1.5971554517745972\n",
      "Training loss:1.6355642080307007\n",
      "Training loss:1.5790636539459229\n",
      "Training loss:1.6203432083129883\n",
      "Training loss:1.5855904817581177\n",
      "Training loss:1.5200481414794922\n",
      "Training loss:1.565315842628479\n",
      "Training loss:1.5518568754196167\n",
      "Training loss:1.5525619983673096\n",
      "Training loss:1.5427049398422241\n",
      "Training loss:1.6511722803115845\n",
      "Training loss:1.571820616722107\n",
      "Training loss:1.5469642877578735\n",
      "Training loss:1.5438504219055176\n",
      "Training loss:1.6468287706375122\n",
      "Training loss:1.5598278045654297\n",
      "Training loss:1.5549678802490234\n",
      "Training loss:1.628772258758545\n",
      "Training loss:1.576755166053772\n",
      "Training loss:1.5605040788650513\n",
      "Training loss:1.5580021142959595\n",
      "Training loss:1.5635924339294434\n",
      "Training loss:1.6239298582077026\n",
      "Training loss:1.5731581449508667\n",
      "Training loss:1.5689841508865356\n",
      "Training loss:1.5516254901885986\n",
      "Training loss:1.5239838361740112\n",
      "Training loss:1.6475520133972168\n",
      "Training loss:1.4779975414276123\n",
      "Training loss:1.5625122785568237\n",
      "Training loss:1.697089433670044\n",
      "Training loss:1.5828169584274292\n",
      "Training loss:1.527683138847351\n",
      "Training loss:1.5483179092407227\n",
      "Training loss:1.547432780265808\n",
      "Training loss:1.5794436931610107\n",
      "Training loss:1.6246724128723145\n",
      "Training loss:1.5275880098342896\n",
      "Training loss:1.6017922163009644\n",
      "Training loss:1.5596319437026978\n",
      "Training loss:1.4863293170928955\n",
      "Training loss:1.6398566961288452\n",
      "Training loss:1.5636317729949951\n",
      "Training loss:1.5135213136672974\n",
      "Training loss:1.5196958780288696\n",
      "Training loss:1.535865068435669\n",
      "Training loss:1.5439947843551636\n",
      "Training loss:1.5528879165649414\n",
      "Training loss:1.6411319971084595\n",
      "Training loss:1.5680383443832397\n",
      "Training loss:1.5248024463653564\n",
      "Training loss:1.5360004901885986\n",
      "Training loss:1.5511683225631714\n",
      "Training loss:1.6232548952102661\n",
      "Training loss:1.5473071336746216\n",
      "Training loss:1.5815775394439697\n",
      "Training loss:1.649601936340332\n",
      "Training loss:1.46815824508667\n",
      "Training loss:1.616885781288147\n",
      "Training loss:1.6138622760772705\n",
      "Training loss:1.5671972036361694\n",
      "Training loss:1.5065962076187134\n",
      "Training loss:1.6050524711608887\n",
      "Training loss:1.5682467222213745\n",
      "Training loss:1.5345853567123413\n",
      "Training loss:1.5089921951293945\n",
      "Training loss:1.5774734020233154\n",
      "Training loss:1.5921818017959595\n",
      "Training loss:1.4609614610671997\n",
      "Training loss:1.5920519828796387\n",
      "Training loss:1.601473093032837\n",
      "Training loss:1.5247347354888916\n",
      "Training loss:1.5057209730148315\n",
      "Training loss:1.610041618347168\n",
      "Training loss:1.5132336616516113\n",
      "Epoch: 0110 loss_train: 147.047354 acc_train: 0.429417 loss_val: 18.571373 acc_val: 0.436167 time: 280179.555346s\n",
      "Training loss:1.5876091718673706\n",
      "Training loss:1.5469167232513428\n",
      "Training loss:1.509393572807312\n",
      "Training loss:1.5448662042617798\n",
      "Training loss:1.5894415378570557\n",
      "Training loss:1.565590500831604\n",
      "Training loss:1.5877037048339844\n",
      "Training loss:1.4803149700164795\n",
      "Training loss:1.5794956684112549\n",
      "Training loss:1.5740175247192383\n",
      "Training loss:1.5119799375534058\n",
      "Training loss:1.57659912109375\n",
      "Training loss:1.5568366050720215\n",
      "Training loss:1.5272948741912842\n",
      "Training loss:1.5250451564788818\n",
      "Training loss:1.5680636167526245\n",
      "Training loss:1.556240200996399\n",
      "Training loss:1.5151238441467285\n",
      "Training loss:1.5240195989608765\n",
      "Training loss:1.5427137613296509\n",
      "Training loss:1.6392154693603516\n",
      "Training loss:1.5179574489593506\n",
      "Training loss:1.5716279745101929\n",
      "Training loss:1.5762133598327637\n",
      "Training loss:1.5045690536499023\n",
      "Training loss:1.5467480421066284\n",
      "Training loss:1.541085124015808\n",
      "Training loss:1.5521812438964844\n",
      "Training loss:1.5605220794677734\n",
      "Training loss:1.530124306678772\n",
      "Training loss:1.5243529081344604\n",
      "Training loss:1.5406465530395508\n",
      "Training loss:1.5190391540527344\n",
      "Training loss:1.5172741413116455\n",
      "Training loss:1.4722919464111328\n",
      "Training loss:1.534460186958313\n",
      "Training loss:1.600529432296753\n",
      "Training loss:1.5963952541351318\n",
      "Training loss:1.529827356338501\n",
      "Training loss:1.7229678630828857\n",
      "Training loss:1.6369504928588867\n",
      "Training loss:1.613423228263855\n",
      "Training loss:1.6140849590301514\n",
      "Training loss:1.5925745964050293\n",
      "Training loss:1.5505881309509277\n",
      "Training loss:1.5504748821258545\n",
      "Training loss:1.525233507156372\n",
      "Training loss:1.541917324066162\n",
      "Training loss:1.6137919425964355\n",
      "Training loss:1.6897684335708618\n",
      "Training loss:1.555564284324646\n",
      "Training loss:1.5702874660491943\n",
      "Training loss:1.5245198011398315\n",
      "Training loss:1.5198417901992798\n",
      "Training loss:1.542932152748108\n",
      "Training loss:1.5785518884658813\n",
      "Training loss:1.5425779819488525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.525523066520691\n",
      "Training loss:1.540892243385315\n",
      "Training loss:1.5709149837493896\n",
      "Training loss:1.5142617225646973\n",
      "Training loss:1.5889463424682617\n",
      "Training loss:1.6285392045974731\n",
      "Training loss:1.6309869289398193\n",
      "Training loss:1.5869674682617188\n",
      "Training loss:1.5924090147018433\n",
      "Training loss:1.5742486715316772\n",
      "Training loss:1.594781517982483\n",
      "Training loss:1.4679269790649414\n",
      "Training loss:1.601635456085205\n",
      "Training loss:1.661782145500183\n",
      "Training loss:1.4758996963500977\n",
      "Training loss:1.5651582479476929\n",
      "Training loss:1.6163315773010254\n",
      "Training loss:1.5643184185028076\n",
      "Training loss:1.6351733207702637\n",
      "Training loss:1.6337101459503174\n",
      "Training loss:1.505919098854065\n",
      "Training loss:1.5022389888763428\n",
      "Training loss:1.5576517581939697\n",
      "Training loss:1.6352862119674683\n",
      "Training loss:1.6329090595245361\n",
      "Training loss:1.5623419284820557\n",
      "Training loss:1.5857397317886353\n",
      "Training loss:1.5669522285461426\n",
      "Training loss:1.568359136581421\n",
      "Training loss:1.6451635360717773\n",
      "Training loss:1.5826058387756348\n",
      "Training loss:1.5514758825302124\n",
      "Training loss:1.6092383861541748\n",
      "Training loss:1.5426818132400513\n",
      "Training loss:1.5574294328689575\n",
      "Training loss:1.564428448677063\n",
      "Training loss:1.5132008790969849\n",
      "Epoch: 0111 loss_train: 147.084408 acc_train: 0.431729 loss_val: 18.541826 acc_val: 0.430500 time: 282914.110546s\n",
      "Training loss:1.556350827217102\n",
      "Training loss:1.4613916873931885\n",
      "Training loss:1.6178706884384155\n",
      "Training loss:1.5711909532546997\n",
      "Training loss:1.537722110748291\n",
      "Training loss:1.708160638809204\n",
      "Training loss:1.5725255012512207\n",
      "Training loss:1.6116650104522705\n",
      "Training loss:1.64276123046875\n",
      "Training loss:1.5425816774368286\n",
      "Training loss:1.5800151824951172\n",
      "Training loss:1.5092393159866333\n",
      "Training loss:1.6222361326217651\n",
      "Training loss:1.552648901939392\n",
      "Training loss:1.6446022987365723\n",
      "Training loss:1.4833014011383057\n",
      "Training loss:1.5273984670639038\n",
      "Training loss:1.6042327880859375\n",
      "Training loss:1.5078920125961304\n",
      "Training loss:1.6538782119750977\n",
      "Training loss:1.5779378414154053\n",
      "Training loss:1.5821220874786377\n",
      "Training loss:1.6075024604797363\n",
      "Training loss:1.5589317083358765\n",
      "Training loss:1.5029491186141968\n",
      "Training loss:1.6313673257827759\n",
      "Training loss:1.503113865852356\n",
      "Training loss:1.5234322547912598\n",
      "Training loss:1.5352541208267212\n",
      "Training loss:1.58681321144104\n",
      "Training loss:1.5435527563095093\n",
      "Training loss:1.6023976802825928\n",
      "Training loss:1.5999867916107178\n",
      "Training loss:1.5518827438354492\n",
      "Training loss:1.6003719568252563\n",
      "Training loss:1.6031701564788818\n",
      "Training loss:1.5102298259735107\n",
      "Training loss:1.5711606740951538\n",
      "Training loss:1.4827252626419067\n",
      "Training loss:1.461717128753662\n",
      "Training loss:1.5411382913589478\n",
      "Training loss:1.565604567527771\n",
      "Training loss:1.548871636390686\n",
      "Training loss:1.592719316482544\n",
      "Training loss:1.5816258192062378\n",
      "Training loss:1.5281195640563965\n",
      "Training loss:1.5888592004776\n",
      "Training loss:1.594201683998108\n",
      "Training loss:1.616429090499878\n",
      "Training loss:1.5917479991912842\n",
      "Training loss:1.5088680982589722\n",
      "Training loss:1.5267847776412964\n",
      "Training loss:1.5696830749511719\n",
      "Training loss:1.5110234022140503\n",
      "Training loss:1.6161412000656128\n",
      "Training loss:1.5222841501235962\n",
      "Training loss:1.5194926261901855\n",
      "Training loss:1.5682216882705688\n",
      "Training loss:1.5328677892684937\n",
      "Training loss:1.576107144355774\n",
      "Training loss:1.552497148513794\n",
      "Training loss:1.5680387020111084\n",
      "Training loss:1.5779739618301392\n",
      "Training loss:1.5499935150146484\n",
      "Training loss:1.5722767114639282\n",
      "Training loss:1.6069906949996948\n",
      "Training loss:1.6161627769470215\n",
      "Training loss:1.524840235710144\n",
      "Training loss:1.5284627676010132\n",
      "Training loss:1.5458952188491821\n",
      "Training loss:1.5713309049606323\n",
      "Training loss:1.6698400974273682\n",
      "Training loss:1.5189063549041748\n",
      "Training loss:1.563962697982788\n",
      "Training loss:1.4644169807434082\n",
      "Training loss:1.5999456644058228\n",
      "Training loss:1.5437649488449097\n",
      "Training loss:1.5664525032043457\n",
      "Training loss:1.5595924854278564\n",
      "Training loss:1.554020881652832\n",
      "Training loss:1.6694318056106567\n",
      "Training loss:1.6005661487579346\n",
      "Training loss:1.4860981702804565\n",
      "Training loss:1.5444154739379883\n",
      "Training loss:1.5264760255813599\n",
      "Training loss:1.518425464630127\n",
      "Training loss:1.539703130722046\n",
      "Training loss:1.5873152017593384\n",
      "Training loss:1.5903980731964111\n",
      "Training loss:1.6104464530944824\n",
      "Training loss:1.531212329864502\n",
      "Training loss:1.564518690109253\n",
      "Training loss:1.6093926429748535\n",
      "Training loss:1.5325952768325806\n",
      "Epoch: 0112 loss_train: 147.011435 acc_train: 0.432312 loss_val: 18.548149 acc_val: 0.439500 time: 285502.220116s\n",
      "Training loss:1.5192008018493652\n",
      "Training loss:1.5927351713180542\n",
      "Training loss:1.5629712343215942\n",
      "Training loss:1.650269627571106\n",
      "Training loss:1.5373129844665527\n",
      "Training loss:1.5872840881347656\n",
      "Training loss:1.5841161012649536\n",
      "Training loss:1.4852632284164429\n",
      "Training loss:1.608446717262268\n",
      "Training loss:1.678870439529419\n",
      "Training loss:1.536148190498352\n",
      "Training loss:1.5353294610977173\n",
      "Training loss:1.569231390953064\n",
      "Training loss:1.5977954864501953\n",
      "Training loss:1.6023684740066528\n",
      "Training loss:1.6825965642929077\n",
      "Training loss:1.4847726821899414\n",
      "Training loss:1.5803219079971313\n",
      "Training loss:1.476462483406067\n",
      "Training loss:1.4908239841461182\n",
      "Training loss:1.625205397605896\n",
      "Training loss:1.5545991659164429\n",
      "Training loss:1.5479352474212646\n",
      "Training loss:1.575974702835083\n",
      "Training loss:1.5445725917816162\n",
      "Training loss:1.5508904457092285\n",
      "Training loss:1.624678373336792\n",
      "Training loss:1.5423868894577026\n",
      "Training loss:1.57595956325531\n",
      "Training loss:1.6581565141677856\n",
      "Training loss:1.6065826416015625\n",
      "Training loss:1.624994158744812\n",
      "Training loss:1.5789339542388916\n",
      "Training loss:1.5328941345214844\n",
      "Training loss:1.5842599868774414\n",
      "Training loss:1.6486108303070068\n",
      "Training loss:1.539581537246704\n",
      "Training loss:1.553970456123352\n",
      "Training loss:1.562170147895813\n",
      "Training loss:1.629366397857666\n",
      "Training loss:1.6021270751953125\n",
      "Training loss:1.5603593587875366\n",
      "Training loss:1.539079189300537\n",
      "Training loss:1.5574588775634766\n",
      "Training loss:1.5695531368255615\n",
      "Training loss:1.6129217147827148\n",
      "Training loss:1.4964179992675781\n",
      "Training loss:1.6397134065628052\n",
      "Training loss:1.5648908615112305\n",
      "Training loss:1.5487488508224487\n",
      "Training loss:1.5907615423202515\n",
      "Training loss:1.5672250986099243\n",
      "Training loss:1.6310697793960571\n",
      "Training loss:1.5783525705337524\n",
      "Training loss:1.6159765720367432\n",
      "Training loss:1.6760135889053345\n",
      "Training loss:1.6208142042160034\n",
      "Training loss:1.5901790857315063\n",
      "Training loss:1.577243447303772\n",
      "Training loss:1.4849777221679688\n",
      "Training loss:1.5036044120788574\n",
      "Training loss:1.5569230318069458\n",
      "Training loss:1.517409324645996\n",
      "Training loss:1.568176031112671\n",
      "Training loss:1.5037603378295898\n",
      "Training loss:1.5130256414413452\n",
      "Training loss:1.6339311599731445\n",
      "Training loss:1.5059856176376343\n",
      "Training loss:1.5897278785705566\n",
      "Training loss:1.606039047241211\n",
      "Training loss:1.462720513343811\n",
      "Training loss:1.5519771575927734\n",
      "Training loss:1.525149941444397\n",
      "Training loss:1.5490607023239136\n",
      "Training loss:1.512385606765747\n",
      "Training loss:1.6029008626937866\n",
      "Training loss:1.5562529563903809\n",
      "Training loss:1.5500671863555908\n",
      "Training loss:1.534923791885376\n",
      "Training loss:1.509974479675293\n",
      "Training loss:1.6054668426513672\n",
      "Training loss:1.5795148611068726\n",
      "Training loss:1.553231120109558\n",
      "Training loss:1.5803040266036987\n",
      "Training loss:1.5250060558319092\n",
      "Training loss:1.617300033569336\n",
      "Training loss:1.5435014963150024\n",
      "Training loss:1.5761998891830444\n",
      "Training loss:1.5256270170211792\n",
      "Training loss:1.4939857721328735\n",
      "Training loss:1.5584486722946167\n",
      "Training loss:1.5659140348434448\n",
      "Training loss:1.553419828414917\n",
      "Training loss:1.512948989868164\n",
      "Epoch: 0113 loss_train: 147.292793 acc_train: 0.428604 loss_val: 18.512081 acc_val: 0.439000 time: 287964.313824s\n",
      "Training loss:1.5268580913543701\n",
      "Training loss:1.5653088092803955\n",
      "Training loss:1.5872403383255005\n",
      "Training loss:1.509172797203064\n",
      "Training loss:1.636799693107605\n",
      "Training loss:1.632078766822815\n",
      "Training loss:1.5929515361785889\n",
      "Training loss:1.6159114837646484\n",
      "Training loss:1.5676085948944092\n",
      "Training loss:1.595084547996521\n",
      "Training loss:1.5531501770019531\n",
      "Training loss:1.5799418687820435\n",
      "Training loss:1.5578316450119019\n",
      "Training loss:1.6496336460113525\n",
      "Training loss:1.5613369941711426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5209323167800903\n",
      "Training loss:1.545190453529358\n",
      "Training loss:1.5357109308242798\n",
      "Training loss:1.5617949962615967\n",
      "Training loss:1.5911712646484375\n",
      "Training loss:1.5790865421295166\n",
      "Training loss:1.5714787244796753\n",
      "Training loss:1.4694277048110962\n",
      "Training loss:1.4952244758605957\n",
      "Training loss:1.5925793647766113\n",
      "Training loss:1.5551899671554565\n",
      "Training loss:1.5485886335372925\n",
      "Training loss:1.5804661512374878\n",
      "Training loss:1.5324358940124512\n",
      "Training loss:1.5675145387649536\n",
      "Training loss:1.5545871257781982\n",
      "Training loss:1.5071128606796265\n",
      "Training loss:1.5111125707626343\n",
      "Training loss:1.519918441772461\n",
      "Training loss:1.569729208946228\n",
      "Training loss:1.5079514980316162\n",
      "Training loss:1.5903112888336182\n",
      "Training loss:1.5328121185302734\n",
      "Training loss:1.5368947982788086\n",
      "Training loss:1.5644561052322388\n",
      "Training loss:1.5719565153121948\n",
      "Training loss:1.572982668876648\n",
      "Training loss:1.602766752243042\n",
      "Training loss:1.4887385368347168\n",
      "Training loss:1.6478921175003052\n",
      "Training loss:1.5294535160064697\n",
      "Training loss:1.5054900646209717\n",
      "Training loss:1.5307995080947876\n",
      "Training loss:1.5327197313308716\n",
      "Training loss:1.5576200485229492\n",
      "Training loss:1.5063790082931519\n",
      "Training loss:1.4920798540115356\n",
      "Training loss:1.5245816707611084\n",
      "Training loss:1.4824312925338745\n",
      "Training loss:1.5433728694915771\n",
      "Training loss:1.549489974975586\n",
      "Training loss:1.5063459873199463\n",
      "Training loss:1.548439860343933\n",
      "Training loss:1.546463966369629\n",
      "Training loss:1.577559232711792\n",
      "Training loss:1.5714879035949707\n",
      "Training loss:1.5281859636306763\n",
      "Training loss:1.5325082540512085\n",
      "Training loss:1.5519709587097168\n",
      "Training loss:1.5203089714050293\n",
      "Training loss:1.6284501552581787\n",
      "Training loss:1.5993497371673584\n",
      "Training loss:1.5472854375839233\n",
      "Training loss:1.5795276165008545\n",
      "Training loss:1.610124111175537\n",
      "Training loss:1.5392061471939087\n",
      "Training loss:1.5600862503051758\n",
      "Training loss:1.496023416519165\n",
      "Training loss:1.4967155456542969\n",
      "Training loss:1.5692713260650635\n",
      "Training loss:1.5460662841796875\n",
      "Training loss:1.5011985301971436\n",
      "Training loss:1.5502647161483765\n",
      "Training loss:1.563164472579956\n",
      "Training loss:1.5588772296905518\n",
      "Training loss:1.6028066873550415\n",
      "Training loss:1.599200963973999\n",
      "Training loss:1.5868252515792847\n",
      "Training loss:1.5665203332901\n",
      "Training loss:1.586694598197937\n",
      "Training loss:1.5830307006835938\n",
      "Training loss:1.5538889169692993\n",
      "Training loss:1.5624476671218872\n",
      "Training loss:1.591017246246338\n",
      "Training loss:1.6162354946136475\n",
      "Training loss:1.5694862604141235\n",
      "Training loss:1.5545496940612793\n",
      "Training loss:1.5389541387557983\n",
      "Training loss:1.4443198442459106\n",
      "Epoch: 0114 loss_train: 146.196201 acc_train: 0.434125 loss_val: 18.510606 acc_val: 0.438833 time: 290765.499945s\n",
      "Training loss:1.5337867736816406\n",
      "Training loss:1.5446178913116455\n",
      "Training loss:1.5250743627548218\n",
      "Training loss:1.63837730884552\n",
      "Training loss:1.569617509841919\n",
      "Training loss:1.5437397956848145\n",
      "Training loss:1.5827257633209229\n",
      "Training loss:1.5788087844848633\n",
      "Training loss:1.5588114261627197\n",
      "Training loss:1.4944568872451782\n",
      "Training loss:1.5807849168777466\n",
      "Training loss:1.652182698249817\n",
      "Training loss:1.5421394109725952\n",
      "Training loss:1.5737107992172241\n",
      "Training loss:1.556960940361023\n",
      "Training loss:1.525181770324707\n",
      "Training loss:1.638903260231018\n",
      "Training loss:1.5773704051971436\n",
      "Training loss:1.5521937608718872\n",
      "Training loss:1.620588779449463\n",
      "Training loss:1.5251888036727905\n",
      "Training loss:1.5715599060058594\n",
      "Training loss:1.5749006271362305\n",
      "Training loss:1.5126620531082153\n",
      "Training loss:1.4979400634765625\n",
      "Training loss:1.5657670497894287\n",
      "Training loss:1.4981087446212769\n",
      "Training loss:1.5551166534423828\n",
      "Training loss:1.6234827041625977\n",
      "Training loss:1.5740453004837036\n",
      "Training loss:1.6367782354354858\n",
      "Training loss:1.615816354751587\n",
      "Training loss:1.539979338645935\n",
      "Training loss:1.5284276008605957\n",
      "Training loss:1.505588173866272\n",
      "Training loss:1.574275255203247\n",
      "Training loss:1.4955031871795654\n",
      "Training loss:1.56291663646698\n",
      "Training loss:1.522790789604187\n",
      "Training loss:1.5676302909851074\n",
      "Training loss:1.5183193683624268\n",
      "Training loss:1.5649304389953613\n",
      "Training loss:1.517364501953125\n",
      "Training loss:1.5876868963241577\n",
      "Training loss:1.4987891912460327\n",
      "Training loss:1.577682375907898\n",
      "Training loss:1.5861648321151733\n",
      "Training loss:1.5554227828979492\n",
      "Training loss:1.5236287117004395\n",
      "Training loss:1.5876580476760864\n",
      "Training loss:1.5352002382278442\n",
      "Training loss:1.5237056016921997\n",
      "Training loss:1.6061116456985474\n",
      "Training loss:1.531890869140625\n",
      "Training loss:1.5393942594528198\n",
      "Training loss:1.5490893125534058\n",
      "Training loss:1.5258079767227173\n",
      "Training loss:1.516748309135437\n",
      "Training loss:1.4907400608062744\n",
      "Training loss:1.5337978601455688\n",
      "Training loss:1.5484482049942017\n",
      "Training loss:1.5492786169052124\n",
      "Training loss:1.5389878749847412\n",
      "Training loss:1.5927331447601318\n",
      "Training loss:1.5848699808120728\n",
      "Training loss:1.57927405834198\n",
      "Training loss:1.5096310377120972\n",
      "Training loss:1.6042146682739258\n",
      "Training loss:1.6270537376403809\n",
      "Training loss:1.607353925704956\n",
      "Training loss:1.5418260097503662\n",
      "Training loss:1.6070243120193481\n",
      "Training loss:1.565930724143982\n",
      "Training loss:1.4894039630889893\n",
      "Training loss:1.6243371963500977\n",
      "Training loss:1.4942322969436646\n",
      "Training loss:1.592937707901001\n",
      "Training loss:1.5654892921447754\n",
      "Training loss:1.5842944383621216\n",
      "Training loss:1.552391767501831\n",
      "Training loss:1.588838815689087\n",
      "Training loss:1.5990809202194214\n",
      "Training loss:1.6666762828826904\n",
      "Training loss:1.5704444646835327\n",
      "Training loss:1.5346176624298096\n",
      "Training loss:1.5173509120941162\n",
      "Training loss:1.4411063194274902\n",
      "Training loss:1.5703412294387817\n",
      "Training loss:1.5660158395767212\n",
      "Training loss:1.5841542482376099\n",
      "Training loss:1.536543369293213\n",
      "Training loss:1.5716636180877686\n",
      "Training loss:1.5501551628112793\n",
      "Training loss:1.5512243509292603\n",
      "Epoch: 0115 loss_train: 146.516570 acc_train: 0.435042 loss_val: 18.552262 acc_val: 0.431167 time: 293765.598711s\n",
      "Training loss:1.5709774494171143\n",
      "Training loss:1.5338305234909058\n",
      "Training loss:1.4912562370300293\n",
      "Training loss:1.5710780620574951\n",
      "Training loss:1.6137276887893677\n",
      "Training loss:1.5573559999465942\n",
      "Training loss:1.4600399732589722\n",
      "Training loss:1.562158226966858\n",
      "Training loss:1.5946065187454224\n",
      "Training loss:1.5473746061325073\n",
      "Training loss:1.5676331520080566\n",
      "Training loss:1.5565017461776733\n",
      "Training loss:1.6081589460372925\n",
      "Training loss:1.6116501092910767\n",
      "Training loss:1.4922573566436768\n",
      "Training loss:1.4958515167236328\n",
      "Training loss:1.5294404029846191\n",
      "Training loss:1.5527654886245728\n",
      "Training loss:1.5310715436935425\n",
      "Training loss:1.6583385467529297\n",
      "Training loss:1.5690289735794067\n",
      "Training loss:1.6414947509765625\n",
      "Training loss:1.4866032600402832\n",
      "Training loss:1.524713158607483\n",
      "Training loss:1.527123212814331\n",
      "Training loss:1.4625520706176758\n",
      "Training loss:1.5445959568023682\n",
      "Training loss:1.5038694143295288\n",
      "Training loss:1.5542306900024414\n",
      "Training loss:1.5399291515350342\n",
      "Training loss:1.5245115756988525\n",
      "Training loss:1.5358225107192993\n",
      "Training loss:1.512040376663208\n",
      "Training loss:1.5611571073532104\n",
      "Training loss:1.5946789979934692\n",
      "Training loss:1.649946689605713\n",
      "Training loss:1.486925721168518\n",
      "Training loss:1.5097236633300781\n",
      "Training loss:1.5717248916625977\n",
      "Training loss:1.5211069583892822\n",
      "Training loss:1.5349842309951782\n",
      "Training loss:1.4799038171768188\n",
      "Training loss:1.6173814535140991\n",
      "Training loss:1.5841593742370605\n",
      "Training loss:1.4901577234268188\n",
      "Training loss:1.5895882844924927\n",
      "Training loss:1.5495178699493408\n",
      "Training loss:1.6283493041992188\n",
      "Training loss:1.5464932918548584\n",
      "Training loss:1.5341678857803345\n",
      "Training loss:1.5417051315307617\n",
      "Training loss:1.5523602962493896\n",
      "Training loss:1.6991870403289795\n",
      "Training loss:1.5441007614135742\n",
      "Training loss:1.510320782661438\n",
      "Training loss:1.6083483695983887\n",
      "Training loss:1.5578840970993042\n",
      "Training loss:1.5834987163543701\n",
      "Training loss:1.517659068107605\n",
      "Training loss:1.6185433864593506\n",
      "Training loss:1.5788195133209229\n",
      "Training loss:1.6148751974105835\n",
      "Training loss:1.5847718715667725\n",
      "Training loss:1.5294394493103027\n",
      "Training loss:1.5732078552246094\n",
      "Training loss:1.5316267013549805\n",
      "Training loss:1.5587201118469238\n",
      "Training loss:1.5330711603164673\n",
      "Training loss:1.6141263246536255\n",
      "Training loss:1.5040283203125\n",
      "Training loss:1.5239861011505127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5500575304031372\n",
      "Training loss:1.5340838432312012\n",
      "Training loss:1.5475958585739136\n",
      "Training loss:1.6095269918441772\n",
      "Training loss:1.5918266773223877\n",
      "Training loss:1.610426425933838\n",
      "Training loss:1.5595970153808594\n",
      "Training loss:1.5089675188064575\n",
      "Training loss:1.510326862335205\n",
      "Training loss:1.6178810596466064\n",
      "Training loss:1.5182608366012573\n",
      "Training loss:1.541227102279663\n",
      "Training loss:1.5286160707473755\n",
      "Training loss:1.565585970878601\n",
      "Training loss:1.5283368825912476\n",
      "Training loss:1.5688631534576416\n",
      "Training loss:1.5282671451568604\n",
      "Training loss:1.5782452821731567\n",
      "Training loss:1.5103657245635986\n",
      "Training loss:1.5198898315429688\n",
      "Training loss:1.530428171157837\n",
      "Training loss:1.4868751764297485\n",
      "Training loss:1.5550061464309692\n",
      "Epoch: 0116 loss_train: 145.963094 acc_train: 0.434729 loss_val: 18.436413 acc_val: 0.437000 time: 296820.383137s\n",
      "Training loss:1.5779917240142822\n",
      "Training loss:1.4952634572982788\n",
      "Training loss:1.5069736242294312\n",
      "Training loss:1.510298490524292\n",
      "Training loss:1.5308326482772827\n",
      "Training loss:1.5541937351226807\n",
      "Training loss:1.5524684190750122\n",
      "Training loss:1.5093201398849487\n",
      "Training loss:1.5026841163635254\n",
      "Training loss:1.5516563653945923\n",
      "Training loss:1.5292737483978271\n",
      "Training loss:1.6054044961929321\n",
      "Training loss:1.6476762294769287\n",
      "Training loss:1.5409092903137207\n",
      "Training loss:1.5091590881347656\n",
      "Training loss:1.5272794961929321\n",
      "Training loss:1.5319057703018188\n",
      "Training loss:1.5296574831008911\n",
      "Training loss:1.5391473770141602\n",
      "Training loss:1.605124831199646\n",
      "Training loss:1.5516095161437988\n",
      "Training loss:1.602465271949768\n",
      "Training loss:1.5498830080032349\n",
      "Training loss:1.5556583404541016\n",
      "Training loss:1.4560983180999756\n",
      "Training loss:1.5867503881454468\n",
      "Training loss:1.5734330415725708\n",
      "Training loss:1.4940266609191895\n",
      "Training loss:1.5076998472213745\n",
      "Training loss:1.562340497970581\n",
      "Training loss:1.5729981660842896\n",
      "Training loss:1.5452436208724976\n",
      "Training loss:1.6020172834396362\n",
      "Training loss:1.5169451236724854\n",
      "Training loss:1.5385370254516602\n",
      "Training loss:1.5654360055923462\n",
      "Training loss:1.5302186012268066\n",
      "Training loss:1.5201150178909302\n",
      "Training loss:1.4531000852584839\n",
      "Training loss:1.531303882598877\n",
      "Training loss:1.5570327043533325\n",
      "Training loss:1.5591806173324585\n",
      "Training loss:1.5781619548797607\n",
      "Training loss:1.5759632587432861\n",
      "Training loss:1.5246179103851318\n",
      "Training loss:1.4879621267318726\n",
      "Training loss:1.5340453386306763\n",
      "Training loss:1.5406973361968994\n",
      "Training loss:1.618350625038147\n",
      "Training loss:1.6069966554641724\n",
      "Training loss:1.6525269746780396\n",
      "Training loss:1.6038978099822998\n",
      "Training loss:1.5223251581192017\n",
      "Training loss:1.5860575437545776\n",
      "Training loss:1.542662501335144\n",
      "Training loss:1.601064682006836\n",
      "Training loss:1.5797840356826782\n",
      "Training loss:1.5123114585876465\n",
      "Training loss:1.486615538597107\n",
      "Training loss:1.5206875801086426\n",
      "Training loss:1.5441714525222778\n",
      "Training loss:1.4762063026428223\n",
      "Training loss:1.6143770217895508\n",
      "Training loss:1.5864568948745728\n",
      "Training loss:1.6085612773895264\n",
      "Training loss:1.5620943307876587\n",
      "Training loss:1.7032849788665771\n",
      "Training loss:1.4827556610107422\n",
      "Training loss:1.5223556756973267\n",
      "Training loss:1.5407006740570068\n",
      "Training loss:1.6028348207473755\n",
      "Training loss:1.5650149583816528\n",
      "Training loss:1.6103243827819824\n",
      "Training loss:1.5574955940246582\n",
      "Training loss:1.5951673984527588\n",
      "Training loss:1.6149909496307373\n",
      "Training loss:1.6148133277893066\n",
      "Training loss:1.5704123973846436\n",
      "Training loss:1.5802476406097412\n",
      "Training loss:1.5460559129714966\n",
      "Training loss:1.5813463926315308\n",
      "Training loss:1.574303150177002\n",
      "Training loss:1.5312421321868896\n",
      "Training loss:1.561711072921753\n",
      "Training loss:1.613856315612793\n",
      "Training loss:1.54090416431427\n",
      "Training loss:1.586764931678772\n",
      "Training loss:1.5727335214614868\n",
      "Training loss:1.5644283294677734\n",
      "Training loss:1.5776015520095825\n",
      "Training loss:1.5644129514694214\n",
      "Training loss:1.5537325143814087\n",
      "Training loss:1.5736380815505981\n",
      "Training loss:1.5230885744094849\n",
      "Epoch: 0117 loss_train: 146.282127 acc_train: 0.435063 loss_val: 18.483407 acc_val: 0.436833 time: 299406.618036s\n",
      "Training loss:1.5251470804214478\n",
      "Training loss:1.6349458694458008\n",
      "Training loss:1.6050233840942383\n",
      "Training loss:1.5762125253677368\n",
      "Training loss:1.5838764905929565\n",
      "Training loss:1.5671497583389282\n",
      "Training loss:1.5840144157409668\n",
      "Training loss:1.5786327123641968\n",
      "Training loss:1.564332127571106\n",
      "Training loss:1.5781008005142212\n",
      "Training loss:1.5083298683166504\n",
      "Training loss:1.597413420677185\n",
      "Training loss:1.6025605201721191\n",
      "Training loss:1.4699528217315674\n",
      "Training loss:1.5278693437576294\n",
      "Training loss:1.4829442501068115\n",
      "Training loss:1.4528498649597168\n",
      "Training loss:1.544928789138794\n",
      "Training loss:1.5506784915924072\n",
      "Training loss:1.5720516443252563\n",
      "Training loss:1.4936400651931763\n",
      "Training loss:1.5868381261825562\n",
      "Training loss:1.5706110000610352\n",
      "Training loss:1.5787066221237183\n",
      "Training loss:1.5969864130020142\n",
      "Training loss:1.471255898475647\n",
      "Training loss:1.519238829612732\n",
      "Training loss:1.5831966400146484\n",
      "Training loss:1.5216467380523682\n",
      "Training loss:1.5124720335006714\n",
      "Training loss:1.4960057735443115\n",
      "Training loss:1.5773766040802002\n",
      "Training loss:1.642612099647522\n",
      "Training loss:1.5768612623214722\n",
      "Training loss:1.5250298976898193\n",
      "Training loss:1.522197961807251\n",
      "Training loss:1.5132200717926025\n",
      "Training loss:1.5707296133041382\n",
      "Training loss:1.5473976135253906\n",
      "Training loss:1.5505969524383545\n",
      "Training loss:1.6046669483184814\n",
      "Training loss:1.5331273078918457\n",
      "Training loss:1.628200888633728\n",
      "Training loss:1.5898518562316895\n",
      "Training loss:1.5787428617477417\n",
      "Training loss:1.5987353324890137\n",
      "Training loss:1.5863780975341797\n",
      "Training loss:1.544033169746399\n",
      "Training loss:1.5175355672836304\n",
      "Training loss:1.419233798980713\n",
      "Training loss:1.577122688293457\n",
      "Training loss:1.527931571006775\n",
      "Training loss:1.5482795238494873\n",
      "Training loss:1.515763759613037\n",
      "Training loss:1.529730200767517\n",
      "Training loss:1.5355528593063354\n",
      "Training loss:1.5723578929901123\n",
      "Training loss:1.5862213373184204\n",
      "Training loss:1.5741595029830933\n",
      "Training loss:1.5547497272491455\n",
      "Training loss:1.5238564014434814\n",
      "Training loss:1.5325289964675903\n",
      "Training loss:1.5369890928268433\n",
      "Training loss:1.4907814264297485\n",
      "Training loss:1.4825694561004639\n",
      "Training loss:1.5344774723052979\n",
      "Training loss:1.4889922142028809\n",
      "Training loss:1.5634338855743408\n",
      "Training loss:1.5093475580215454\n",
      "Training loss:1.5652339458465576\n",
      "Training loss:1.5636228322982788\n",
      "Training loss:1.5195056200027466\n",
      "Training loss:1.5720373392105103\n",
      "Training loss:1.6201436519622803\n",
      "Training loss:1.6313611268997192\n",
      "Training loss:1.5269466638565063\n",
      "Training loss:1.527955174446106\n",
      "Training loss:1.5617306232452393\n",
      "Training loss:1.5276451110839844\n",
      "Training loss:1.5643837451934814\n",
      "Training loss:1.5485478639602661\n",
      "Training loss:1.586672306060791\n",
      "Training loss:1.5616375207901\n",
      "Training loss:1.511643409729004\n",
      "Training loss:1.6103042364120483\n",
      "Training loss:1.475836992263794\n",
      "Training loss:1.5947413444519043\n",
      "Training loss:1.5676240921020508\n",
      "Training loss:1.5831516981124878\n",
      "Training loss:1.5118858814239502\n",
      "Training loss:1.5820239782333374\n",
      "Training loss:1.5746384859085083\n",
      "Training loss:1.6714489459991455\n",
      "Training loss:1.4768428802490234\n",
      "Epoch: 0118 loss_train: 145.886551 acc_train: 0.435563 loss_val: 18.662753 acc_val: 0.433667 time: 301870.838270s\n",
      "Training loss:1.5408682823181152\n",
      "Training loss:1.4821338653564453\n",
      "Training loss:1.5397788286209106\n",
      "Training loss:1.501295804977417\n",
      "Training loss:1.5161924362182617\n",
      "Training loss:1.6251078844070435\n",
      "Training loss:1.6150500774383545\n",
      "Training loss:1.542885422706604\n",
      "Training loss:1.5972611904144287\n",
      "Training loss:1.541343331336975\n",
      "Training loss:1.5001362562179565\n",
      "Training loss:1.5598200559616089\n",
      "Training loss:1.559853434562683\n",
      "Training loss:1.541898250579834\n",
      "Training loss:1.4952863454818726\n",
      "Training loss:1.5305856466293335\n",
      "Training loss:1.5241632461547852\n",
      "Training loss:1.4613442420959473\n",
      "Training loss:1.5433677434921265\n",
      "Training loss:1.5221277475357056\n",
      "Training loss:1.5514522790908813\n",
      "Training loss:1.5428234338760376\n",
      "Training loss:1.5807024240493774\n",
      "Training loss:1.497752070426941\n",
      "Training loss:1.5039961338043213\n",
      "Training loss:1.5701566934585571\n",
      "Training loss:1.6239925622940063\n",
      "Training loss:1.513056755065918\n",
      "Training loss:1.5520092248916626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.499451994895935\n",
      "Training loss:1.4821078777313232\n",
      "Training loss:1.5314339399337769\n",
      "Training loss:1.6137449741363525\n",
      "Training loss:1.5695046186447144\n",
      "Training loss:1.5034515857696533\n",
      "Training loss:1.5861905813217163\n",
      "Training loss:1.556330919265747\n",
      "Training loss:1.5712041854858398\n",
      "Training loss:1.4934053421020508\n",
      "Training loss:1.5883959531784058\n",
      "Training loss:1.5569599866867065\n",
      "Training loss:1.5895441770553589\n",
      "Training loss:1.4913854598999023\n",
      "Training loss:1.5401313304901123\n",
      "Training loss:1.547133445739746\n",
      "Training loss:1.4264949560165405\n",
      "Training loss:1.5491715669631958\n",
      "Training loss:1.5376485586166382\n",
      "Training loss:1.524140477180481\n",
      "Training loss:1.5500394105911255\n",
      "Training loss:1.5447300672531128\n",
      "Training loss:1.5410345792770386\n",
      "Training loss:1.5694959163665771\n",
      "Training loss:1.541183352470398\n",
      "Training loss:1.5857175588607788\n",
      "Training loss:1.5280591249465942\n",
      "Training loss:1.4891585111618042\n",
      "Training loss:1.4914283752441406\n",
      "Training loss:1.595508098602295\n",
      "Training loss:1.5186113119125366\n",
      "Training loss:1.5156817436218262\n",
      "Training loss:1.5156030654907227\n",
      "Training loss:1.4965474605560303\n",
      "Training loss:1.5665308237075806\n",
      "Training loss:1.609710454940796\n",
      "Training loss:1.4707525968551636\n",
      "Training loss:1.5541962385177612\n",
      "Training loss:1.6272468566894531\n",
      "Training loss:1.557161808013916\n",
      "Training loss:1.5925565958023071\n",
      "Training loss:1.6620547771453857\n",
      "Training loss:1.5412713289260864\n",
      "Training loss:1.5734012126922607\n",
      "Training loss:1.5964394807815552\n",
      "Training loss:1.661594271659851\n",
      "Training loss:1.6577460765838623\n",
      "Training loss:1.566423773765564\n",
      "Training loss:1.6405121088027954\n",
      "Training loss:1.52183997631073\n",
      "Training loss:1.5072154998779297\n",
      "Training loss:1.5600876808166504\n",
      "Training loss:1.5819177627563477\n",
      "Training loss:1.551247000694275\n",
      "Training loss:1.5468536615371704\n",
      "Training loss:1.6096701622009277\n",
      "Training loss:1.5547386407852173\n",
      "Training loss:1.5122967958450317\n",
      "Training loss:1.6317850351333618\n",
      "Training loss:1.5383365154266357\n",
      "Training loss:1.6224168539047241\n",
      "Training loss:1.6435686349868774\n",
      "Training loss:1.5155391693115234\n",
      "Training loss:1.615860104560852\n",
      "Training loss:1.5782113075256348\n",
      "Epoch: 0119 loss_train: 145.886255 acc_train: 0.435917 loss_val: 18.313911 acc_val: 0.443500 time: 304339.991781s\n",
      "Training loss:1.5664715766906738\n",
      "Training loss:1.5287965536117554\n",
      "Training loss:1.4676238298416138\n",
      "Training loss:1.51128089427948\n",
      "Training loss:1.559765100479126\n",
      "Training loss:1.497700572013855\n",
      "Training loss:1.5520339012145996\n",
      "Training loss:1.5056077241897583\n",
      "Training loss:1.5568957328796387\n",
      "Training loss:1.5597259998321533\n",
      "Training loss:1.6607444286346436\n",
      "Training loss:1.5703610181808472\n",
      "Training loss:1.4833542108535767\n",
      "Training loss:1.5315295457839966\n",
      "Training loss:1.6321228742599487\n",
      "Training loss:1.5594587326049805\n",
      "Training loss:1.5184338092803955\n",
      "Training loss:1.5085937976837158\n",
      "Training loss:1.5424450635910034\n",
      "Training loss:1.6414779424667358\n",
      "Training loss:1.521661400794983\n",
      "Training loss:1.5774153470993042\n",
      "Training loss:1.5216472148895264\n",
      "Training loss:1.5210899114608765\n",
      "Training loss:1.5949345827102661\n",
      "Training loss:1.4651753902435303\n",
      "Training loss:1.5404986143112183\n",
      "Training loss:1.58424973487854\n",
      "Training loss:1.6090861558914185\n",
      "Training loss:1.541759967803955\n",
      "Training loss:1.5804896354675293\n",
      "Training loss:1.5353244543075562\n",
      "Training loss:1.5186792612075806\n",
      "Training loss:1.5996955633163452\n",
      "Training loss:1.4651801586151123\n",
      "Training loss:1.4818711280822754\n",
      "Training loss:1.5948039293289185\n",
      "Training loss:1.520755410194397\n",
      "Training loss:1.6161117553710938\n",
      "Training loss:1.6443965435028076\n",
      "Training loss:1.5762057304382324\n",
      "Training loss:1.5253251791000366\n",
      "Training loss:1.560856580734253\n",
      "Training loss:1.5141042470932007\n",
      "Training loss:1.478496789932251\n",
      "Training loss:1.5783100128173828\n",
      "Training loss:1.5965913534164429\n",
      "Training loss:1.523970365524292\n",
      "Training loss:1.5529930591583252\n",
      "Training loss:1.489547848701477\n",
      "Training loss:1.526392936706543\n",
      "Training loss:1.5214061737060547\n",
      "Training loss:1.5783337354660034\n",
      "Training loss:1.557112693786621\n",
      "Training loss:1.517171859741211\n",
      "Training loss:1.4212613105773926\n",
      "Training loss:1.5225542783737183\n",
      "Training loss:1.4880359172821045\n",
      "Training loss:1.5441855192184448\n",
      "Training loss:1.5398396253585815\n",
      "Training loss:1.471737265586853\n",
      "Training loss:1.542367935180664\n",
      "Training loss:1.5793124437332153\n",
      "Training loss:1.6643941402435303\n",
      "Training loss:1.598908543586731\n",
      "Training loss:1.5612998008728027\n",
      "Training loss:1.5588710308074951\n",
      "Training loss:1.5575278997421265\n",
      "Training loss:1.5323405265808105\n",
      "Training loss:1.5583875179290771\n",
      "Training loss:1.523014783859253\n",
      "Training loss:1.5108839273452759\n",
      "Training loss:1.541096568107605\n",
      "Training loss:1.5434235334396362\n",
      "Training loss:1.6031984090805054\n",
      "Training loss:1.5131289958953857\n",
      "Training loss:1.5148261785507202\n",
      "Training loss:1.5251520872116089\n",
      "Training loss:1.5167945623397827\n",
      "Training loss:1.5050556659698486\n",
      "Training loss:1.5966342687606812\n",
      "Training loss:1.5531318187713623\n",
      "Training loss:1.5218180418014526\n",
      "Training loss:1.465021014213562\n",
      "Training loss:1.5305933952331543\n",
      "Training loss:1.568993091583252\n",
      "Training loss:1.5200834274291992\n",
      "Training loss:1.503246545791626\n",
      "Training loss:1.513970136642456\n",
      "Training loss:1.5766520500183105\n",
      "Training loss:1.5543267726898193\n",
      "Training loss:1.5035648345947266\n",
      "Training loss:1.534820795059204\n",
      "Training loss:1.6505523920059204\n",
      "Epoch: 0120 loss_train: 145.117075 acc_train: 0.441063 loss_val: 18.445198 acc_val: 0.441000 time: 306792.831517s\n",
      "Training loss:1.5627073049545288\n",
      "Training loss:1.5641274452209473\n",
      "Training loss:1.573400616645813\n",
      "Training loss:1.5418903827667236\n",
      "Training loss:1.5007636547088623\n",
      "Training loss:1.5130990743637085\n",
      "Training loss:1.5128591060638428\n",
      "Training loss:1.5582692623138428\n",
      "Training loss:1.54193913936615\n",
      "Training loss:1.5140748023986816\n",
      "Training loss:1.5029388666152954\n",
      "Training loss:1.5391839742660522\n",
      "Training loss:1.4630277156829834\n",
      "Training loss:1.532048225402832\n",
      "Training loss:1.5578266382217407\n",
      "Training loss:1.563973069190979\n",
      "Training loss:1.5759129524230957\n",
      "Training loss:1.4991092681884766\n",
      "Training loss:1.6161859035491943\n",
      "Training loss:1.5784580707550049\n",
      "Training loss:1.5202162265777588\n",
      "Training loss:1.4744430780410767\n",
      "Training loss:1.6273210048675537\n",
      "Training loss:1.5565032958984375\n",
      "Training loss:1.4802420139312744\n",
      "Training loss:1.52449631690979\n",
      "Training loss:1.5325101613998413\n",
      "Training loss:1.5887342691421509\n",
      "Training loss:1.5618696212768555\n",
      "Training loss:1.5488773584365845\n",
      "Training loss:1.5179502964019775\n",
      "Training loss:1.561387300491333\n",
      "Training loss:1.6484159231185913\n",
      "Training loss:1.5160878896713257\n",
      "Training loss:1.602211356163025\n",
      "Training loss:1.5033684968948364\n",
      "Training loss:1.5122323036193848\n",
      "Training loss:1.5088849067687988\n",
      "Training loss:1.5985878705978394\n",
      "Training loss:1.5510125160217285\n",
      "Training loss:1.5821471214294434\n",
      "Training loss:1.4975063800811768\n",
      "Training loss:1.5316870212554932\n",
      "Training loss:1.547194480895996\n",
      "Training loss:1.5648598670959473\n",
      "Training loss:1.5402002334594727\n",
      "Training loss:1.527450442314148\n",
      "Training loss:1.4777246713638306\n",
      "Training loss:1.4871922731399536\n",
      "Training loss:1.5478878021240234\n",
      "Training loss:1.5174751281738281\n",
      "Training loss:1.5170655250549316\n",
      "Training loss:1.5826536417007446\n",
      "Training loss:1.5066602230072021\n",
      "Training loss:1.5886980295181274\n",
      "Training loss:1.5570164918899536\n",
      "Training loss:1.4458483457565308\n",
      "Training loss:1.5711512565612793\n",
      "Training loss:1.4988163709640503\n",
      "Training loss:1.6304857730865479\n",
      "Training loss:1.6519302129745483\n",
      "Training loss:1.5772641897201538\n",
      "Training loss:1.4802765846252441\n",
      "Training loss:1.4582390785217285\n",
      "Training loss:1.562184453010559\n",
      "Training loss:1.4878634214401245\n",
      "Training loss:1.6336281299591064\n",
      "Training loss:1.533083438873291\n",
      "Training loss:1.6683365106582642\n",
      "Training loss:1.638448715209961\n",
      "Training loss:1.5818676948547363\n",
      "Training loss:1.5345067977905273\n",
      "Training loss:1.4879642724990845\n",
      "Training loss:1.5857864618301392\n",
      "Training loss:1.6045801639556885\n",
      "Training loss:1.4754496812820435\n",
      "Training loss:1.5867995023727417\n",
      "Training loss:1.608550786972046\n",
      "Training loss:1.5480668544769287\n",
      "Training loss:1.5984447002410889\n",
      "Training loss:1.5272793769836426\n",
      "Training loss:1.6174970865249634\n",
      "Training loss:1.5058767795562744\n",
      "Training loss:1.4882627725601196\n",
      "Training loss:1.5361616611480713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5325716733932495\n",
      "Training loss:1.5419921875\n",
      "Training loss:1.4895124435424805\n",
      "Training loss:1.5498709678649902\n",
      "Training loss:1.586829423904419\n",
      "Training loss:1.5416679382324219\n",
      "Training loss:1.510412573814392\n",
      "Training loss:1.5800361633300781\n",
      "Training loss:1.4580727815628052\n",
      "Epoch: 0121 loss_train: 145.236184 acc_train: 0.438937 loss_val: 18.358956 acc_val: 0.440833 time: 309244.548002s\n",
      "Training loss:1.5230035781860352\n",
      "Training loss:1.5289850234985352\n",
      "Training loss:1.5832593441009521\n",
      "Training loss:1.5376161336898804\n",
      "Training loss:1.5113595724105835\n",
      "Training loss:1.5258294343948364\n",
      "Training loss:1.567095398902893\n",
      "Training loss:1.5488337278366089\n",
      "Training loss:1.5520776510238647\n",
      "Training loss:1.5732271671295166\n",
      "Training loss:1.4738894701004028\n",
      "Training loss:1.4926739931106567\n",
      "Training loss:1.5126608610153198\n",
      "Training loss:1.517749547958374\n",
      "Training loss:1.4780250787734985\n",
      "Training loss:1.5805054903030396\n",
      "Training loss:1.5746358633041382\n",
      "Training loss:1.5523332357406616\n",
      "Training loss:1.494165062904358\n",
      "Training loss:1.546540379524231\n",
      "Training loss:1.5463494062423706\n",
      "Training loss:1.5493625402450562\n",
      "Training loss:1.6082569360733032\n",
      "Training loss:1.5486888885498047\n",
      "Training loss:1.5493742227554321\n",
      "Training loss:1.606990933418274\n",
      "Training loss:1.5728768110275269\n",
      "Training loss:1.5384458303451538\n",
      "Training loss:1.5644043684005737\n",
      "Training loss:1.5363268852233887\n",
      "Training loss:1.5733240842819214\n",
      "Training loss:1.5591950416564941\n",
      "Training loss:1.5056136846542358\n",
      "Training loss:1.5283068418502808\n",
      "Training loss:1.4825420379638672\n",
      "Training loss:1.568673014640808\n",
      "Training loss:1.5361301898956299\n",
      "Training loss:1.5102099180221558\n",
      "Training loss:1.4897003173828125\n",
      "Training loss:1.5693414211273193\n",
      "Training loss:1.5899066925048828\n",
      "Training loss:1.4656038284301758\n",
      "Training loss:1.5939873456954956\n",
      "Training loss:1.5081738233566284\n",
      "Training loss:1.5249220132827759\n",
      "Training loss:1.5426437854766846\n",
      "Training loss:1.5005167722702026\n",
      "Training loss:1.5311037302017212\n",
      "Training loss:1.5367331504821777\n",
      "Training loss:1.5128840208053589\n",
      "Training loss:1.6025702953338623\n",
      "Training loss:1.5008565187454224\n",
      "Training loss:1.4633071422576904\n",
      "Training loss:1.503791093826294\n",
      "Training loss:1.5453970432281494\n",
      "Training loss:1.4910244941711426\n",
      "Training loss:1.5839639902114868\n",
      "Training loss:1.52764892578125\n",
      "Training loss:1.5642350912094116\n",
      "Training loss:1.5711694955825806\n",
      "Training loss:1.655849575996399\n",
      "Training loss:1.5633678436279297\n",
      "Training loss:1.569231629371643\n",
      "Training loss:1.6026129722595215\n",
      "Training loss:1.453659176826477\n",
      "Training loss:1.6101404428482056\n",
      "Training loss:1.500717282295227\n",
      "Training loss:1.56403648853302\n",
      "Training loss:1.547386884689331\n",
      "Training loss:1.5029181241989136\n",
      "Training loss:1.5751335620880127\n",
      "Training loss:1.528355598449707\n",
      "Training loss:1.5259425640106201\n",
      "Training loss:1.5306042432785034\n",
      "Training loss:1.5967869758605957\n",
      "Training loss:1.5620064735412598\n",
      "Training loss:1.5316665172576904\n",
      "Training loss:1.5649372339248657\n",
      "Training loss:1.5620852708816528\n",
      "Training loss:1.6423978805541992\n",
      "Training loss:1.6177148818969727\n",
      "Training loss:1.6970431804656982\n",
      "Training loss:1.6053401231765747\n",
      "Training loss:1.5470662117004395\n",
      "Training loss:1.583805799484253\n",
      "Training loss:1.5468944311141968\n",
      "Training loss:1.6379648447036743\n",
      "Training loss:1.5160691738128662\n",
      "Training loss:1.5371274948120117\n",
      "Training loss:1.5355260372161865\n",
      "Training loss:1.515198826789856\n",
      "Training loss:1.637287974357605\n",
      "Training loss:1.5988292694091797\n",
      "Training loss:1.6049550771713257\n",
      "Epoch: 0122 loss_train: 145.649679 acc_train: 0.437479 loss_val: 18.377774 acc_val: 0.437167 time: 311698.277768s\n",
      "Training loss:1.544534683227539\n",
      "Training loss:1.6505202054977417\n",
      "Training loss:1.588503122329712\n",
      "Training loss:1.534726619720459\n",
      "Training loss:1.5599321126937866\n",
      "Training loss:1.6083933115005493\n",
      "Training loss:1.6569682359695435\n",
      "Training loss:1.5027188062667847\n",
      "Training loss:1.5841681957244873\n",
      "Training loss:1.5819181203842163\n",
      "Training loss:1.5312947034835815\n",
      "Training loss:1.5736335515975952\n",
      "Training loss:1.5207971334457397\n",
      "Training loss:1.5642200708389282\n",
      "Training loss:1.5495648384094238\n",
      "Training loss:1.5418202877044678\n",
      "Training loss:1.6198029518127441\n",
      "Training loss:1.5725486278533936\n",
      "Training loss:1.6028227806091309\n",
      "Training loss:1.5187851190567017\n",
      "Training loss:1.591598391532898\n",
      "Training loss:1.628059983253479\n",
      "Training loss:1.613377571105957\n",
      "Training loss:1.587539553642273\n",
      "Training loss:1.5575007200241089\n",
      "Training loss:1.516310691833496\n",
      "Training loss:1.598557472229004\n",
      "Training loss:1.5551555156707764\n",
      "Training loss:1.4740394353866577\n",
      "Training loss:1.5305532217025757\n",
      "Training loss:1.5113319158554077\n",
      "Training loss:1.5609039068222046\n",
      "Training loss:1.6101258993148804\n",
      "Training loss:1.5122209787368774\n",
      "Training loss:1.476048469543457\n",
      "Training loss:1.5174084901809692\n",
      "Training loss:1.5998026132583618\n",
      "Training loss:1.5478219985961914\n",
      "Training loss:1.5207219123840332\n",
      "Training loss:1.5332837104797363\n",
      "Training loss:1.5100585222244263\n",
      "Training loss:1.4265778064727783\n",
      "Training loss:1.6131455898284912\n",
      "Training loss:1.4576767683029175\n",
      "Training loss:1.5361381769180298\n",
      "Training loss:1.531404972076416\n",
      "Training loss:1.53322172164917\n",
      "Training loss:1.5251939296722412\n",
      "Training loss:1.5586875677108765\n",
      "Training loss:1.552871823310852\n",
      "Training loss:1.5023524761199951\n",
      "Training loss:1.530387282371521\n",
      "Training loss:1.5517488718032837\n",
      "Training loss:1.4942084550857544\n",
      "Training loss:1.537902593612671\n",
      "Training loss:1.515197515487671\n",
      "Training loss:1.4927051067352295\n",
      "Training loss:1.4896718263626099\n",
      "Training loss:1.5664033889770508\n",
      "Training loss:1.524091124534607\n",
      "Training loss:1.5484541654586792\n",
      "Training loss:1.5463944673538208\n",
      "Training loss:1.5677120685577393\n",
      "Training loss:1.5747989416122437\n",
      "Training loss:1.555802822113037\n",
      "Training loss:1.5230494737625122\n",
      "Training loss:1.4899955987930298\n",
      "Training loss:1.6330057382583618\n",
      "Training loss:1.5005282163619995\n",
      "Training loss:1.5160609483718872\n",
      "Training loss:1.5128926038742065\n",
      "Training loss:1.562785267829895\n",
      "Training loss:1.5484259128570557\n",
      "Training loss:1.461012363433838\n",
      "Training loss:1.5793676376342773\n",
      "Training loss:1.6169281005859375\n",
      "Training loss:1.566681146621704\n",
      "Training loss:1.5822649002075195\n",
      "Training loss:1.429473638534546\n",
      "Training loss:1.5979506969451904\n",
      "Training loss:1.5157546997070312\n",
      "Training loss:1.5591039657592773\n",
      "Training loss:1.5134135484695435\n",
      "Training loss:1.5793437957763672\n",
      "Training loss:1.613924503326416\n",
      "Training loss:1.5306679010391235\n",
      "Training loss:1.4673588275909424\n",
      "Training loss:1.505708932876587\n",
      "Training loss:1.535119891166687\n",
      "Training loss:1.5504510402679443\n",
      "Training loss:1.5535112619400024\n",
      "Training loss:1.532923936843872\n",
      "Training loss:1.5368307828903198\n",
      "Training loss:1.4532442092895508\n",
      "Epoch: 0123 loss_train: 145.260623 acc_train: 0.437333 loss_val: 18.649471 acc_val: 0.424500 time: 314152.390173s\n",
      "Training loss:1.5802983045578003\n",
      "Training loss:1.5432556867599487\n",
      "Training loss:1.5324749946594238\n",
      "Training loss:1.532431721687317\n",
      "Training loss:1.5656205415725708\n",
      "Training loss:1.5266591310501099\n",
      "Training loss:1.5594635009765625\n",
      "Training loss:1.4244989156723022\n",
      "Training loss:1.5345135927200317\n",
      "Training loss:1.547849416732788\n",
      "Training loss:1.5877763032913208\n",
      "Training loss:1.5088415145874023\n",
      "Training loss:1.5077375173568726\n",
      "Training loss:1.5352472066879272\n",
      "Training loss:1.6089494228363037\n",
      "Training loss:1.5440529584884644\n",
      "Training loss:1.5812820196151733\n",
      "Training loss:1.617112159729004\n",
      "Training loss:1.5899734497070312\n",
      "Training loss:1.570231556892395\n",
      "Training loss:1.6015007495880127\n",
      "Training loss:1.5566673278808594\n",
      "Training loss:1.5477641820907593\n",
      "Training loss:1.5699623823165894\n",
      "Training loss:1.4925788640975952\n",
      "Training loss:1.610571265220642\n",
      "Training loss:1.5330710411071777\n",
      "Training loss:1.5248117446899414\n",
      "Training loss:1.5790953636169434\n",
      "Training loss:1.5859289169311523\n",
      "Training loss:1.5080534219741821\n",
      "Training loss:1.5289113521575928\n",
      "Training loss:1.56630539894104\n",
      "Training loss:1.5543216466903687\n",
      "Training loss:1.4514973163604736\n",
      "Training loss:1.520803451538086\n",
      "Training loss:1.5471926927566528\n",
      "Training loss:1.574901819229126\n",
      "Training loss:1.493369221687317\n",
      "Training loss:1.4826511144638062\n",
      "Training loss:1.547370195388794\n",
      "Training loss:1.4391416311264038\n",
      "Training loss:1.5762522220611572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5388553142547607\n",
      "Training loss:1.5206217765808105\n",
      "Training loss:1.545880675315857\n",
      "Training loss:1.6092818975448608\n",
      "Training loss:1.4473003149032593\n",
      "Training loss:1.5206204652786255\n",
      "Training loss:1.5964864492416382\n",
      "Training loss:1.5400843620300293\n",
      "Training loss:1.5864367485046387\n",
      "Training loss:1.510249137878418\n",
      "Training loss:1.5553449392318726\n",
      "Training loss:1.5946123600006104\n",
      "Training loss:1.5105444192886353\n",
      "Training loss:1.5297460556030273\n",
      "Training loss:1.4785131216049194\n",
      "Training loss:1.577375888824463\n",
      "Training loss:1.5428584814071655\n",
      "Training loss:1.4926669597625732\n",
      "Training loss:1.5071431398391724\n",
      "Training loss:1.5682240724563599\n",
      "Training loss:1.5502064228057861\n",
      "Training loss:1.5267566442489624\n",
      "Training loss:1.577252984046936\n",
      "Training loss:1.572911024093628\n",
      "Training loss:1.580539584159851\n",
      "Training loss:1.5434240102767944\n",
      "Training loss:1.534712791442871\n",
      "Training loss:1.506678581237793\n",
      "Training loss:1.5067256689071655\n",
      "Training loss:1.5289642810821533\n",
      "Training loss:1.541683316230774\n",
      "Training loss:1.5294115543365479\n",
      "Training loss:1.5232514142990112\n",
      "Training loss:1.515621304512024\n",
      "Training loss:1.6035114526748657\n",
      "Training loss:1.5172104835510254\n",
      "Training loss:1.5130105018615723\n",
      "Training loss:1.5433073043823242\n",
      "Training loss:1.5467993021011353\n",
      "Training loss:1.5909757614135742\n",
      "Training loss:1.4858945608139038\n",
      "Training loss:1.5192197561264038\n",
      "Training loss:1.5614231824874878\n",
      "Training loss:1.5694485902786255\n",
      "Training loss:1.5430635213851929\n",
      "Training loss:1.6027305126190186\n",
      "Training loss:1.5140657424926758\n",
      "Training loss:1.56243097782135\n",
      "Training loss:1.521001935005188\n",
      "Training loss:1.5133056640625\n",
      "Training loss:1.4789676666259766\n",
      "Epoch: 0124 loss_train: 144.888346 acc_train: 0.441479 loss_val: 18.375728 acc_val: 0.443500 time: 316620.786876s\n",
      "Training loss:1.5251438617706299\n",
      "Training loss:1.5345537662506104\n",
      "Training loss:1.5309284925460815\n",
      "Training loss:1.5020086765289307\n",
      "Training loss:1.5686743259429932\n",
      "Training loss:1.549551010131836\n",
      "Training loss:1.555582880973816\n",
      "Training loss:1.5291767120361328\n",
      "Training loss:1.576026439666748\n",
      "Training loss:1.5478086471557617\n",
      "Training loss:1.4908736944198608\n",
      "Training loss:1.5203876495361328\n",
      "Training loss:1.605314016342163\n",
      "Training loss:1.6057114601135254\n",
      "Training loss:1.526629090309143\n",
      "Training loss:1.4938586950302124\n",
      "Training loss:1.566002607345581\n",
      "Training loss:1.5699039697647095\n",
      "Training loss:1.6023439168930054\n",
      "Training loss:1.5263677835464478\n",
      "Training loss:1.57992422580719\n",
      "Training loss:1.5485049486160278\n",
      "Training loss:1.4918264150619507\n",
      "Training loss:1.5561182498931885\n",
      "Training loss:1.5175877809524536\n",
      "Training loss:1.5437734127044678\n",
      "Training loss:1.532753586769104\n",
      "Training loss:1.5489369630813599\n",
      "Training loss:1.564278244972229\n",
      "Training loss:1.504894733428955\n",
      "Training loss:1.5211491584777832\n",
      "Training loss:1.6618058681488037\n",
      "Training loss:1.5554461479187012\n",
      "Training loss:1.5997605323791504\n",
      "Training loss:1.542561650276184\n",
      "Training loss:1.5657674074172974\n",
      "Training loss:1.5370501279830933\n",
      "Training loss:1.6010187864303589\n",
      "Training loss:1.5381343364715576\n",
      "Training loss:1.5599555969238281\n",
      "Training loss:1.5235530138015747\n",
      "Training loss:1.509052038192749\n",
      "Training loss:1.6163686513900757\n",
      "Training loss:1.5783072710037231\n",
      "Training loss:1.5571562051773071\n",
      "Training loss:1.5692224502563477\n",
      "Training loss:1.5152347087860107\n",
      "Training loss:1.5214598178863525\n",
      "Training loss:1.5534700155258179\n",
      "Training loss:1.5280067920684814\n",
      "Training loss:1.5626230239868164\n",
      "Training loss:1.5055288076400757\n",
      "Training loss:1.5341777801513672\n",
      "Training loss:1.5268325805664062\n",
      "Training loss:1.5900017023086548\n",
      "Training loss:1.5565204620361328\n",
      "Training loss:1.5793377161026\n",
      "Training loss:1.4983206987380981\n",
      "Training loss:1.5109776258468628\n",
      "Training loss:1.529160737991333\n",
      "Training loss:1.515724539756775\n",
      "Training loss:1.561942219734192\n",
      "Training loss:1.5100181102752686\n",
      "Training loss:1.5519094467163086\n",
      "Training loss:1.4710304737091064\n",
      "Training loss:1.462711215019226\n",
      "Training loss:1.5632034540176392\n",
      "Training loss:1.550431489944458\n",
      "Training loss:1.469796895980835\n",
      "Training loss:1.5053285360336304\n",
      "Training loss:1.4807310104370117\n",
      "Training loss:1.5167938470840454\n",
      "Training loss:1.6078203916549683\n",
      "Training loss:1.546220064163208\n",
      "Training loss:1.6083468198776245\n",
      "Training loss:1.5020033121109009\n",
      "Training loss:1.511899471282959\n",
      "Training loss:1.5396755933761597\n",
      "Training loss:1.4515526294708252\n",
      "Training loss:1.5408644676208496\n",
      "Training loss:1.5617247819900513\n",
      "Training loss:1.5499893426895142\n",
      "Training loss:1.5243512392044067\n",
      "Training loss:1.5199675559997559\n",
      "Training loss:1.5138354301452637\n",
      "Training loss:1.5189884901046753\n",
      "Training loss:1.5452282428741455\n",
      "Training loss:1.5318983793258667\n",
      "Training loss:1.497450590133667\n",
      "Training loss:1.4636099338531494\n",
      "Training loss:1.492548942565918\n",
      "Training loss:1.5413084030151367\n",
      "Training loss:1.4793553352355957\n",
      "Training loss:1.624097228050232\n",
      "Epoch: 0125 loss_train: 144.695766 acc_train: 0.440646 loss_val: 18.294785 acc_val: 0.446000 time: 319085.929281s\n",
      "Training loss:1.5916080474853516\n",
      "Training loss:1.497746229171753\n",
      "Training loss:1.522817611694336\n",
      "Training loss:1.4363654851913452\n",
      "Training loss:1.5053582191467285\n",
      "Training loss:1.5127805471420288\n",
      "Training loss:1.5123714208602905\n",
      "Training loss:1.4802509546279907\n",
      "Training loss:1.48752760887146\n",
      "Training loss:1.41962730884552\n",
      "Training loss:1.5002703666687012\n",
      "Training loss:1.5772193670272827\n",
      "Training loss:1.5877273082733154\n",
      "Training loss:1.5350308418273926\n",
      "Training loss:1.5185341835021973\n",
      "Training loss:1.559189796447754\n",
      "Training loss:1.5515884160995483\n",
      "Training loss:1.472095012664795\n",
      "Training loss:1.6352405548095703\n",
      "Training loss:1.4958722591400146\n",
      "Training loss:1.605940341949463\n",
      "Training loss:1.5689374208450317\n",
      "Training loss:1.5167497396469116\n",
      "Training loss:1.6072909832000732\n",
      "Training loss:1.5899100303649902\n",
      "Training loss:1.6380749940872192\n",
      "Training loss:1.5422919988632202\n",
      "Training loss:1.4758914709091187\n",
      "Training loss:1.6160348653793335\n",
      "Training loss:1.5581331253051758\n",
      "Training loss:1.5239731073379517\n",
      "Training loss:1.637689232826233\n",
      "Training loss:1.6159842014312744\n",
      "Training loss:1.573032021522522\n",
      "Training loss:1.4490330219268799\n",
      "Training loss:1.5395253896713257\n",
      "Training loss:1.5856273174285889\n",
      "Training loss:1.4873446226119995\n",
      "Training loss:1.5097898244857788\n",
      "Training loss:1.48910653591156\n",
      "Training loss:1.5328999757766724\n",
      "Training loss:1.497292160987854\n",
      "Training loss:1.5730265378952026\n",
      "Training loss:1.5578705072402954\n",
      "Training loss:1.5377882719039917\n",
      "Training loss:1.5566587448120117\n",
      "Training loss:1.545193076133728\n",
      "Training loss:1.5213208198547363\n",
      "Training loss:1.5594263076782227\n",
      "Training loss:1.54248046875\n",
      "Training loss:1.6135505437850952\n",
      "Training loss:1.5138323307037354\n",
      "Training loss:1.5279971361160278\n",
      "Training loss:1.5415325164794922\n",
      "Training loss:1.495336651802063\n",
      "Training loss:1.4779077768325806\n",
      "Training loss:1.5501614809036255\n",
      "Training loss:1.5382248163223267\n",
      "Training loss:1.5662323236465454\n",
      "Training loss:1.5804115533828735\n",
      "Training loss:1.5436573028564453\n",
      "Training loss:1.6391031742095947\n",
      "Training loss:1.544102430343628\n",
      "Training loss:1.46610426902771\n",
      "Training loss:1.574000358581543\n",
      "Training loss:1.5176652669906616\n",
      "Training loss:1.5654544830322266\n",
      "Training loss:1.5167591571807861\n",
      "Training loss:1.5198032855987549\n",
      "Training loss:1.595930814743042\n",
      "Training loss:1.5202387571334839\n",
      "Training loss:1.4652920961380005\n",
      "Training loss:1.4924452304840088\n",
      "Training loss:1.5700199604034424\n",
      "Training loss:1.525422215461731\n",
      "Training loss:1.5202088356018066\n",
      "Training loss:1.51813805103302\n",
      "Training loss:1.5733126401901245\n",
      "Training loss:1.5387152433395386\n",
      "Training loss:1.5976455211639404\n",
      "Training loss:1.4965239763259888\n",
      "Training loss:1.6363306045532227\n",
      "Training loss:1.5269484519958496\n",
      "Training loss:1.518973708152771\n",
      "Training loss:1.60396409034729\n",
      "Training loss:1.5382853746414185\n",
      "Training loss:1.5550258159637451\n",
      "Training loss:1.5081787109375\n",
      "Training loss:1.4829204082489014\n",
      "Training loss:1.6364489793777466\n",
      "Training loss:1.5085034370422363\n",
      "Training loss:1.5245237350463867\n",
      "Training loss:1.5976529121398926\n",
      "Training loss:1.5317450761795044\n",
      "Epoch: 0126 loss_train: 144.828770 acc_train: 0.439979 loss_val: 18.578591 acc_val: 0.438000 time: 321549.268189s\n",
      "Training loss:1.5806353092193604\n",
      "Training loss:1.5563538074493408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5664464235305786\n",
      "Training loss:1.5331705808639526\n",
      "Training loss:1.5010987520217896\n",
      "Training loss:1.5208871364593506\n",
      "Training loss:1.5476683378219604\n",
      "Training loss:1.4601004123687744\n",
      "Training loss:1.4855271577835083\n",
      "Training loss:1.5154547691345215\n",
      "Training loss:1.6195486783981323\n",
      "Training loss:1.556352138519287\n",
      "Training loss:1.5429085493087769\n",
      "Training loss:1.5239167213439941\n",
      "Training loss:1.5729361772537231\n",
      "Training loss:1.64873206615448\n",
      "Training loss:1.5163769721984863\n",
      "Training loss:1.559554934501648\n",
      "Training loss:1.4686129093170166\n",
      "Training loss:1.501381754875183\n",
      "Training loss:1.5542080402374268\n",
      "Training loss:1.52546226978302\n",
      "Training loss:1.5212905406951904\n",
      "Training loss:1.4898226261138916\n",
      "Training loss:1.6084506511688232\n",
      "Training loss:1.576302170753479\n",
      "Training loss:1.4658867120742798\n",
      "Training loss:1.6516073942184448\n",
      "Training loss:1.5181320905685425\n",
      "Training loss:1.5915218591690063\n",
      "Training loss:1.523309350013733\n",
      "Training loss:1.655341386795044\n",
      "Training loss:1.4807482957839966\n",
      "Training loss:1.5554604530334473\n",
      "Training loss:1.5440012216567993\n",
      "Training loss:1.4680784940719604\n",
      "Training loss:1.5928845405578613\n",
      "Training loss:1.5198732614517212\n",
      "Training loss:1.5470634698867798\n",
      "Training loss:1.4743289947509766\n",
      "Training loss:1.484087586402893\n",
      "Training loss:1.541658639907837\n",
      "Training loss:1.5819021463394165\n",
      "Training loss:1.5384315252304077\n",
      "Training loss:1.507681965827942\n",
      "Training loss:1.5272284746170044\n",
      "Training loss:1.4768365621566772\n",
      "Training loss:1.5500249862670898\n",
      "Training loss:1.508660078048706\n",
      "Training loss:1.5536433458328247\n",
      "Training loss:1.480437159538269\n",
      "Training loss:1.4931002855300903\n",
      "Training loss:1.6248079538345337\n",
      "Training loss:1.6321687698364258\n",
      "Training loss:1.5634702444076538\n",
      "Training loss:1.5264261960983276\n",
      "Training loss:1.5683332681655884\n",
      "Training loss:1.5220208168029785\n",
      "Training loss:1.540333867073059\n",
      "Training loss:1.5471155643463135\n",
      "Training loss:1.4795279502868652\n",
      "Training loss:1.5645164251327515\n",
      "Training loss:1.5696228742599487\n",
      "Training loss:1.5074101686477661\n",
      "Training loss:1.5269352197647095\n",
      "Training loss:1.5504846572875977\n",
      "Training loss:1.5168582201004028\n",
      "Training loss:1.4787400960922241\n",
      "Training loss:1.5881377458572388\n",
      "Training loss:1.466707468032837\n",
      "Training loss:1.5487087965011597\n",
      "Training loss:1.5169024467468262\n",
      "Training loss:1.5681811571121216\n",
      "Training loss:1.4976706504821777\n",
      "Training loss:1.6240336894989014\n",
      "Training loss:1.4904760122299194\n",
      "Training loss:1.5291317701339722\n",
      "Training loss:1.588924765586853\n",
      "Training loss:1.4923944473266602\n",
      "Training loss:1.4864327907562256\n",
      "Training loss:1.5240875482559204\n",
      "Training loss:1.5502296686172485\n",
      "Training loss:1.5903711318969727\n",
      "Training loss:1.5219498872756958\n",
      "Training loss:1.5149635076522827\n",
      "Training loss:1.5756654739379883\n",
      "Training loss:1.4725326299667358\n",
      "Training loss:1.5557663440704346\n",
      "Training loss:1.5361231565475464\n",
      "Training loss:1.515328288078308\n",
      "Training loss:1.5150800943374634\n",
      "Training loss:1.4797677993774414\n",
      "Training loss:1.495274543762207\n",
      "Training loss:1.5033339262008667\n",
      "Epoch: 0127 loss_train: 144.352078 acc_train: 0.444229 loss_val: 18.243505 acc_val: 0.444333 time: 324011.729217s\n",
      "Training loss:1.5609068870544434\n",
      "Training loss:1.524618148803711\n",
      "Training loss:1.4970002174377441\n",
      "Training loss:1.4708898067474365\n",
      "Training loss:1.5672045946121216\n",
      "Training loss:1.5093477964401245\n",
      "Training loss:1.5671392679214478\n",
      "Training loss:1.5448392629623413\n",
      "Training loss:1.5243381261825562\n",
      "Training loss:1.5136491060256958\n",
      "Training loss:1.5231213569641113\n",
      "Training loss:1.5294008255004883\n",
      "Training loss:1.5774720907211304\n",
      "Training loss:1.6236817836761475\n",
      "Training loss:1.5471302270889282\n",
      "Training loss:1.5787901878356934\n",
      "Training loss:1.532964825630188\n",
      "Training loss:1.4794139862060547\n",
      "Training loss:1.551103115081787\n",
      "Training loss:1.551076889038086\n",
      "Training loss:1.6309860944747925\n",
      "Training loss:1.4758048057556152\n",
      "Training loss:1.5574568510055542\n",
      "Training loss:1.5908702611923218\n",
      "Training loss:1.544516921043396\n",
      "Training loss:1.4993176460266113\n",
      "Training loss:1.5783101320266724\n",
      "Training loss:1.5349974632263184\n",
      "Training loss:1.519709587097168\n",
      "Training loss:1.5250002145767212\n",
      "Training loss:1.5639492273330688\n",
      "Training loss:1.6071491241455078\n",
      "Training loss:1.5362051725387573\n",
      "Training loss:1.5266543626785278\n",
      "Training loss:1.5470659732818604\n",
      "Training loss:1.5790278911590576\n",
      "Training loss:1.5205096006393433\n",
      "Training loss:1.4501326084136963\n",
      "Training loss:1.5001115798950195\n",
      "Training loss:1.5194621086120605\n",
      "Training loss:1.4858204126358032\n",
      "Training loss:1.4344370365142822\n",
      "Training loss:1.4871906042099\n",
      "Training loss:1.5045455694198608\n",
      "Training loss:1.5089887380599976\n",
      "Training loss:1.5338066816329956\n",
      "Training loss:1.5475575923919678\n",
      "Training loss:1.7008817195892334\n",
      "Training loss:1.4665703773498535\n",
      "Training loss:1.4826183319091797\n",
      "Training loss:1.5401815176010132\n",
      "Training loss:1.4587223529815674\n",
      "Training loss:1.558285117149353\n",
      "Training loss:1.5342233180999756\n",
      "Training loss:1.5500322580337524\n",
      "Training loss:1.534137487411499\n",
      "Training loss:1.475635290145874\n",
      "Training loss:1.609527349472046\n",
      "Training loss:1.5514898300170898\n",
      "Training loss:1.56235671043396\n",
      "Training loss:1.5086919069290161\n",
      "Training loss:1.5131006240844727\n",
      "Training loss:1.5427883863449097\n",
      "Training loss:1.5427695512771606\n",
      "Training loss:1.5723650455474854\n",
      "Training loss:1.5370205640792847\n",
      "Training loss:1.5308953523635864\n",
      "Training loss:1.6112345457077026\n",
      "Training loss:1.492932915687561\n",
      "Training loss:1.4959603548049927\n",
      "Training loss:1.567872405052185\n",
      "Training loss:1.5336873531341553\n",
      "Training loss:1.6841967105865479\n",
      "Training loss:1.4886759519577026\n",
      "Training loss:1.5885416269302368\n",
      "Training loss:1.5033890008926392\n",
      "Training loss:1.5035046339035034\n",
      "Training loss:1.6057056188583374\n",
      "Training loss:1.4663197994232178\n",
      "Training loss:1.5281010866165161\n",
      "Training loss:1.4536662101745605\n",
      "Training loss:1.5868492126464844\n",
      "Training loss:1.5419912338256836\n",
      "Training loss:1.5711731910705566\n",
      "Training loss:1.495455265045166\n",
      "Training loss:1.5298757553100586\n",
      "Training loss:1.538503885269165\n",
      "Training loss:1.5239180326461792\n",
      "Training loss:1.5304375886917114\n",
      "Training loss:1.588508129119873\n",
      "Training loss:1.5702877044677734\n",
      "Training loss:1.4911893606185913\n",
      "Training loss:1.5123004913330078\n",
      "Training loss:1.5294551849365234\n",
      "Epoch: 0128 loss_train: 144.389699 acc_train: 0.442312 loss_val: 18.619756 acc_val: 0.432167 time: 326478.118164s\n",
      "Training loss:1.656908392906189\n",
      "Training loss:1.5553381443023682\n",
      "Training loss:1.5671069622039795\n",
      "Training loss:1.5504742860794067\n",
      "Training loss:1.5123528242111206\n",
      "Training loss:1.4991371631622314\n",
      "Training loss:1.5242642164230347\n",
      "Training loss:1.4230098724365234\n",
      "Training loss:1.5848469734191895\n",
      "Training loss:1.5618371963500977\n",
      "Training loss:1.5718588829040527\n",
      "Training loss:1.5899152755737305\n",
      "Training loss:1.4688056707382202\n",
      "Training loss:1.564408302307129\n",
      "Training loss:1.4971572160720825\n",
      "Training loss:1.5521326065063477\n",
      "Training loss:1.5769457817077637\n",
      "Training loss:1.564801573753357\n",
      "Training loss:1.5488919019699097\n",
      "Training loss:1.5501258373260498\n",
      "Training loss:1.6088483333587646\n",
      "Training loss:1.5147854089736938\n",
      "Training loss:1.476472020149231\n",
      "Training loss:1.6886671781539917\n",
      "Training loss:1.4898478984832764\n",
      "Training loss:1.5840659141540527\n",
      "Training loss:1.5123378038406372\n",
      "Training loss:1.6123979091644287\n",
      "Training loss:1.5083091259002686\n",
      "Training loss:1.4864124059677124\n",
      "Training loss:1.4783251285552979\n",
      "Training loss:1.537469506263733\n",
      "Training loss:1.5699001550674438\n",
      "Training loss:1.5036462545394897\n",
      "Training loss:1.521762490272522\n",
      "Training loss:1.465407371520996\n",
      "Training loss:1.5484853982925415\n",
      "Training loss:1.5208463668823242\n",
      "Training loss:1.471762776374817\n",
      "Training loss:1.4761890172958374\n",
      "Training loss:1.5656079053878784\n",
      "Training loss:1.4814482927322388\n",
      "Training loss:1.5332332849502563\n",
      "Training loss:1.5814447402954102\n",
      "Training loss:1.4674900770187378\n",
      "Training loss:1.529020071029663\n",
      "Training loss:1.6055233478546143\n",
      "Training loss:1.6157728433609009\n",
      "Training loss:1.496057391166687\n",
      "Training loss:1.5685724020004272\n",
      "Training loss:1.5344347953796387\n",
      "Training loss:1.5697132349014282\n",
      "Training loss:1.4876121282577515\n",
      "Training loss:1.6337248086929321\n",
      "Training loss:1.5566084384918213\n",
      "Training loss:1.5439718961715698\n",
      "Training loss:1.5401952266693115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.544228196144104\n",
      "Training loss:1.5134772062301636\n",
      "Training loss:1.5204014778137207\n",
      "Training loss:1.5309741497039795\n",
      "Training loss:1.5832122564315796\n",
      "Training loss:1.6024645566940308\n",
      "Training loss:1.583395004272461\n",
      "Training loss:1.5309334993362427\n",
      "Training loss:1.5692169666290283\n",
      "Training loss:1.5548219680786133\n",
      "Training loss:1.6219793558120728\n",
      "Training loss:1.5912171602249146\n",
      "Training loss:1.5303529500961304\n",
      "Training loss:1.5423197746276855\n",
      "Training loss:1.5552054643630981\n",
      "Training loss:1.5489685535430908\n",
      "Training loss:1.5447092056274414\n",
      "Training loss:1.5800074338912964\n",
      "Training loss:1.6253365278244019\n",
      "Training loss:1.484621524810791\n",
      "Training loss:1.4629253149032593\n",
      "Training loss:1.5349348783493042\n",
      "Training loss:1.567948818206787\n",
      "Training loss:1.520211935043335\n",
      "Training loss:1.4824951887130737\n",
      "Training loss:1.5144704580307007\n",
      "Training loss:1.5578433275222778\n",
      "Training loss:1.5547314882278442\n",
      "Training loss:1.5858553647994995\n",
      "Training loss:1.5433859825134277\n",
      "Training loss:1.4869734048843384\n",
      "Training loss:1.5592340230941772\n",
      "Training loss:1.483214259147644\n",
      "Training loss:1.553680181503296\n",
      "Training loss:1.6302294731140137\n",
      "Training loss:1.473339319229126\n",
      "Training loss:1.596879482269287\n",
      "Epoch: 0129 loss_train: 145.072885 acc_train: 0.439604 loss_val: 18.449566 acc_val: 0.441000 time: 329070.619807s\n",
      "Training loss:1.5464991331100464\n",
      "Training loss:1.574539065361023\n",
      "Training loss:1.5938849449157715\n",
      "Training loss:1.5446439981460571\n",
      "Training loss:1.5531774759292603\n",
      "Training loss:1.5308085680007935\n",
      "Training loss:1.512263298034668\n",
      "Training loss:1.590112566947937\n",
      "Training loss:1.5360448360443115\n",
      "Training loss:1.510525107383728\n",
      "Training loss:1.5427240133285522\n",
      "Training loss:1.5580894947052002\n",
      "Training loss:1.5731704235076904\n",
      "Training loss:1.5138040781021118\n",
      "Training loss:1.5200369358062744\n",
      "Training loss:1.5031068325042725\n",
      "Training loss:1.6288561820983887\n",
      "Training loss:1.5401591062545776\n",
      "Training loss:1.5091218948364258\n",
      "Training loss:1.5545225143432617\n",
      "Training loss:1.4933691024780273\n",
      "Training loss:1.5408856868743896\n",
      "Training loss:1.569554090499878\n",
      "Training loss:1.5465803146362305\n",
      "Training loss:1.5930531024932861\n",
      "Training loss:1.5494259595870972\n",
      "Training loss:1.5117958784103394\n",
      "Training loss:1.4484870433807373\n",
      "Training loss:1.600746512413025\n",
      "Training loss:1.5010029077529907\n",
      "Training loss:1.5822621583938599\n",
      "Training loss:1.5924022197723389\n",
      "Training loss:1.6133317947387695\n",
      "Training loss:1.5572236776351929\n",
      "Training loss:1.554345726966858\n",
      "Training loss:1.4969229698181152\n",
      "Training loss:1.5043017864227295\n",
      "Training loss:1.5695772171020508\n",
      "Training loss:1.4634391069412231\n",
      "Training loss:1.4933671951293945\n",
      "Training loss:1.5155441761016846\n",
      "Training loss:1.51625394821167\n",
      "Training loss:1.5117069482803345\n",
      "Training loss:1.5575830936431885\n",
      "Training loss:1.4562253952026367\n",
      "Training loss:1.5542731285095215\n",
      "Training loss:1.5323090553283691\n",
      "Training loss:1.5458638668060303\n",
      "Training loss:1.524122714996338\n",
      "Training loss:1.5943189859390259\n",
      "Training loss:1.5757532119750977\n",
      "Training loss:1.5309823751449585\n",
      "Training loss:1.5444202423095703\n",
      "Training loss:1.5060091018676758\n",
      "Training loss:1.503200650215149\n",
      "Training loss:1.5479177236557007\n",
      "Training loss:1.4883300065994263\n",
      "Training loss:1.5231921672821045\n",
      "Training loss:1.6594678163528442\n",
      "Training loss:1.6123449802398682\n",
      "Training loss:1.535032868385315\n",
      "Training loss:1.5070133209228516\n",
      "Training loss:1.5219802856445312\n",
      "Training loss:1.5969231128692627\n",
      "Training loss:1.517633080482483\n",
      "Training loss:1.5509848594665527\n",
      "Training loss:1.5950429439544678\n",
      "Training loss:1.5395182371139526\n",
      "Training loss:1.5991191864013672\n",
      "Training loss:1.4746140241622925\n",
      "Training loss:1.5742673873901367\n",
      "Training loss:1.5244276523590088\n",
      "Training loss:1.4090532064437866\n",
      "Training loss:1.4872784614562988\n",
      "Training loss:1.536620855331421\n",
      "Training loss:1.4715948104858398\n",
      "Training loss:1.5222499370574951\n",
      "Training loss:1.5840686559677124\n",
      "Training loss:1.5409491062164307\n",
      "Training loss:1.5163209438323975\n",
      "Training loss:1.5330873727798462\n",
      "Training loss:1.4793914556503296\n",
      "Training loss:1.545911192893982\n",
      "Training loss:1.5081108808517456\n",
      "Training loss:1.6203317642211914\n",
      "Training loss:1.506115436553955\n",
      "Training loss:1.4677518606185913\n",
      "Training loss:1.5700455904006958\n",
      "Training loss:1.572065830230713\n",
      "Training loss:1.5070643424987793\n",
      "Training loss:1.5167179107666016\n",
      "Training loss:1.586337685585022\n",
      "Training loss:1.4959192276000977\n",
      "Training loss:1.5273585319519043\n",
      "Epoch: 0130 loss_train: 144.558887 acc_train: 0.442750 loss_val: 18.213097 acc_val: 0.446000 time: 331580.130098s\n",
      "Training loss:1.4755922555923462\n",
      "Training loss:1.5394734144210815\n",
      "Training loss:1.459251046180725\n",
      "Training loss:1.4898202419281006\n",
      "Training loss:1.5700470209121704\n",
      "Training loss:1.5074516534805298\n",
      "Training loss:1.4961742162704468\n",
      "Training loss:1.5502406358718872\n",
      "Training loss:1.6493936777114868\n",
      "Training loss:1.5289616584777832\n",
      "Training loss:1.5812841653823853\n",
      "Training loss:1.5549817085266113\n",
      "Training loss:1.5915517807006836\n",
      "Training loss:1.5247901678085327\n",
      "Training loss:1.5800426006317139\n",
      "Training loss:1.540797233581543\n",
      "Training loss:1.50566828250885\n",
      "Training loss:1.5567994117736816\n",
      "Training loss:1.5917917490005493\n",
      "Training loss:1.4473958015441895\n",
      "Training loss:1.538141131401062\n",
      "Training loss:1.5447674989700317\n",
      "Training loss:1.5614354610443115\n",
      "Training loss:1.513654351234436\n",
      "Training loss:1.458090901374817\n",
      "Training loss:1.4875385761260986\n",
      "Training loss:1.4851611852645874\n",
      "Training loss:1.487959623336792\n",
      "Training loss:1.572228193283081\n",
      "Training loss:1.5899075269699097\n",
      "Training loss:1.5251015424728394\n",
      "Training loss:1.5574615001678467\n",
      "Training loss:1.4948915243148804\n",
      "Training loss:1.4772439002990723\n",
      "Training loss:1.534780740737915\n",
      "Training loss:1.496802806854248\n",
      "Training loss:1.5896663665771484\n",
      "Training loss:1.4454180002212524\n",
      "Training loss:1.5367329120635986\n",
      "Training loss:1.516798973083496\n",
      "Training loss:1.5815632343292236\n",
      "Training loss:1.466206431388855\n",
      "Training loss:1.6100687980651855\n",
      "Training loss:1.6082533597946167\n",
      "Training loss:1.4969592094421387\n",
      "Training loss:1.5505586862564087\n",
      "Training loss:1.5818986892700195\n",
      "Training loss:1.4992344379425049\n",
      "Training loss:1.4966257810592651\n",
      "Training loss:1.5118250846862793\n",
      "Training loss:1.506554365158081\n",
      "Training loss:1.5282707214355469\n",
      "Training loss:1.51975417137146\n",
      "Training loss:1.49051034450531\n",
      "Training loss:1.5429272651672363\n",
      "Training loss:1.539512276649475\n",
      "Training loss:1.4841697216033936\n",
      "Training loss:1.5334854125976562\n",
      "Training loss:1.491970181465149\n",
      "Training loss:1.5977706909179688\n",
      "Training loss:1.5549250841140747\n",
      "Training loss:1.5997923612594604\n",
      "Training loss:1.4976251125335693\n",
      "Training loss:1.5986578464508057\n",
      "Training loss:1.4132568836212158\n",
      "Training loss:1.5008643865585327\n",
      "Training loss:1.5541694164276123\n",
      "Training loss:1.6209605932235718\n",
      "Training loss:1.4670636653900146\n",
      "Training loss:1.5607054233551025\n",
      "Training loss:1.5084360837936401\n",
      "Training loss:1.5565835237503052\n",
      "Training loss:1.493876338005066\n",
      "Training loss:1.4932091236114502\n",
      "Training loss:1.5300865173339844\n",
      "Training loss:1.5466443300247192\n",
      "Training loss:1.5394582748413086\n",
      "Training loss:1.477177619934082\n",
      "Training loss:1.522119164466858\n",
      "Training loss:1.591304063796997\n",
      "Training loss:1.5237152576446533\n",
      "Training loss:1.5017614364624023\n",
      "Training loss:1.460292935371399\n",
      "Training loss:1.534586787223816\n",
      "Training loss:1.5180776119232178\n",
      "Training loss:1.5519453287124634\n",
      "Training loss:1.4454604387283325\n",
      "Training loss:1.5317022800445557\n",
      "Training loss:1.5870014429092407\n",
      "Training loss:1.4999287128448486\n",
      "Training loss:1.5451149940490723\n",
      "Training loss:1.5221961736679077\n",
      "Training loss:1.5625505447387695\n",
      "Training loss:1.5055804252624512\n",
      "Epoch: 0131 loss_train: 143.710240 acc_train: 0.444646 loss_val: 18.140416 acc_val: 0.451500 time: 334054.785954s\n",
      "Training loss:1.5101606845855713\n",
      "Training loss:1.4996509552001953\n",
      "Training loss:1.478028416633606\n",
      "Training loss:1.4806588888168335\n",
      "Training loss:1.5836292505264282\n",
      "Training loss:1.5246044397354126\n",
      "Training loss:1.5405418872833252\n",
      "Training loss:1.5205060243606567\n",
      "Training loss:1.5617077350616455\n",
      "Training loss:1.4915101528167725\n",
      "Training loss:1.5407114028930664\n",
      "Training loss:1.607399582862854\n",
      "Training loss:1.4529720544815063\n",
      "Training loss:1.6242448091506958\n",
      "Training loss:1.514151692390442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5004018545150757\n",
      "Training loss:1.585512399673462\n",
      "Training loss:1.4859681129455566\n",
      "Training loss:1.5850772857666016\n",
      "Training loss:1.4952969551086426\n",
      "Training loss:1.5454846620559692\n",
      "Training loss:1.5212422609329224\n",
      "Training loss:1.5587607622146606\n",
      "Training loss:1.5405961275100708\n",
      "Training loss:1.592349648475647\n",
      "Training loss:1.60515558719635\n",
      "Training loss:1.6087437868118286\n",
      "Training loss:1.592506766319275\n",
      "Training loss:1.52582848072052\n",
      "Training loss:1.4967964887619019\n",
      "Training loss:1.5001883506774902\n",
      "Training loss:1.5173094272613525\n",
      "Training loss:1.5588312149047852\n",
      "Training loss:1.4616625308990479\n",
      "Training loss:1.508205771446228\n",
      "Training loss:1.519482135772705\n",
      "Training loss:1.5346239805221558\n",
      "Training loss:1.4644047021865845\n",
      "Training loss:1.544611930847168\n",
      "Training loss:1.5375038385391235\n",
      "Training loss:1.4653371572494507\n",
      "Training loss:1.4602481126785278\n",
      "Training loss:1.5291022062301636\n",
      "Training loss:1.4802852869033813\n",
      "Training loss:1.5287754535675049\n",
      "Training loss:1.5488876104354858\n",
      "Training loss:1.6163041591644287\n",
      "Training loss:1.4489684104919434\n",
      "Training loss:1.3963712453842163\n",
      "Training loss:1.4907857179641724\n",
      "Training loss:1.5136278867721558\n",
      "Training loss:1.5541781187057495\n",
      "Training loss:1.49901282787323\n",
      "Training loss:1.529657244682312\n",
      "Training loss:1.4679452180862427\n",
      "Training loss:1.4852195978164673\n",
      "Training loss:1.525617241859436\n",
      "Training loss:1.6053733825683594\n",
      "Training loss:1.4705967903137207\n",
      "Training loss:1.5395245552062988\n",
      "Training loss:1.5497268438339233\n",
      "Training loss:1.524827003479004\n",
      "Training loss:1.4977810382843018\n",
      "Training loss:1.5289570093154907\n",
      "Training loss:1.462533712387085\n",
      "Training loss:1.5061702728271484\n",
      "Training loss:1.625913143157959\n",
      "Training loss:1.5063676834106445\n",
      "Training loss:1.4865542650222778\n",
      "Training loss:1.6240248680114746\n",
      "Training loss:1.538945198059082\n",
      "Training loss:1.4554131031036377\n",
      "Training loss:1.5735052824020386\n",
      "Training loss:1.5556584596633911\n",
      "Training loss:1.5818971395492554\n",
      "Training loss:1.5586051940917969\n",
      "Training loss:1.4888625144958496\n",
      "Training loss:1.5992763042449951\n",
      "Training loss:1.525037169456482\n",
      "Training loss:1.6129156351089478\n",
      "Training loss:1.5722161531448364\n",
      "Training loss:1.5786813497543335\n",
      "Training loss:1.5161995887756348\n",
      "Training loss:1.5138593912124634\n",
      "Training loss:1.534079670906067\n",
      "Training loss:1.4861583709716797\n",
      "Training loss:1.4988094568252563\n",
      "Training loss:1.5334794521331787\n",
      "Training loss:1.4917157888412476\n",
      "Training loss:1.6095093488693237\n",
      "Training loss:1.6404527425765991\n",
      "Training loss:1.615027666091919\n",
      "Training loss:1.5204861164093018\n",
      "Training loss:1.4722261428833008\n",
      "Epoch: 0132 loss_train: 143.858714 acc_train: 0.445521 loss_val: 18.437836 acc_val: 0.436333 time: 336550.368448s\n",
      "Training loss:1.6022379398345947\n",
      "Training loss:1.4652174711227417\n",
      "Training loss:1.6108914613723755\n",
      "Training loss:1.5997231006622314\n",
      "Training loss:1.6283705234527588\n",
      "Training loss:1.660386323928833\n",
      "Training loss:1.5074796676635742\n",
      "Training loss:1.5327301025390625\n",
      "Training loss:1.515692949295044\n",
      "Training loss:1.5623506307601929\n",
      "Training loss:1.4967854022979736\n",
      "Training loss:1.4742392301559448\n",
      "Training loss:1.4783122539520264\n",
      "Training loss:1.4941998720169067\n",
      "Training loss:1.5632591247558594\n",
      "Training loss:1.4892526865005493\n",
      "Training loss:1.5272568464279175\n",
      "Training loss:1.5029698610305786\n",
      "Training loss:1.5298811197280884\n",
      "Training loss:1.5630167722702026\n",
      "Training loss:1.5142582654953003\n",
      "Training loss:1.5355156660079956\n",
      "Training loss:1.4828823804855347\n",
      "Training loss:1.5796257257461548\n",
      "Training loss:1.514359474182129\n",
      "Training loss:1.5031304359436035\n",
      "Training loss:1.5282255411148071\n",
      "Training loss:1.425472617149353\n",
      "Training loss:1.528212547302246\n",
      "Training loss:1.5588524341583252\n",
      "Training loss:1.5281449556350708\n",
      "Training loss:1.5104955434799194\n",
      "Training loss:1.6087901592254639\n",
      "Training loss:1.524841070175171\n",
      "Training loss:1.4977781772613525\n",
      "Training loss:1.4835318326950073\n",
      "Training loss:1.5446223020553589\n",
      "Training loss:1.5705236196517944\n",
      "Training loss:1.5275611877441406\n",
      "Training loss:1.5106639862060547\n",
      "Training loss:1.4756349325180054\n",
      "Training loss:1.4474916458129883\n",
      "Training loss:1.578749179840088\n",
      "Training loss:1.5930061340332031\n",
      "Training loss:1.4782755374908447\n",
      "Training loss:1.5771374702453613\n",
      "Training loss:1.4671378135681152\n",
      "Training loss:1.5362043380737305\n",
      "Training loss:1.4792011976242065\n",
      "Training loss:1.4806935787200928\n",
      "Training loss:1.5931625366210938\n",
      "Training loss:1.4684133529663086\n",
      "Training loss:1.5640084743499756\n",
      "Training loss:1.6537282466888428\n",
      "Training loss:1.5520015954971313\n",
      "Training loss:1.4914753437042236\n",
      "Training loss:1.555935263633728\n",
      "Training loss:1.5367976427078247\n",
      "Training loss:1.4878578186035156\n",
      "Training loss:1.5592283010482788\n",
      "Training loss:1.5076241493225098\n",
      "Training loss:1.541236162185669\n",
      "Training loss:1.5253454446792603\n",
      "Training loss:1.565171241760254\n",
      "Training loss:1.5519609451293945\n",
      "Training loss:1.4982969760894775\n",
      "Training loss:1.5507893562316895\n",
      "Training loss:1.5395599603652954\n",
      "Training loss:1.5696165561676025\n",
      "Training loss:1.5396056175231934\n",
      "Training loss:1.5496891736984253\n",
      "Training loss:1.4682586193084717\n",
      "Training loss:1.5335112810134888\n",
      "Training loss:1.4855417013168335\n",
      "Training loss:1.5016294717788696\n",
      "Training loss:1.5262702703475952\n",
      "Training loss:1.479525089263916\n",
      "Training loss:1.5561689138412476\n",
      "Training loss:1.5851755142211914\n",
      "Training loss:1.5238374471664429\n",
      "Training loss:1.5336562395095825\n",
      "Training loss:1.5248768329620361\n",
      "Training loss:1.5813432931900024\n",
      "Training loss:1.5359960794448853\n",
      "Training loss:1.4898221492767334\n",
      "Training loss:1.5754421949386597\n",
      "Training loss:1.6302241086959839\n",
      "Training loss:1.578125238418579\n",
      "Training loss:1.5138927698135376\n",
      "Training loss:1.4734307527542114\n",
      "Training loss:1.502328634262085\n",
      "Training loss:1.5883821249008179\n",
      "Training loss:1.4565199613571167\n",
      "Training loss:1.5949376821517944\n",
      "Epoch: 0133 loss_train: 144.035674 acc_train: 0.442688 loss_val: 18.185134 acc_val: 0.450167 time: 339171.745117s\n",
      "Training loss:1.5242013931274414\n",
      "Training loss:1.559868335723877\n",
      "Training loss:1.5658748149871826\n",
      "Training loss:1.6101093292236328\n",
      "Training loss:1.5229852199554443\n",
      "Training loss:1.5194809436798096\n",
      "Training loss:1.544407606124878\n",
      "Training loss:1.519609808921814\n",
      "Training loss:1.551619052886963\n",
      "Training loss:1.5271580219268799\n",
      "Training loss:1.5460485219955444\n",
      "Training loss:1.5502527952194214\n",
      "Training loss:1.4950535297393799\n",
      "Training loss:1.6204887628555298\n",
      "Training loss:1.4983208179473877\n",
      "Training loss:1.486744999885559\n",
      "Training loss:1.4535253047943115\n",
      "Training loss:1.4770467281341553\n",
      "Training loss:1.5972617864608765\n",
      "Training loss:1.5188319683074951\n",
      "Training loss:1.6214580535888672\n",
      "Training loss:1.5603681802749634\n",
      "Training loss:1.5584757328033447\n",
      "Training loss:1.5061477422714233\n",
      "Training loss:1.475043773651123\n",
      "Training loss:1.5492277145385742\n",
      "Training loss:1.5891650915145874\n",
      "Training loss:1.5491515398025513\n",
      "Training loss:1.4791148900985718\n",
      "Training loss:1.5745810270309448\n",
      "Training loss:1.522329330444336\n",
      "Training loss:1.5002343654632568\n",
      "Training loss:1.507949709892273\n",
      "Training loss:1.4908260107040405\n",
      "Training loss:1.5254900455474854\n",
      "Training loss:1.478519082069397\n",
      "Training loss:1.6063467264175415\n",
      "Training loss:1.5939524173736572\n",
      "Training loss:1.5319626331329346\n",
      "Training loss:1.504335641860962\n",
      "Training loss:1.5506447553634644\n",
      "Training loss:1.4943833351135254\n",
      "Training loss:1.6160893440246582\n",
      "Training loss:1.5427403450012207\n",
      "Training loss:1.5250521898269653\n",
      "Training loss:1.5817005634307861\n",
      "Training loss:1.4618744850158691\n",
      "Training loss:1.4931886196136475\n",
      "Training loss:1.5607210397720337\n",
      "Training loss:1.526597023010254\n",
      "Training loss:1.493602991104126\n",
      "Training loss:1.6191260814666748\n",
      "Training loss:1.5509860515594482\n",
      "Training loss:1.5627309083938599\n",
      "Training loss:1.4106498956680298\n",
      "Training loss:1.4623240232467651\n",
      "Training loss:1.5392030477523804\n",
      "Training loss:1.5715858936309814\n",
      "Training loss:1.51316499710083\n",
      "Training loss:1.5741703510284424\n",
      "Training loss:1.4861046075820923\n",
      "Training loss:1.5237661600112915\n",
      "Training loss:1.502878189086914\n",
      "Training loss:1.5463666915893555\n",
      "Training loss:1.477809190750122\n",
      "Training loss:1.5141204595565796\n",
      "Training loss:1.4720289707183838\n",
      "Training loss:1.5196294784545898\n",
      "Training loss:1.5507140159606934\n",
      "Training loss:1.5332233905792236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5294748544692993\n",
      "Training loss:1.433530569076538\n",
      "Training loss:1.5278757810592651\n",
      "Training loss:1.5442880392074585\n",
      "Training loss:1.4820562601089478\n",
      "Training loss:1.476421594619751\n",
      "Training loss:1.4970829486846924\n",
      "Training loss:1.562503695487976\n",
      "Training loss:1.564340353012085\n",
      "Training loss:1.4684919118881226\n",
      "Training loss:1.5209665298461914\n",
      "Training loss:1.4823975563049316\n",
      "Training loss:1.5192638635635376\n",
      "Training loss:1.5617294311523438\n",
      "Training loss:1.5031704902648926\n",
      "Training loss:1.5136613845825195\n",
      "Training loss:1.542921543121338\n",
      "Training loss:1.5346879959106445\n",
      "Training loss:1.5813161134719849\n",
      "Training loss:1.4748241901397705\n",
      "Training loss:1.5114891529083252\n",
      "Training loss:1.4901275634765625\n",
      "Training loss:1.5874121189117432\n",
      "Training loss:1.5755176544189453\n",
      "Epoch: 0134 loss_train: 143.676298 acc_train: 0.443833 loss_val: 18.235539 acc_val: 0.443833 time: 342194.836334s\n",
      "Training loss:1.4965436458587646\n",
      "Training loss:1.62332022190094\n",
      "Training loss:1.5222574472427368\n",
      "Training loss:1.590415596961975\n",
      "Training loss:1.5110681056976318\n",
      "Training loss:1.5483638048171997\n",
      "Training loss:1.5742136240005493\n",
      "Training loss:1.4528305530548096\n",
      "Training loss:1.5080598592758179\n",
      "Training loss:1.457842469215393\n",
      "Training loss:1.530137300491333\n",
      "Training loss:1.578324556350708\n",
      "Training loss:1.5420501232147217\n",
      "Training loss:1.5508053302764893\n",
      "Training loss:1.4809207916259766\n",
      "Training loss:1.53042733669281\n",
      "Training loss:1.3946943283081055\n",
      "Training loss:1.5404675006866455\n",
      "Training loss:1.4878981113433838\n",
      "Training loss:1.4838345050811768\n",
      "Training loss:1.6209156513214111\n",
      "Training loss:1.5625944137573242\n",
      "Training loss:1.5369267463684082\n",
      "Training loss:1.4580811262130737\n",
      "Training loss:1.5747162103652954\n",
      "Training loss:1.4746103286743164\n",
      "Training loss:1.523442268371582\n",
      "Training loss:1.575632929801941\n",
      "Training loss:1.5998114347457886\n",
      "Training loss:1.541883111000061\n",
      "Training loss:1.5060793161392212\n",
      "Training loss:1.5304813385009766\n",
      "Training loss:1.5468084812164307\n",
      "Training loss:1.513956069946289\n",
      "Training loss:1.4629755020141602\n",
      "Training loss:1.5517827272415161\n",
      "Training loss:1.4837627410888672\n",
      "Training loss:1.5463863611221313\n",
      "Training loss:1.54635751247406\n",
      "Training loss:1.5445611476898193\n",
      "Training loss:1.5027737617492676\n",
      "Training loss:1.4852955341339111\n",
      "Training loss:1.5331710577011108\n",
      "Training loss:1.4904448986053467\n",
      "Training loss:1.4933499097824097\n",
      "Training loss:1.5297130346298218\n",
      "Training loss:1.4652495384216309\n",
      "Training loss:1.511282205581665\n",
      "Training loss:1.5830742120742798\n",
      "Training loss:1.5639994144439697\n",
      "Training loss:1.5947809219360352\n",
      "Training loss:1.5632970333099365\n",
      "Training loss:1.5401360988616943\n",
      "Training loss:1.604591965675354\n",
      "Training loss:1.5448789596557617\n",
      "Training loss:1.6437265872955322\n",
      "Training loss:1.56756591796875\n",
      "Training loss:1.5892726182937622\n",
      "Training loss:1.5015010833740234\n",
      "Training loss:1.5041093826293945\n",
      "Training loss:1.4420820474624634\n",
      "Training loss:1.5658408403396606\n",
      "Training loss:1.5502021312713623\n",
      "Training loss:1.5782078504562378\n",
      "Training loss:1.5868414640426636\n",
      "Training loss:1.5471423864364624\n",
      "Training loss:1.5483022928237915\n",
      "Training loss:1.5233721733093262\n",
      "Training loss:1.5633368492126465\n",
      "Training loss:1.562071442604065\n",
      "Training loss:1.5254451036453247\n",
      "Training loss:1.4750123023986816\n",
      "Training loss:1.5450944900512695\n",
      "Training loss:1.5722166299819946\n",
      "Training loss:1.4567177295684814\n",
      "Training loss:1.4808056354522705\n",
      "Training loss:1.5234155654907227\n",
      "Training loss:1.480749249458313\n",
      "Training loss:1.5294526815414429\n",
      "Training loss:1.516310214996338\n",
      "Training loss:1.4892675876617432\n",
      "Training loss:1.5270155668258667\n",
      "Training loss:1.5008519887924194\n",
      "Training loss:1.459831714630127\n",
      "Training loss:1.5926721096038818\n",
      "Training loss:1.5967026948928833\n",
      "Training loss:1.440733790397644\n",
      "Training loss:1.4623326063156128\n",
      "Training loss:1.530295729637146\n",
      "Training loss:1.4435776472091675\n",
      "Training loss:1.5817433595657349\n",
      "Training loss:1.632509469985962\n",
      "Training loss:1.4692778587341309\n",
      "Training loss:1.5415812730789185\n",
      "Epoch: 0135 loss_train: 143.759467 acc_train: 0.443917 loss_val: 18.272497 acc_val: 0.447167 time: 345005.271611s\n",
      "Training loss:1.5015180110931396\n",
      "Training loss:1.533660888671875\n",
      "Training loss:1.5422950983047485\n",
      "Training loss:1.5533310174942017\n",
      "Training loss:1.5217052698135376\n",
      "Training loss:1.5595601797103882\n",
      "Training loss:1.503015160560608\n",
      "Training loss:1.5253634452819824\n",
      "Training loss:1.5336397886276245\n",
      "Training loss:1.5244410037994385\n",
      "Training loss:1.5642712116241455\n",
      "Training loss:1.502530574798584\n",
      "Training loss:1.4614163637161255\n",
      "Training loss:1.5469543933868408\n",
      "Training loss:1.5099574327468872\n",
      "Training loss:1.5568350553512573\n",
      "Training loss:1.445809006690979\n",
      "Training loss:1.5666412115097046\n",
      "Training loss:1.4690905809402466\n",
      "Training loss:1.458039402961731\n",
      "Training loss:1.478760004043579\n",
      "Training loss:1.5570213794708252\n",
      "Training loss:1.4914743900299072\n",
      "Training loss:1.4227893352508545\n",
      "Training loss:1.472928762435913\n",
      "Training loss:1.5211875438690186\n",
      "Training loss:1.5100914239883423\n",
      "Training loss:1.4908173084259033\n",
      "Training loss:1.5364638566970825\n",
      "Training loss:1.590003490447998\n",
      "Training loss:1.5242750644683838\n",
      "Training loss:1.4946928024291992\n",
      "Training loss:1.5599830150604248\n",
      "Training loss:1.503129005432129\n",
      "Training loss:1.4833588600158691\n",
      "Training loss:1.5381613969802856\n",
      "Training loss:1.4624810218811035\n",
      "Training loss:1.4945068359375\n",
      "Training loss:1.5850179195404053\n",
      "Training loss:1.601435899734497\n",
      "Training loss:1.44791841506958\n",
      "Training loss:1.5297224521636963\n",
      "Training loss:1.5262362957000732\n",
      "Training loss:1.5597833395004272\n",
      "Training loss:1.6118799448013306\n",
      "Training loss:1.43294095993042\n",
      "Training loss:1.4788042306900024\n",
      "Training loss:1.4981073141098022\n",
      "Training loss:1.5708786249160767\n",
      "Training loss:1.5440500974655151\n",
      "Training loss:1.5148555040359497\n",
      "Training loss:1.4876248836517334\n",
      "Training loss:1.5189872980117798\n",
      "Training loss:1.4768153429031372\n",
      "Training loss:1.601811408996582\n",
      "Training loss:1.5531359910964966\n",
      "Training loss:1.5271590948104858\n",
      "Training loss:1.4962235689163208\n",
      "Training loss:1.5193777084350586\n",
      "Training loss:1.518533706665039\n",
      "Training loss:1.5755797624588013\n",
      "Training loss:1.5748687982559204\n",
      "Training loss:1.5435764789581299\n",
      "Training loss:1.5280606746673584\n",
      "Training loss:1.5614607334136963\n",
      "Training loss:1.520830512046814\n",
      "Training loss:1.4908541440963745\n",
      "Training loss:1.466260552406311\n",
      "Training loss:1.5064960718154907\n",
      "Training loss:1.5964291095733643\n",
      "Training loss:1.4988805055618286\n",
      "Training loss:1.4071435928344727\n",
      "Training loss:1.495062232017517\n",
      "Training loss:1.5512115955352783\n",
      "Training loss:1.4969089031219482\n",
      "Training loss:1.4913206100463867\n",
      "Training loss:1.5344181060791016\n",
      "Training loss:1.4696245193481445\n",
      "Training loss:1.4772638082504272\n",
      "Training loss:1.4844486713409424\n",
      "Training loss:1.593526005744934\n",
      "Training loss:1.6040496826171875\n",
      "Training loss:1.5282318592071533\n",
      "Training loss:1.4703885316848755\n",
      "Training loss:1.5189387798309326\n",
      "Training loss:1.527739405632019\n",
      "Training loss:1.486392855644226\n",
      "Training loss:1.5699347257614136\n",
      "Training loss:1.5979809761047363\n",
      "Training loss:1.5694050788879395\n",
      "Training loss:1.5172523260116577\n",
      "Training loss:1.564255714416504\n",
      "Training loss:1.5772253274917603\n",
      "Training loss:1.5693265199661255\n",
      "Epoch: 0136 loss_train: 143.078848 acc_train: 0.448396 loss_val: 18.150974 acc_val: 0.452000 time: 347892.519600s\n",
      "Training loss:1.5359265804290771\n",
      "Training loss:1.5800613164901733\n",
      "Training loss:1.4509828090667725\n",
      "Training loss:1.4605778455734253\n",
      "Training loss:1.5184597969055176\n",
      "Training loss:1.5573533773422241\n",
      "Training loss:1.417109727859497\n",
      "Training loss:1.5005930662155151\n",
      "Training loss:1.5864139795303345\n",
      "Training loss:1.4484848976135254\n",
      "Training loss:1.4766545295715332\n",
      "Training loss:1.4768487215042114\n",
      "Training loss:1.5659962892532349\n",
      "Training loss:1.4521105289459229\n",
      "Training loss:1.5436880588531494\n",
      "Training loss:1.5410453081130981\n",
      "Training loss:1.4752330780029297\n",
      "Training loss:1.5314372777938843\n",
      "Training loss:1.5174736976623535\n",
      "Training loss:1.5394092798233032\n",
      "Training loss:1.5817006826400757\n",
      "Training loss:1.5527801513671875\n",
      "Training loss:1.5442019701004028\n",
      "Training loss:1.5224891901016235\n",
      "Training loss:1.5126234292984009\n",
      "Training loss:1.5896302461624146\n",
      "Training loss:1.4464421272277832\n",
      "Training loss:1.5470268726348877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5563690662384033\n",
      "Training loss:1.551838994026184\n",
      "Training loss:1.548018455505371\n",
      "Training loss:1.5246200561523438\n",
      "Training loss:1.489743709564209\n",
      "Training loss:1.5216413736343384\n",
      "Training loss:1.5399965047836304\n",
      "Training loss:1.549975872039795\n",
      "Training loss:1.4633071422576904\n",
      "Training loss:1.5307577848434448\n",
      "Training loss:1.5941696166992188\n",
      "Training loss:1.549336314201355\n",
      "Training loss:1.4784866571426392\n",
      "Training loss:1.4823040962219238\n",
      "Training loss:1.5084505081176758\n",
      "Training loss:1.5310709476470947\n",
      "Training loss:1.4894062280654907\n",
      "Training loss:1.5539491176605225\n",
      "Training loss:1.5250712633132935\n",
      "Training loss:1.5743203163146973\n",
      "Training loss:1.47434663772583\n",
      "Training loss:1.5235931873321533\n",
      "Training loss:1.4557631015777588\n",
      "Training loss:1.5158041715621948\n",
      "Training loss:1.595584511756897\n",
      "Training loss:1.549601435661316\n",
      "Training loss:1.5946770906448364\n",
      "Training loss:1.5388951301574707\n",
      "Training loss:1.472220778465271\n",
      "Training loss:1.525452733039856\n",
      "Training loss:1.4478808641433716\n",
      "Training loss:1.539634108543396\n",
      "Training loss:1.4968880414962769\n",
      "Training loss:1.5195304155349731\n",
      "Training loss:1.4903289079666138\n",
      "Training loss:1.5327565670013428\n",
      "Training loss:1.5555768013000488\n",
      "Training loss:1.5229933261871338\n",
      "Training loss:1.5197006464004517\n",
      "Training loss:1.5890880823135376\n",
      "Training loss:1.5093625783920288\n",
      "Training loss:1.4450262784957886\n",
      "Training loss:1.5136053562164307\n",
      "Training loss:1.5109353065490723\n",
      "Training loss:1.6215254068374634\n",
      "Training loss:1.544644832611084\n",
      "Training loss:1.4941613674163818\n",
      "Training loss:1.5280457735061646\n",
      "Training loss:1.5644370317459106\n",
      "Training loss:1.5150830745697021\n",
      "Training loss:1.5073350667953491\n",
      "Training loss:1.5205711126327515\n",
      "Training loss:1.4584815502166748\n",
      "Training loss:1.5538500547409058\n",
      "Training loss:1.5343296527862549\n",
      "Training loss:1.4646183252334595\n",
      "Training loss:1.4853603839874268\n",
      "Training loss:1.5994683504104614\n",
      "Training loss:1.5694209337234497\n",
      "Training loss:1.5586076974868774\n",
      "Training loss:1.5226197242736816\n",
      "Training loss:1.51669442653656\n",
      "Training loss:1.520399570465088\n",
      "Training loss:1.567013144493103\n",
      "Training loss:1.5367145538330078\n",
      "Training loss:1.6111079454421997\n",
      "Epoch: 0137 loss_train: 143.269325 acc_train: 0.445458 loss_val: 18.247790 acc_val: 0.441167 time: 353043.141937s\n",
      "Training loss:1.5282138586044312\n",
      "Training loss:1.4989815950393677\n",
      "Training loss:1.5766572952270508\n",
      "Training loss:1.5314925909042358\n",
      "Training loss:1.520182490348816\n",
      "Training loss:1.567217469215393\n",
      "Training loss:1.554630994796753\n",
      "Training loss:1.5064858198165894\n",
      "Training loss:1.5046746730804443\n",
      "Training loss:1.5206348896026611\n",
      "Training loss:1.5859808921813965\n",
      "Training loss:1.5438902378082275\n",
      "Training loss:1.5921319723129272\n",
      "Training loss:1.4471936225891113\n",
      "Training loss:1.4842928647994995\n",
      "Training loss:1.5145949125289917\n",
      "Training loss:1.5520700216293335\n",
      "Training loss:1.5811553001403809\n",
      "Training loss:1.5021779537200928\n",
      "Training loss:1.5408352613449097\n",
      "Training loss:1.5371935367584229\n",
      "Training loss:1.5291756391525269\n",
      "Training loss:1.5043635368347168\n",
      "Training loss:1.593848466873169\n",
      "Training loss:1.4629374742507935\n",
      "Training loss:1.5230331420898438\n",
      "Training loss:1.4818789958953857\n",
      "Training loss:1.52857506275177\n",
      "Training loss:1.4872190952301025\n",
      "Training loss:1.5709120035171509\n",
      "Training loss:1.4317115545272827\n",
      "Training loss:1.5335874557495117\n",
      "Training loss:1.602709412574768\n",
      "Training loss:1.5322456359863281\n",
      "Training loss:1.585868239402771\n",
      "Training loss:1.5724430084228516\n",
      "Training loss:1.5418531894683838\n",
      "Training loss:1.5352250337600708\n",
      "Training loss:1.5380799770355225\n",
      "Training loss:1.4734383821487427\n",
      "Training loss:1.5717233419418335\n",
      "Training loss:1.539076328277588\n",
      "Training loss:1.5512421131134033\n",
      "Training loss:1.506695032119751\n",
      "Training loss:1.6193608045578003\n",
      "Training loss:1.491517186164856\n",
      "Training loss:1.4427651166915894\n",
      "Training loss:1.5597456693649292\n",
      "Training loss:1.5056710243225098\n",
      "Training loss:1.5321564674377441\n",
      "Training loss:1.4446742534637451\n",
      "Training loss:1.5265687704086304\n",
      "Training loss:1.4803223609924316\n",
      "Training loss:1.5020923614501953\n",
      "Training loss:1.5695792436599731\n",
      "Training loss:1.5136363506317139\n",
      "Training loss:1.502825140953064\n",
      "Training loss:1.5211279392242432\n",
      "Training loss:1.574778437614441\n",
      "Training loss:1.4797282218933105\n",
      "Training loss:1.5767059326171875\n",
      "Training loss:1.5306613445281982\n",
      "Training loss:1.604474663734436\n",
      "Training loss:1.5021544694900513\n",
      "Training loss:1.4777404069900513\n",
      "Training loss:1.4544808864593506\n",
      "Training loss:1.5743334293365479\n",
      "Training loss:1.5944936275482178\n",
      "Training loss:1.5514894723892212\n",
      "Training loss:1.4344894886016846\n",
      "Training loss:1.4890351295471191\n",
      "Training loss:1.4904805421829224\n",
      "Training loss:1.581308126449585\n",
      "Training loss:1.5202933549880981\n",
      "Training loss:1.512385368347168\n",
      "Training loss:1.51741361618042\n",
      "Training loss:1.5070253610610962\n",
      "Training loss:1.58059823513031\n",
      "Training loss:1.4745230674743652\n",
      "Training loss:1.549638032913208\n",
      "Training loss:1.4975194931030273\n",
      "Training loss:1.4538462162017822\n",
      "Training loss:1.5692046880722046\n",
      "Training loss:1.5020827054977417\n",
      "Training loss:1.5271655321121216\n",
      "Training loss:1.4824055433273315\n",
      "Training loss:1.4507015943527222\n",
      "Training loss:1.5195409059524536\n",
      "Training loss:1.653199315071106\n",
      "Training loss:1.5257327556610107\n",
      "Training loss:1.579594373703003\n",
      "Training loss:1.565840482711792\n",
      "Training loss:1.501102089881897\n",
      "Training loss:1.4742975234985352\n",
      "Epoch: 0138 loss_train: 143.481037 acc_train: 0.444688 loss_val: 18.094346 acc_val: 0.444500 time: 355697.782577s\n",
      "Training loss:1.58516263961792\n",
      "Training loss:1.5162192583084106\n",
      "Training loss:1.5474518537521362\n",
      "Training loss:1.5195388793945312\n",
      "Training loss:1.504486322402954\n",
      "Training loss:1.5132346153259277\n",
      "Training loss:1.5085973739624023\n",
      "Training loss:1.4958828687667847\n",
      "Training loss:1.5475987195968628\n",
      "Training loss:1.548474907875061\n",
      "Training loss:1.5655900239944458\n",
      "Training loss:1.5097098350524902\n",
      "Training loss:1.5386391878128052\n",
      "Training loss:1.5096440315246582\n",
      "Training loss:1.5230270624160767\n",
      "Training loss:1.4163312911987305\n",
      "Training loss:1.5702409744262695\n",
      "Training loss:1.5494097471237183\n",
      "Training loss:1.511411428451538\n",
      "Training loss:1.5050653219223022\n",
      "Training loss:1.5551377534866333\n",
      "Training loss:1.4650336503982544\n",
      "Training loss:1.4786670207977295\n",
      "Training loss:1.571828007698059\n",
      "Training loss:1.5342552661895752\n",
      "Training loss:1.486312985420227\n",
      "Training loss:1.5729637145996094\n",
      "Training loss:1.5703476667404175\n",
      "Training loss:1.5205886363983154\n",
      "Training loss:1.4634796380996704\n",
      "Training loss:1.5118825435638428\n",
      "Training loss:1.5021772384643555\n",
      "Training loss:1.4986296892166138\n",
      "Training loss:1.5265949964523315\n",
      "Training loss:1.4862309694290161\n",
      "Training loss:1.4616141319274902\n",
      "Training loss:1.5204180479049683\n",
      "Training loss:1.5024267435073853\n",
      "Training loss:1.492749571800232\n",
      "Training loss:1.5844441652297974\n",
      "Training loss:1.4913482666015625\n",
      "Training loss:1.4918341636657715\n",
      "Training loss:1.4393037557601929\n",
      "Training loss:1.4855806827545166\n",
      "Training loss:1.5410206317901611\n",
      "Training loss:1.5245137214660645\n",
      "Training loss:1.560671329498291\n",
      "Training loss:1.4640058279037476\n",
      "Training loss:1.493938684463501\n",
      "Training loss:1.541947841644287\n",
      "Training loss:1.5560652017593384\n",
      "Training loss:1.5384376049041748\n",
      "Training loss:1.530400037765503\n",
      "Training loss:1.4600523710250854\n",
      "Training loss:1.5486903190612793\n",
      "Training loss:1.5661500692367554\n",
      "Training loss:1.525014042854309\n",
      "Training loss:1.4876861572265625\n",
      "Training loss:1.5352087020874023\n",
      "Training loss:1.4563016891479492\n",
      "Training loss:1.5704684257507324\n",
      "Training loss:1.4933679103851318\n",
      "Training loss:1.494859218597412\n",
      "Training loss:1.4951986074447632\n",
      "Training loss:1.491031289100647\n",
      "Training loss:1.5915312767028809\n",
      "Training loss:1.4871900081634521\n",
      "Training loss:1.528130292892456\n",
      "Training loss:1.568860411643982\n",
      "Training loss:1.5646679401397705\n",
      "Training loss:1.5825388431549072\n",
      "Training loss:1.4531550407409668\n",
      "Training loss:1.481041431427002\n",
      "Training loss:1.5221478939056396\n",
      "Training loss:1.5387630462646484\n",
      "Training loss:1.5397472381591797\n",
      "Training loss:1.4805283546447754\n",
      "Training loss:1.5102012157440186\n",
      "Training loss:1.512657880783081\n",
      "Training loss:1.532989501953125\n",
      "Training loss:1.5185788869857788\n",
      "Training loss:1.514845609664917\n",
      "Training loss:1.5321803092956543\n",
      "Training loss:1.4880088567733765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5228171348571777\n",
      "Training loss:1.4558368921279907\n",
      "Training loss:1.5161296129226685\n",
      "Training loss:1.4810986518859863\n",
      "Training loss:1.4922171831130981\n",
      "Training loss:1.559394359588623\n",
      "Training loss:1.4369678497314453\n",
      "Training loss:1.5289212465286255\n",
      "Training loss:1.5530940294265747\n",
      "Training loss:1.5969010591506958\n",
      "Epoch: 0139 loss_train: 142.665737 acc_train: 0.449771 loss_val: 18.104248 acc_val: 0.451167 time: 358282.508761s\n",
      "Training loss:1.5787655115127563\n",
      "Training loss:1.5803173780441284\n",
      "Training loss:1.5294265747070312\n",
      "Training loss:1.538102149963379\n",
      "Training loss:1.4278063774108887\n",
      "Training loss:1.4262011051177979\n",
      "Training loss:1.5676040649414062\n",
      "Training loss:1.6134932041168213\n",
      "Training loss:1.4819920063018799\n",
      "Training loss:1.5126348733901978\n",
      "Training loss:1.5012778043746948\n",
      "Training loss:1.5578699111938477\n",
      "Training loss:1.4997658729553223\n",
      "Training loss:1.5132488012313843\n",
      "Training loss:1.5243892669677734\n",
      "Training loss:1.4993737936019897\n",
      "Training loss:1.53447425365448\n",
      "Training loss:1.539063811302185\n",
      "Training loss:1.492013931274414\n",
      "Training loss:1.5340125560760498\n",
      "Training loss:1.5050750970840454\n",
      "Training loss:1.6065295934677124\n",
      "Training loss:1.4838693141937256\n",
      "Training loss:1.5620026588439941\n",
      "Training loss:1.5435775518417358\n",
      "Training loss:1.5221805572509766\n",
      "Training loss:1.5417349338531494\n",
      "Training loss:1.5927497148513794\n",
      "Training loss:1.5279245376586914\n",
      "Training loss:1.4420119524002075\n",
      "Training loss:1.5556950569152832\n",
      "Training loss:1.6721272468566895\n",
      "Training loss:1.5158157348632812\n",
      "Training loss:1.6441237926483154\n",
      "Training loss:1.4553992748260498\n",
      "Training loss:1.5333706140518188\n",
      "Training loss:1.4390342235565186\n",
      "Training loss:1.5549203157424927\n",
      "Training loss:1.6064121723175049\n",
      "Training loss:1.5880067348480225\n",
      "Training loss:1.5104001760482788\n",
      "Training loss:1.4919954538345337\n",
      "Training loss:1.4661602973937988\n",
      "Training loss:1.447389841079712\n",
      "Training loss:1.4982177019119263\n",
      "Training loss:1.5819612741470337\n",
      "Training loss:1.5418297052383423\n",
      "Training loss:1.5367635488510132\n",
      "Training loss:1.5552881956100464\n",
      "Training loss:1.5980994701385498\n",
      "Training loss:1.520753026008606\n",
      "Training loss:1.5897960662841797\n",
      "Training loss:1.4678664207458496\n",
      "Training loss:1.4206820726394653\n",
      "Training loss:1.4817129373550415\n",
      "Training loss:1.4972811937332153\n",
      "Training loss:1.4762957096099854\n",
      "Training loss:1.4986428022384644\n",
      "Training loss:1.4867223501205444\n",
      "Training loss:1.4799730777740479\n",
      "Training loss:1.4398126602172852\n",
      "Training loss:1.6091482639312744\n",
      "Training loss:1.520735740661621\n",
      "Training loss:1.4288465976715088\n",
      "Training loss:1.5297770500183105\n",
      "Training loss:1.4937469959259033\n",
      "Training loss:1.5260810852050781\n",
      "Training loss:1.5264984369277954\n",
      "Training loss:1.5844796895980835\n",
      "Training loss:1.4599401950836182\n",
      "Training loss:1.4706380367279053\n",
      "Training loss:1.4044137001037598\n",
      "Training loss:1.552797794342041\n",
      "Training loss:1.5559475421905518\n",
      "Training loss:1.586722493171692\n",
      "Training loss:1.643765926361084\n",
      "Training loss:1.5495492219924927\n",
      "Training loss:1.5984598398208618\n",
      "Training loss:1.5550836324691772\n",
      "Training loss:1.500623345375061\n",
      "Training loss:1.4682978391647339\n",
      "Training loss:1.5617244243621826\n",
      "Training loss:1.5088744163513184\n",
      "Training loss:1.5790044069290161\n",
      "Training loss:1.5829668045043945\n",
      "Training loss:1.619249701499939\n",
      "Training loss:1.4855375289916992\n",
      "Training loss:1.5332638025283813\n",
      "Training loss:1.5301519632339478\n",
      "Training loss:1.511093258857727\n",
      "Training loss:1.5102750062942505\n",
      "Training loss:1.4230254888534546\n",
      "Training loss:1.4965343475341797\n",
      "Training loss:1.494907021522522\n",
      "Epoch: 0140 loss_train: 143.334200 acc_train: 0.446229 loss_val: 18.016194 acc_val: 0.450500 time: 360888.788005s\n",
      "Training loss:1.4965548515319824\n",
      "Training loss:1.506718397140503\n",
      "Training loss:1.5179104804992676\n",
      "Training loss:1.5019776821136475\n",
      "Training loss:1.4976178407669067\n",
      "Training loss:1.5000431537628174\n",
      "Training loss:1.5073953866958618\n",
      "Training loss:1.4740585088729858\n",
      "Training loss:1.5061229467391968\n",
      "Training loss:1.4141346216201782\n",
      "Training loss:1.5428622961044312\n",
      "Training loss:1.4938383102416992\n",
      "Training loss:1.512178897857666\n",
      "Training loss:1.4660634994506836\n",
      "Training loss:1.4790215492248535\n",
      "Training loss:1.4867185354232788\n",
      "Training loss:1.5399632453918457\n",
      "Training loss:1.4890245199203491\n",
      "Training loss:1.522135615348816\n",
      "Training loss:1.5230443477630615\n",
      "Training loss:1.4581300020217896\n",
      "Training loss:1.586051106452942\n",
      "Training loss:1.6001830101013184\n",
      "Training loss:1.4744319915771484\n",
      "Training loss:1.4483734369277954\n",
      "Training loss:1.469960331916809\n",
      "Training loss:1.464450716972351\n",
      "Training loss:1.523686170578003\n",
      "Training loss:1.5521174669265747\n",
      "Training loss:1.5000357627868652\n",
      "Training loss:1.5136152505874634\n",
      "Training loss:1.5197312831878662\n",
      "Training loss:1.4447200298309326\n",
      "Training loss:1.5305029153823853\n",
      "Training loss:1.58248770236969\n",
      "Training loss:1.557632327079773\n",
      "Training loss:1.5151854753494263\n",
      "Training loss:1.5014698505401611\n",
      "Training loss:1.481293797492981\n",
      "Training loss:1.5462685823440552\n",
      "Training loss:1.5846116542816162\n",
      "Training loss:1.4490505456924438\n",
      "Training loss:1.4638328552246094\n",
      "Training loss:1.5673576593399048\n",
      "Training loss:1.5592694282531738\n",
      "Training loss:1.628684401512146\n",
      "Training loss:1.5112024545669556\n",
      "Training loss:1.5183324813842773\n",
      "Training loss:1.5475361347198486\n",
      "Training loss:1.515146017074585\n",
      "Training loss:1.506156325340271\n",
      "Training loss:1.5731343030929565\n",
      "Training loss:1.515627145767212\n",
      "Training loss:1.5241948366165161\n",
      "Training loss:1.5092333555221558\n",
      "Training loss:1.5335859060287476\n",
      "Training loss:1.5020925998687744\n",
      "Training loss:1.5408411026000977\n",
      "Training loss:1.5164847373962402\n",
      "Training loss:1.4840419292449951\n",
      "Training loss:1.571284294128418\n",
      "Training loss:1.4710849523544312\n",
      "Training loss:1.4917032718658447\n",
      "Training loss:1.479893445968628\n",
      "Training loss:1.486934781074524\n",
      "Training loss:1.5950168371200562\n",
      "Training loss:1.5332163572311401\n",
      "Training loss:1.4675697088241577\n",
      "Training loss:1.5424329042434692\n",
      "Training loss:1.5568270683288574\n",
      "Training loss:1.470866322517395\n",
      "Training loss:1.5157734155654907\n",
      "Training loss:1.605865478515625\n",
      "Training loss:1.5751872062683105\n",
      "Training loss:1.5106937885284424\n",
      "Training loss:1.5109636783599854\n",
      "Training loss:1.4524792432785034\n",
      "Training loss:1.539392113685608\n",
      "Training loss:1.516971230506897\n",
      "Training loss:1.5048798322677612\n",
      "Training loss:1.4754865169525146\n",
      "Training loss:1.519649863243103\n",
      "Training loss:1.505244255065918\n",
      "Training loss:1.6173747777938843\n",
      "Training loss:1.488694429397583\n",
      "Training loss:1.4910149574279785\n",
      "Training loss:1.515273094177246\n",
      "Training loss:1.5450021028518677\n",
      "Training loss:1.5105609893798828\n",
      "Training loss:1.5557693243026733\n",
      "Training loss:1.5503019094467163\n",
      "Training loss:1.5100504159927368\n",
      "Training loss:1.4942409992218018\n",
      "Training loss:1.643246054649353\n",
      "Epoch: 0141 loss_train: 142.619073 acc_train: 0.447771 loss_val: 18.159901 acc_val: 0.448167 time: 363454.653445s\n",
      "Training loss:1.5365122556686401\n",
      "Training loss:1.582127332687378\n",
      "Training loss:1.5449974536895752\n",
      "Training loss:1.6176975965499878\n",
      "Training loss:1.532117247581482\n",
      "Training loss:1.4494333267211914\n",
      "Training loss:1.5177512168884277\n",
      "Training loss:1.5260518789291382\n",
      "Training loss:1.5421897172927856\n",
      "Training loss:1.5224977731704712\n",
      "Training loss:1.5633407831192017\n",
      "Training loss:1.5582722425460815\n",
      "Training loss:1.4558340311050415\n",
      "Training loss:1.4692658185958862\n",
      "Training loss:1.5372676849365234\n",
      "Training loss:1.5198436975479126\n",
      "Training loss:1.5155134201049805\n",
      "Training loss:1.5357273817062378\n",
      "Training loss:1.4644854068756104\n",
      "Training loss:1.5501089096069336\n",
      "Training loss:1.5004793405532837\n",
      "Training loss:1.482118010520935\n",
      "Training loss:1.5983314514160156\n",
      "Training loss:1.44582998752594\n",
      "Training loss:1.509668231010437\n",
      "Training loss:1.529706597328186\n",
      "Training loss:1.5225248336791992\n",
      "Training loss:1.4552747011184692\n",
      "Training loss:1.520469069480896\n",
      "Training loss:1.506906509399414\n",
      "Training loss:1.519562840461731\n",
      "Training loss:1.5614784955978394\n",
      "Training loss:1.5500975847244263\n",
      "Training loss:1.49280846118927\n",
      "Training loss:1.5585554838180542\n",
      "Training loss:1.5039366483688354\n",
      "Training loss:1.4281561374664307\n",
      "Training loss:1.576540470123291\n",
      "Training loss:1.5331518650054932\n",
      "Training loss:1.5305206775665283\n",
      "Training loss:1.4551101922988892\n",
      "Training loss:1.4787310361862183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5007474422454834\n",
      "Training loss:1.5362763404846191\n",
      "Training loss:1.5749088525772095\n",
      "Training loss:1.5918914079666138\n",
      "Training loss:1.464200496673584\n",
      "Training loss:1.5359351634979248\n",
      "Training loss:1.514887809753418\n",
      "Training loss:1.4559149742126465\n",
      "Training loss:1.5259469747543335\n",
      "Training loss:1.5595645904541016\n",
      "Training loss:1.5121121406555176\n",
      "Training loss:1.4939627647399902\n",
      "Training loss:1.5045630931854248\n",
      "Training loss:1.5576316118240356\n",
      "Training loss:1.4791871309280396\n",
      "Training loss:1.479561448097229\n",
      "Training loss:1.4226698875427246\n",
      "Training loss:1.4920399188995361\n",
      "Training loss:1.5330942869186401\n",
      "Training loss:1.5748685598373413\n",
      "Training loss:1.5870357751846313\n",
      "Training loss:1.4740155935287476\n",
      "Training loss:1.5261203050613403\n",
      "Training loss:1.4831595420837402\n",
      "Training loss:1.4825501441955566\n",
      "Training loss:1.5928705930709839\n",
      "Training loss:1.4749051332473755\n",
      "Training loss:1.5424325466156006\n",
      "Training loss:1.467087745666504\n",
      "Training loss:1.4495463371276855\n",
      "Training loss:1.5407624244689941\n",
      "Training loss:1.4306710958480835\n",
      "Training loss:1.4778845310211182\n",
      "Training loss:1.4773380756378174\n",
      "Training loss:1.5301268100738525\n",
      "Training loss:1.5406328439712524\n",
      "Training loss:1.5890517234802246\n",
      "Training loss:1.4857194423675537\n",
      "Training loss:1.495497226715088\n",
      "Training loss:1.5870846509933472\n",
      "Training loss:1.487706184387207\n",
      "Training loss:1.6117606163024902\n",
      "Training loss:1.5239887237548828\n",
      "Training loss:1.5392768383026123\n",
      "Training loss:1.510035514831543\n",
      "Training loss:1.4727230072021484\n",
      "Training loss:1.489843487739563\n",
      "Training loss:1.5160696506500244\n",
      "Training loss:1.5085387229919434\n",
      "Training loss:1.4940946102142334\n",
      "Training loss:1.531661868095398\n",
      "Training loss:1.4595240354537964\n",
      "Epoch: 0142 loss_train: 142.490674 acc_train: 0.450604 loss_val: 17.996765 acc_val: 0.452500 time: 366049.034103s\n",
      "Training loss:1.522109866142273\n",
      "Training loss:1.4933338165283203\n",
      "Training loss:1.5559906959533691\n",
      "Training loss:1.5603851079940796\n",
      "Training loss:1.5830587148666382\n",
      "Training loss:1.516974687576294\n",
      "Training loss:1.5006815195083618\n",
      "Training loss:1.5625853538513184\n",
      "Training loss:1.4564968347549438\n",
      "Training loss:1.5950697660446167\n",
      "Training loss:1.5739184617996216\n",
      "Training loss:1.5891518592834473\n",
      "Training loss:1.4990222454071045\n",
      "Training loss:1.4633212089538574\n",
      "Training loss:1.5274404287338257\n",
      "Training loss:1.5280016660690308\n",
      "Training loss:1.4746782779693604\n",
      "Training loss:1.5591464042663574\n",
      "Training loss:1.541954517364502\n",
      "Training loss:1.51413893699646\n",
      "Training loss:1.5481542348861694\n",
      "Training loss:1.5118911266326904\n",
      "Training loss:1.5004526376724243\n",
      "Training loss:1.4873449802398682\n",
      "Training loss:1.5704950094223022\n",
      "Training loss:1.5917879343032837\n",
      "Training loss:1.5717531442642212\n",
      "Training loss:1.6231541633605957\n",
      "Training loss:1.5498144626617432\n",
      "Training loss:1.514689564704895\n",
      "Training loss:1.5275205373764038\n",
      "Training loss:1.6622225046157837\n",
      "Training loss:1.4931423664093018\n",
      "Training loss:1.5077886581420898\n",
      "Training loss:1.5681473016738892\n",
      "Training loss:1.5502514839172363\n",
      "Training loss:1.5103693008422852\n",
      "Training loss:1.4674824476242065\n",
      "Training loss:1.6220283508300781\n",
      "Training loss:1.5618308782577515\n",
      "Training loss:1.5585334300994873\n",
      "Training loss:1.4800875186920166\n",
      "Training loss:1.4489264488220215\n",
      "Training loss:1.5666989088058472\n",
      "Training loss:1.5115386247634888\n",
      "Training loss:1.4546946287155151\n",
      "Training loss:1.4954071044921875\n",
      "Training loss:1.5162737369537354\n",
      "Training loss:1.541746735572815\n",
      "Training loss:1.4817098379135132\n",
      "Training loss:1.5257567167282104\n",
      "Training loss:1.5236619710922241\n",
      "Training loss:1.4744093418121338\n",
      "Training loss:1.533042550086975\n",
      "Training loss:1.5113369226455688\n",
      "Training loss:1.4582421779632568\n",
      "Training loss:1.5084789991378784\n",
      "Training loss:1.4208104610443115\n",
      "Training loss:1.582552194595337\n",
      "Training loss:1.489353895187378\n",
      "Training loss:1.5571526288986206\n",
      "Training loss:1.5094952583312988\n",
      "Training loss:1.5801148414611816\n",
      "Training loss:1.5766593217849731\n",
      "Training loss:1.458939552307129\n",
      "Training loss:1.5069224834442139\n",
      "Training loss:1.4969921112060547\n",
      "Training loss:1.5665227174758911\n",
      "Training loss:1.4246444702148438\n",
      "Training loss:1.4956055879592896\n",
      "Training loss:1.5406394004821777\n",
      "Training loss:1.610590934753418\n",
      "Training loss:1.4807621240615845\n",
      "Training loss:1.5192686319351196\n",
      "Training loss:1.5743976831436157\n",
      "Training loss:1.4667654037475586\n",
      "Training loss:1.5193994045257568\n",
      "Training loss:1.5057764053344727\n",
      "Training loss:1.513332724571228\n",
      "Training loss:1.5348541736602783\n",
      "Training loss:1.4462065696716309\n",
      "Training loss:1.512492060661316\n",
      "Training loss:1.509403944015503\n",
      "Training loss:1.5323057174682617\n",
      "Training loss:1.4752446413040161\n",
      "Training loss:1.552687168121338\n",
      "Training loss:1.48110830783844\n",
      "Training loss:1.530832052230835\n",
      "Training loss:1.4797557592391968\n",
      "Training loss:1.6273853778839111\n",
      "Training loss:1.5089607238769531\n",
      "Training loss:1.5336664915084839\n",
      "Training loss:1.5139209032058716\n",
      "Training loss:1.4617072343826294\n",
      "Epoch: 0143 loss_train: 143.245556 acc_train: 0.445833 loss_val: 18.076169 acc_val: 0.449667 time: 368738.855711s\n",
      "Training loss:1.5308364629745483\n",
      "Training loss:1.4674797058105469\n",
      "Training loss:1.5471140146255493\n",
      "Training loss:1.485474705696106\n",
      "Training loss:1.4876118898391724\n",
      "Training loss:1.5882400274276733\n",
      "Training loss:1.4997690916061401\n",
      "Training loss:1.6219810247421265\n",
      "Training loss:1.5280137062072754\n",
      "Training loss:1.6176824569702148\n",
      "Training loss:1.568609595298767\n",
      "Training loss:1.5290287733078003\n",
      "Training loss:1.5233298540115356\n",
      "Training loss:1.5104570388793945\n",
      "Training loss:1.5041245222091675\n",
      "Training loss:1.5074174404144287\n",
      "Training loss:1.4793670177459717\n",
      "Training loss:1.5115554332733154\n",
      "Training loss:1.5423734188079834\n",
      "Training loss:1.5049203634262085\n",
      "Training loss:1.5589470863342285\n",
      "Training loss:1.4421772956848145\n",
      "Training loss:1.5726649761199951\n",
      "Training loss:1.5210456848144531\n",
      "Training loss:1.486311435699463\n",
      "Training loss:1.5662473440170288\n",
      "Training loss:1.4475923776626587\n",
      "Training loss:1.439265251159668\n",
      "Training loss:1.532044768333435\n",
      "Training loss:1.5334081649780273\n",
      "Training loss:1.522666573524475\n",
      "Training loss:1.4757071733474731\n",
      "Training loss:1.5008255243301392\n",
      "Training loss:1.5216119289398193\n",
      "Training loss:1.5654122829437256\n",
      "Training loss:1.4309908151626587\n",
      "Training loss:1.5693696737289429\n",
      "Training loss:1.57820725440979\n",
      "Training loss:1.499830722808838\n",
      "Training loss:1.5092291831970215\n",
      "Training loss:1.4857250452041626\n",
      "Training loss:1.4795472621917725\n",
      "Training loss:1.4660017490386963\n",
      "Training loss:1.5374724864959717\n",
      "Training loss:1.5883280038833618\n",
      "Training loss:1.4928961992263794\n",
      "Training loss:1.50480055809021\n",
      "Training loss:1.4698725938796997\n",
      "Training loss:1.5075267553329468\n",
      "Training loss:1.5081305503845215\n",
      "Training loss:1.4273300170898438\n",
      "Training loss:1.5505082607269287\n",
      "Training loss:1.4445297718048096\n",
      "Training loss:1.5478005409240723\n",
      "Training loss:1.480089545249939\n",
      "Training loss:1.571767807006836\n",
      "Training loss:1.4970054626464844\n",
      "Training loss:1.5070408582687378\n",
      "Training loss:1.560948133468628\n",
      "Training loss:1.5946238040924072\n",
      "Training loss:1.5670750141143799\n",
      "Training loss:1.5117608308792114\n",
      "Training loss:1.438792109489441\n",
      "Training loss:1.5254085063934326\n",
      "Training loss:1.5623725652694702\n",
      "Training loss:1.5247414112091064\n",
      "Training loss:1.5765235424041748\n",
      "Training loss:1.4939508438110352\n",
      "Training loss:1.5150753259658813\n",
      "Training loss:1.5232832431793213\n",
      "Training loss:1.5153926610946655\n",
      "Training loss:1.5273901224136353\n",
      "Training loss:1.4925378561019897\n",
      "Training loss:1.459519863128662\n",
      "Training loss:1.4764535427093506\n",
      "Training loss:1.465990424156189\n",
      "Training loss:1.5240159034729004\n",
      "Training loss:1.562559962272644\n",
      "Training loss:1.5421935319900513\n",
      "Training loss:1.4611200094223022\n",
      "Training loss:1.529082179069519\n",
      "Training loss:1.5148566961288452\n",
      "Training loss:1.4992907047271729\n",
      "Training loss:1.4620521068572998\n",
      "Training loss:1.5131062269210815\n",
      "Training loss:1.4944918155670166\n",
      "Training loss:1.5120192766189575\n",
      "Training loss:1.5221920013427734\n",
      "Training loss:1.4857020378112793\n",
      "Training loss:1.558924674987793\n",
      "Training loss:1.4580528736114502\n",
      "Training loss:1.542112112045288\n",
      "Training loss:1.4912824630737305\n",
      "Training loss:1.6032071113586426\n",
      "Epoch: 0144 loss_train: 142.503417 acc_train: 0.450458 loss_val: 18.079813 acc_val: 0.447000 time: 371318.278694s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.4255884885787964\n",
      "Training loss:1.517500877380371\n",
      "Training loss:1.5207575559616089\n",
      "Training loss:1.5160102844238281\n",
      "Training loss:1.5452094078063965\n",
      "Training loss:1.55582857131958\n",
      "Training loss:1.5584440231323242\n",
      "Training loss:1.4781326055526733\n",
      "Training loss:1.5014653205871582\n",
      "Training loss:1.5413167476654053\n",
      "Training loss:1.5230892896652222\n",
      "Training loss:1.5607661008834839\n",
      "Training loss:1.5539661645889282\n",
      "Training loss:1.5053826570510864\n",
      "Training loss:1.4957630634307861\n",
      "Training loss:1.5223435163497925\n",
      "Training loss:1.5228594541549683\n",
      "Training loss:1.4822012186050415\n",
      "Training loss:1.5133397579193115\n",
      "Training loss:1.54397714138031\n",
      "Training loss:1.500767469406128\n",
      "Training loss:1.5457288026809692\n",
      "Training loss:1.5584099292755127\n",
      "Training loss:1.551285743713379\n",
      "Training loss:1.5307869911193848\n",
      "Training loss:1.5645536184310913\n",
      "Training loss:1.4772119522094727\n",
      "Training loss:1.5646213293075562\n",
      "Training loss:1.5501456260681152\n",
      "Training loss:1.4889158010482788\n",
      "Training loss:1.5476627349853516\n",
      "Training loss:1.4619141817092896\n",
      "Training loss:1.5238596200942993\n",
      "Training loss:1.5572508573532104\n",
      "Training loss:1.4685684442520142\n",
      "Training loss:1.5850539207458496\n",
      "Training loss:1.5456137657165527\n",
      "Training loss:1.5966932773590088\n",
      "Training loss:1.6136817932128906\n",
      "Training loss:1.4850441217422485\n",
      "Training loss:1.4492573738098145\n",
      "Training loss:1.5007885694503784\n",
      "Training loss:1.5558898448944092\n",
      "Training loss:1.5420926809310913\n",
      "Training loss:1.582254409790039\n",
      "Training loss:1.546485424041748\n",
      "Training loss:1.5257301330566406\n",
      "Training loss:1.524638295173645\n",
      "Training loss:1.4820237159729004\n",
      "Training loss:1.513236403465271\n",
      "Training loss:1.5268796682357788\n",
      "Training loss:1.5206483602523804\n",
      "Training loss:1.5285133123397827\n",
      "Training loss:1.507615327835083\n",
      "Training loss:1.5431710481643677\n",
      "Training loss:1.4714831113815308\n",
      "Training loss:1.5017160177230835\n",
      "Training loss:1.4555093050003052\n",
      "Training loss:1.5104761123657227\n",
      "Training loss:1.4890035390853882\n",
      "Training loss:1.5177780389785767\n",
      "Training loss:1.5545735359191895\n",
      "Training loss:1.5116163492202759\n",
      "Training loss:1.4400854110717773\n",
      "Training loss:1.44046950340271\n",
      "Training loss:1.5184797048568726\n",
      "Training loss:1.5851372480392456\n",
      "Training loss:1.4303758144378662\n",
      "Training loss:1.512351632118225\n",
      "Training loss:1.5338271856307983\n",
      "Training loss:1.4400922060012817\n",
      "Training loss:1.5031486749649048\n",
      "Training loss:1.401444435119629\n",
      "Training loss:1.427403211593628\n",
      "Training loss:1.455600380897522\n",
      "Training loss:1.4893364906311035\n",
      "Training loss:1.536069631576538\n",
      "Training loss:1.5048104524612427\n",
      "Training loss:1.4935396909713745\n",
      "Training loss:1.5260435342788696\n",
      "Training loss:1.5055019855499268\n",
      "Training loss:1.4426020383834839\n",
      "Training loss:1.4641398191452026\n",
      "Training loss:1.558640956878662\n",
      "Training loss:1.519652009010315\n",
      "Training loss:1.500836968421936\n",
      "Training loss:1.5077705383300781\n",
      "Training loss:1.475466251373291\n",
      "Training loss:1.4836326837539673\n",
      "Training loss:1.4877398014068604\n",
      "Training loss:1.537158727645874\n",
      "Training loss:1.486172080039978\n",
      "Training loss:1.545952558517456\n",
      "Training loss:1.497544765472412\n",
      "Epoch: 0145 loss_train: 142.214119 acc_train: 0.450417 loss_val: 18.142372 acc_val: 0.448500 time: 373880.628860s\n",
      "Training loss:1.5570414066314697\n",
      "Training loss:1.493986964225769\n",
      "Training loss:1.4895614385604858\n",
      "Training loss:1.544935941696167\n",
      "Training loss:1.4417718648910522\n",
      "Training loss:1.5164703130722046\n",
      "Training loss:1.5389212369918823\n",
      "Training loss:1.431016206741333\n",
      "Training loss:1.5845637321472168\n",
      "Training loss:1.5248275995254517\n",
      "Training loss:1.5105345249176025\n",
      "Training loss:1.5424703359603882\n",
      "Training loss:1.4986861944198608\n",
      "Training loss:1.4212185144424438\n",
      "Training loss:1.5238879919052124\n",
      "Training loss:1.5719467401504517\n",
      "Training loss:1.5109769105911255\n",
      "Training loss:1.5113118886947632\n",
      "Training loss:1.5155397653579712\n",
      "Training loss:1.5757434368133545\n",
      "Training loss:1.4602800607681274\n",
      "Training loss:1.5294888019561768\n",
      "Training loss:1.5374470949172974\n",
      "Training loss:1.493951439857483\n",
      "Training loss:1.527434229850769\n",
      "Training loss:1.4703261852264404\n",
      "Training loss:1.3933948278427124\n",
      "Training loss:1.470069408416748\n",
      "Training loss:1.460654854774475\n",
      "Training loss:1.5370652675628662\n",
      "Training loss:1.4867689609527588\n",
      "Training loss:1.540906548500061\n",
      "Training loss:1.4824600219726562\n",
      "Training loss:1.562361478805542\n",
      "Training loss:1.4621495008468628\n",
      "Training loss:1.407604455947876\n",
      "Training loss:1.5256218910217285\n",
      "Training loss:1.5410704612731934\n",
      "Training loss:1.5019488334655762\n",
      "Training loss:1.538572907447815\n",
      "Training loss:1.5474557876586914\n",
      "Training loss:1.4330108165740967\n",
      "Training loss:1.5762302875518799\n",
      "Training loss:1.4972519874572754\n",
      "Training loss:1.4167847633361816\n",
      "Training loss:1.542873501777649\n",
      "Training loss:1.5095385313034058\n",
      "Training loss:1.4253402948379517\n",
      "Training loss:1.5397655963897705\n",
      "Training loss:1.553770661354065\n",
      "Training loss:1.5591175556182861\n",
      "Training loss:1.4797804355621338\n",
      "Training loss:1.5375256538391113\n",
      "Training loss:1.5600440502166748\n",
      "Training loss:1.4630184173583984\n",
      "Training loss:1.5721365213394165\n",
      "Training loss:1.42815101146698\n",
      "Training loss:1.5156351327896118\n",
      "Training loss:1.4921447038650513\n",
      "Training loss:1.597672462463379\n",
      "Training loss:1.4477096796035767\n",
      "Training loss:1.491550326347351\n",
      "Training loss:1.5392920970916748\n",
      "Training loss:1.5529824495315552\n",
      "Training loss:1.5290262699127197\n",
      "Training loss:1.4782445430755615\n",
      "Training loss:1.5293608903884888\n",
      "Training loss:1.5909591913223267\n",
      "Training loss:1.484852910041809\n",
      "Training loss:1.6000893115997314\n",
      "Training loss:1.4574819803237915\n",
      "Training loss:1.456138014793396\n",
      "Training loss:1.5929545164108276\n",
      "Training loss:1.48782479763031\n",
      "Training loss:1.4507077932357788\n",
      "Training loss:1.5955530405044556\n",
      "Training loss:1.551571011543274\n",
      "Training loss:1.521079421043396\n",
      "Training loss:1.4645771980285645\n",
      "Training loss:1.5726615190505981\n",
      "Training loss:1.4797929525375366\n",
      "Training loss:1.5274767875671387\n",
      "Training loss:1.5389058589935303\n",
      "Training loss:1.5780583620071411\n",
      "Training loss:1.558727741241455\n",
      "Training loss:1.5679155588150024\n",
      "Training loss:1.4361969232559204\n",
      "Training loss:1.5315356254577637\n",
      "Training loss:1.5576006174087524\n",
      "Training loss:1.4763000011444092\n",
      "Training loss:1.484627604484558\n",
      "Training loss:1.5523134469985962\n",
      "Training loss:1.5099244117736816\n",
      "Training loss:1.5623763799667358\n",
      "Epoch: 0146 loss_train: 142.238608 acc_train: 0.448750 loss_val: 18.054896 acc_val: 0.451833 time: 376442.467892s\n",
      "Training loss:1.4531835317611694\n",
      "Training loss:1.5053552389144897\n",
      "Training loss:1.5338128805160522\n",
      "Training loss:1.5430078506469727\n",
      "Training loss:1.4885486364364624\n",
      "Training loss:1.557019829750061\n",
      "Training loss:1.532692551612854\n",
      "Training loss:1.504211187362671\n",
      "Training loss:1.5068860054016113\n",
      "Training loss:1.5182151794433594\n",
      "Training loss:1.4388976097106934\n",
      "Training loss:1.6269103288650513\n",
      "Training loss:1.4838277101516724\n",
      "Training loss:1.5167955160140991\n",
      "Training loss:1.4495527744293213\n",
      "Training loss:1.4900726079940796\n",
      "Training loss:1.514664649963379\n",
      "Training loss:1.4797884225845337\n",
      "Training loss:1.4305520057678223\n",
      "Training loss:1.4765323400497437\n",
      "Training loss:1.6015971899032593\n",
      "Training loss:1.4614924192428589\n",
      "Training loss:1.4924204349517822\n",
      "Training loss:1.4708271026611328\n",
      "Training loss:1.4712175130844116\n",
      "Training loss:1.5835802555084229\n",
      "Training loss:1.5587788820266724\n",
      "Training loss:1.4837135076522827\n",
      "Training loss:1.496275544166565\n",
      "Training loss:1.554886817932129\n",
      "Training loss:1.5970367193222046\n",
      "Training loss:1.5264092683792114\n",
      "Training loss:1.5141441822052002\n",
      "Training loss:1.59012770652771\n",
      "Training loss:1.524672031402588\n",
      "Training loss:1.4320709705352783\n",
      "Training loss:1.560771107673645\n",
      "Training loss:1.5327986478805542\n",
      "Training loss:1.5415488481521606\n",
      "Training loss:1.5346391201019287\n",
      "Training loss:1.5776021480560303\n",
      "Training loss:1.4855234622955322\n",
      "Training loss:1.554327130317688\n",
      "Training loss:1.480949878692627\n",
      "Training loss:1.5320767164230347\n",
      "Training loss:1.5679817199707031\n",
      "Training loss:1.480933666229248\n",
      "Training loss:1.5090696811676025\n",
      "Training loss:1.5285096168518066\n",
      "Training loss:1.4903639554977417\n",
      "Training loss:1.5181686878204346\n",
      "Training loss:1.5048387050628662\n",
      "Training loss:1.528383493423462\n",
      "Training loss:1.5093523263931274\n",
      "Training loss:1.5458441972732544\n",
      "Training loss:1.4761459827423096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5394471883773804\n",
      "Training loss:1.4702118635177612\n",
      "Training loss:1.4933035373687744\n",
      "Training loss:1.4865111112594604\n",
      "Training loss:1.5158345699310303\n",
      "Training loss:1.5107086896896362\n",
      "Training loss:1.4359428882598877\n",
      "Training loss:1.471647024154663\n",
      "Training loss:1.4377264976501465\n",
      "Training loss:1.4892024993896484\n",
      "Training loss:1.5941590070724487\n",
      "Training loss:1.5520901679992676\n",
      "Training loss:1.550377368927002\n",
      "Training loss:1.496754765510559\n",
      "Training loss:1.5562429428100586\n",
      "Training loss:1.481330156326294\n",
      "Training loss:1.506898045539856\n",
      "Training loss:1.4571846723556519\n",
      "Training loss:1.511262059211731\n",
      "Training loss:1.5480222702026367\n",
      "Training loss:1.4748562574386597\n",
      "Training loss:1.4834991693496704\n",
      "Training loss:1.5733140707015991\n",
      "Training loss:1.5534368753433228\n",
      "Training loss:1.467069149017334\n",
      "Training loss:1.55733060836792\n",
      "Training loss:1.4750159978866577\n",
      "Training loss:1.4735323190689087\n",
      "Training loss:1.5080220699310303\n",
      "Training loss:1.4972031116485596\n",
      "Training loss:1.581447720527649\n",
      "Training loss:1.4706372022628784\n",
      "Training loss:1.5228054523468018\n",
      "Training loss:1.5314706563949585\n",
      "Training loss:1.514243483543396\n",
      "Training loss:1.4617180824279785\n",
      "Training loss:1.5592257976531982\n",
      "Training loss:1.4512405395507812\n",
      "Epoch: 0147 loss_train: 142.130532 acc_train: 0.450833 loss_val: 17.900607 acc_val: 0.453833 time: 379188.964139s\n",
      "Training loss:1.4245774745941162\n",
      "Training loss:1.5138790607452393\n",
      "Training loss:1.5571942329406738\n",
      "Training loss:1.4464417695999146\n",
      "Training loss:1.527665615081787\n",
      "Training loss:1.5592310428619385\n",
      "Training loss:1.5413659811019897\n",
      "Training loss:1.551785945892334\n",
      "Training loss:1.5220400094985962\n",
      "Training loss:1.5422759056091309\n",
      "Training loss:1.4676103591918945\n",
      "Training loss:1.5376393795013428\n",
      "Training loss:1.5455387830734253\n",
      "Training loss:1.5557996034622192\n",
      "Training loss:1.510074496269226\n",
      "Training loss:1.489073395729065\n",
      "Training loss:1.4636162519454956\n",
      "Training loss:1.5307904481887817\n",
      "Training loss:1.5220993757247925\n",
      "Training loss:1.4744445085525513\n",
      "Training loss:1.5191926956176758\n",
      "Training loss:1.520604133605957\n",
      "Training loss:1.5609344244003296\n",
      "Training loss:1.5395163297653198\n",
      "Training loss:1.5775762796401978\n",
      "Training loss:1.520075798034668\n",
      "Training loss:1.5690460205078125\n",
      "Training loss:1.5129748582839966\n",
      "Training loss:1.534988284111023\n",
      "Training loss:1.4383851289749146\n",
      "Training loss:1.4408429861068726\n",
      "Training loss:1.4633738994598389\n",
      "Training loss:1.5995211601257324\n",
      "Training loss:1.5215349197387695\n",
      "Training loss:1.579949975013733\n",
      "Training loss:1.489923357963562\n",
      "Training loss:1.5041844844818115\n",
      "Training loss:1.508490800857544\n",
      "Training loss:1.593173623085022\n",
      "Training loss:1.497132658958435\n",
      "Training loss:1.5640743970870972\n",
      "Training loss:1.5472934246063232\n",
      "Training loss:1.5570476055145264\n",
      "Training loss:1.431286096572876\n",
      "Training loss:1.5243874788284302\n",
      "Training loss:1.4892027378082275\n",
      "Training loss:1.5547949075698853\n",
      "Training loss:1.5013103485107422\n",
      "Training loss:1.5455124378204346\n",
      "Training loss:1.5956354141235352\n",
      "Training loss:1.3841615915298462\n",
      "Training loss:1.4673045873641968\n",
      "Training loss:1.5587971210479736\n",
      "Training loss:1.4622660875320435\n",
      "Training loss:1.484838843345642\n",
      "Training loss:1.5186772346496582\n",
      "Training loss:1.5481083393096924\n",
      "Training loss:1.5062432289123535\n",
      "Training loss:1.5913394689559937\n",
      "Training loss:1.4814255237579346\n",
      "Training loss:1.6322828531265259\n",
      "Training loss:1.5331820249557495\n",
      "Training loss:1.4652587175369263\n",
      "Training loss:1.5127828121185303\n",
      "Training loss:1.50033700466156\n",
      "Training loss:1.5554723739624023\n",
      "Training loss:1.4716978073120117\n",
      "Training loss:1.4778941869735718\n",
      "Training loss:1.5507323741912842\n",
      "Training loss:1.5435172319412231\n",
      "Training loss:1.4760737419128418\n",
      "Training loss:1.504577398300171\n",
      "Training loss:1.531496286392212\n",
      "Training loss:1.4425294399261475\n",
      "Training loss:1.5591411590576172\n",
      "Training loss:1.560746669769287\n",
      "Training loss:1.551988124847412\n",
      "Training loss:1.510385274887085\n",
      "Training loss:1.5183571577072144\n",
      "Training loss:1.6297907829284668\n",
      "Training loss:1.600640892982483\n",
      "Training loss:1.4999395608901978\n",
      "Training loss:1.4733375310897827\n",
      "Training loss:1.5334181785583496\n",
      "Training loss:1.4853898286819458\n",
      "Training loss:1.5432188510894775\n",
      "Training loss:1.4992235898971558\n",
      "Training loss:1.5208158493041992\n",
      "Training loss:1.5483280420303345\n",
      "Training loss:1.514301061630249\n",
      "Training loss:1.5121475458145142\n",
      "Training loss:1.4267585277557373\n",
      "Training loss:1.5798859596252441\n",
      "Training loss:1.426376461982727\n",
      "Epoch: 0148 loss_train: 142.780298 acc_train: 0.447354 loss_val: 18.394862 acc_val: 0.438333 time: 383029.589172s\n",
      "Training loss:1.4756118059158325\n",
      "Training loss:1.528396487236023\n",
      "Training loss:1.5055400133132935\n",
      "Training loss:1.6033574342727661\n",
      "Training loss:1.4944953918457031\n",
      "Training loss:1.5733156204223633\n",
      "Training loss:1.5783149003982544\n",
      "Training loss:1.5602786540985107\n",
      "Training loss:1.5076448917388916\n",
      "Training loss:1.6116323471069336\n",
      "Training loss:1.6132479906082153\n",
      "Training loss:1.57256281375885\n",
      "Training loss:1.5321341753005981\n",
      "Training loss:1.5610575675964355\n",
      "Training loss:1.5554239749908447\n",
      "Training loss:1.499276041984558\n",
      "Training loss:1.5205897092819214\n",
      "Training loss:1.486203670501709\n",
      "Training loss:1.5557403564453125\n",
      "Training loss:1.4636614322662354\n",
      "Training loss:1.547451138496399\n",
      "Training loss:1.4962824583053589\n",
      "Training loss:1.47386634349823\n",
      "Training loss:1.529161810874939\n",
      "Training loss:1.4801886081695557\n",
      "Training loss:1.4785584211349487\n",
      "Training loss:1.5381159782409668\n",
      "Training loss:1.4560009241104126\n",
      "Training loss:1.4671213626861572\n",
      "Training loss:1.5360159873962402\n",
      "Training loss:1.5162787437438965\n",
      "Training loss:1.5966495275497437\n",
      "Training loss:1.5473570823669434\n",
      "Training loss:1.5545061826705933\n",
      "Training loss:1.5015634298324585\n",
      "Training loss:1.5602055788040161\n",
      "Training loss:1.5882885456085205\n",
      "Training loss:1.425583839416504\n",
      "Training loss:1.4204503297805786\n",
      "Training loss:1.5241994857788086\n",
      "Training loss:1.522886872291565\n",
      "Training loss:1.4473246335983276\n",
      "Training loss:1.5060031414031982\n",
      "Training loss:1.449666976928711\n",
      "Training loss:1.5079981088638306\n",
      "Training loss:1.5080516338348389\n",
      "Training loss:1.3665635585784912\n",
      "Training loss:1.523276925086975\n",
      "Training loss:1.583216905593872\n",
      "Training loss:1.486103892326355\n",
      "Training loss:1.4910876750946045\n",
      "Training loss:1.5196301937103271\n",
      "Training loss:1.575200080871582\n",
      "Training loss:1.528698444366455\n",
      "Training loss:1.462173342704773\n",
      "Training loss:1.5445646047592163\n",
      "Training loss:1.5337330102920532\n",
      "Training loss:1.5042853355407715\n",
      "Training loss:1.5050684213638306\n",
      "Training loss:1.4960793256759644\n",
      "Training loss:1.4511638879776\n",
      "Training loss:1.50180983543396\n",
      "Training loss:1.5531283617019653\n",
      "Training loss:1.4720090627670288\n",
      "Training loss:1.4597781896591187\n",
      "Training loss:1.5320717096328735\n",
      "Training loss:1.5226185321807861\n",
      "Training loss:1.5397851467132568\n",
      "Training loss:1.5104159116744995\n",
      "Training loss:1.3960508108139038\n",
      "Training loss:1.5545570850372314\n",
      "Training loss:1.5312601327896118\n",
      "Training loss:1.5926721096038818\n",
      "Training loss:1.5474425554275513\n",
      "Training loss:1.565280556678772\n",
      "Training loss:1.548014760017395\n",
      "Training loss:1.4448024034500122\n",
      "Training loss:1.5278828144073486\n",
      "Training loss:1.482678771018982\n",
      "Training loss:1.5105869770050049\n",
      "Training loss:1.5263811349868774\n",
      "Training loss:1.5141205787658691\n",
      "Training loss:1.55574369430542\n",
      "Training loss:1.511144995689392\n",
      "Training loss:1.5440887212753296\n",
      "Training loss:1.4375243186950684\n",
      "Training loss:1.5091537237167358\n",
      "Training loss:1.5185612440109253\n",
      "Training loss:1.5505620241165161\n",
      "Training loss:1.5116137266159058\n",
      "Training loss:1.5088083744049072\n",
      "Training loss:1.5381624698638916\n",
      "Training loss:1.4526137113571167\n",
      "Training loss:1.5735639333724976\n",
      "Epoch: 0149 loss_train: 142.593996 acc_train: 0.446979 loss_val: 17.945082 acc_val: 0.457500 time: 385611.174597s\n",
      "Training loss:1.5376237630844116\n",
      "Training loss:1.5057296752929688\n",
      "Training loss:1.5270358324050903\n",
      "Training loss:1.4505268335342407\n",
      "Training loss:1.5036143064498901\n",
      "Training loss:1.494033694267273\n",
      "Training loss:1.5377399921417236\n",
      "Training loss:1.5185104608535767\n",
      "Training loss:1.4976489543914795\n",
      "Training loss:1.5994702577590942\n",
      "Training loss:1.5418401956558228\n",
      "Training loss:1.544858455657959\n",
      "Training loss:1.5258545875549316\n",
      "Training loss:1.4146634340286255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5201256275177002\n",
      "Training loss:1.5190386772155762\n",
      "Training loss:1.510538935661316\n",
      "Training loss:1.5686559677124023\n",
      "Training loss:1.5300155878067017\n",
      "Training loss:1.4841642379760742\n",
      "Training loss:1.397087574005127\n",
      "Training loss:1.4278433322906494\n",
      "Training loss:1.5526351928710938\n",
      "Training loss:1.4530154466629028\n",
      "Training loss:1.5563101768493652\n",
      "Training loss:1.4713441133499146\n",
      "Training loss:1.4474577903747559\n",
      "Training loss:1.5745415687561035\n",
      "Training loss:1.5601273775100708\n",
      "Training loss:1.5675939321517944\n",
      "Training loss:1.473618984222412\n",
      "Training loss:1.453283667564392\n",
      "Training loss:1.4649772644042969\n",
      "Training loss:1.5436640977859497\n",
      "Training loss:1.4728282690048218\n",
      "Training loss:1.621134638786316\n",
      "Training loss:1.468650460243225\n",
      "Training loss:1.567246675491333\n",
      "Training loss:1.6001023054122925\n",
      "Training loss:1.4401870965957642\n",
      "Training loss:1.512446641921997\n",
      "Training loss:1.5626764297485352\n",
      "Training loss:1.5733698606491089\n",
      "Training loss:1.4230142831802368\n",
      "Training loss:1.4923404455184937\n",
      "Training loss:1.5408806800842285\n",
      "Training loss:1.5296379327774048\n",
      "Training loss:1.451775312423706\n",
      "Training loss:1.4652349948883057\n",
      "Training loss:1.4946457147598267\n",
      "Training loss:1.475334644317627\n",
      "Training loss:1.4982081651687622\n",
      "Training loss:1.4448950290679932\n",
      "Training loss:1.4954293966293335\n",
      "Training loss:1.4667348861694336\n",
      "Training loss:1.5573025941848755\n",
      "Training loss:1.4916412830352783\n",
      "Training loss:1.4978219270706177\n",
      "Training loss:1.466496229171753\n",
      "Training loss:1.46314537525177\n",
      "Training loss:1.4869664907455444\n",
      "Training loss:1.6207753419876099\n",
      "Training loss:1.5200190544128418\n",
      "Training loss:1.4808084964752197\n",
      "Training loss:1.5773353576660156\n",
      "Training loss:1.5146119594573975\n",
      "Training loss:1.5198612213134766\n",
      "Training loss:1.5774190425872803\n",
      "Training loss:1.5552372932434082\n",
      "Training loss:1.4767993688583374\n",
      "Training loss:1.5385690927505493\n",
      "Training loss:1.5262342691421509\n",
      "Training loss:1.5206100940704346\n",
      "Training loss:1.550229549407959\n",
      "Training loss:1.5048699378967285\n",
      "Training loss:1.4877318143844604\n",
      "Training loss:1.5261714458465576\n",
      "Training loss:1.4644464254379272\n",
      "Training loss:1.4673327207565308\n",
      "Training loss:1.5506178140640259\n",
      "Training loss:1.4608045816421509\n",
      "Training loss:1.4989362955093384\n",
      "Training loss:1.4310393333435059\n",
      "Training loss:1.4648929834365845\n",
      "Training loss:1.490774154663086\n",
      "Training loss:1.4759514331817627\n",
      "Training loss:1.418300747871399\n",
      "Training loss:1.443603754043579\n",
      "Training loss:1.477781891822815\n",
      "Training loss:1.532381534576416\n",
      "Training loss:1.5201066732406616\n",
      "Training loss:1.5421154499053955\n",
      "Training loss:1.5331541299819946\n",
      "Training loss:1.4122799634933472\n",
      "Epoch: 0150 loss_train: 141.517141 acc_train: 0.453792 loss_val: 17.999912 acc_val: 0.453333 time: 388064.884372s\n",
      "Training loss:1.5913547277450562\n",
      "Training loss:1.61396062374115\n",
      "Training loss:1.5376311540603638\n",
      "Training loss:1.5476537942886353\n",
      "Training loss:1.388412356376648\n",
      "Training loss:1.4506016969680786\n",
      "Training loss:1.5095088481903076\n",
      "Training loss:1.551404356956482\n",
      "Training loss:1.5632792711257935\n",
      "Training loss:1.5963895320892334\n",
      "Training loss:1.4848740100860596\n",
      "Training loss:1.5153470039367676\n",
      "Training loss:1.5090407133102417\n",
      "Training loss:1.4576613903045654\n",
      "Training loss:1.6039994955062866\n",
      "Training loss:1.4343515634536743\n",
      "Training loss:1.542189359664917\n",
      "Training loss:1.4580241441726685\n",
      "Training loss:1.4895482063293457\n",
      "Training loss:1.507058024406433\n",
      "Training loss:1.5048502683639526\n",
      "Training loss:1.5663923025131226\n",
      "Training loss:1.4865388870239258\n",
      "Training loss:1.5447331666946411\n",
      "Training loss:1.4754738807678223\n",
      "Training loss:1.4867709875106812\n",
      "Training loss:1.5361948013305664\n",
      "Training loss:1.5035812854766846\n",
      "Training loss:1.5140529870986938\n",
      "Training loss:1.5457072257995605\n",
      "Training loss:1.4751150608062744\n",
      "Training loss:1.5334734916687012\n",
      "Training loss:1.5215193033218384\n",
      "Training loss:1.5208712816238403\n",
      "Training loss:1.4942293167114258\n",
      "Training loss:1.5605404376983643\n",
      "Training loss:1.5017473697662354\n",
      "Training loss:1.5107653141021729\n",
      "Training loss:1.431456446647644\n",
      "Training loss:1.4645558595657349\n",
      "Training loss:1.523748755455017\n",
      "Training loss:1.5757801532745361\n",
      "Training loss:1.5384193658828735\n",
      "Training loss:1.5260376930236816\n",
      "Training loss:1.5125313997268677\n",
      "Training loss:1.4872658252716064\n",
      "Training loss:1.5482984781265259\n",
      "Training loss:1.4828152656555176\n",
      "Training loss:1.453674077987671\n",
      "Training loss:1.4632227420806885\n",
      "Training loss:1.4981300830841064\n",
      "Training loss:1.4557323455810547\n",
      "Training loss:1.5278127193450928\n",
      "Training loss:1.5904122591018677\n",
      "Training loss:1.4542450904846191\n",
      "Training loss:1.5049374103546143\n",
      "Training loss:1.514712929725647\n",
      "Training loss:1.4423766136169434\n",
      "Training loss:1.4567357301712036\n",
      "Training loss:1.4802204370498657\n",
      "Training loss:1.4592204093933105\n",
      "Training loss:1.553297758102417\n",
      "Training loss:1.5165343284606934\n",
      "Training loss:1.582115888595581\n",
      "Training loss:1.4800108671188354\n",
      "Training loss:1.4932531118392944\n",
      "Training loss:1.5142297744750977\n",
      "Training loss:1.4410210847854614\n",
      "Training loss:1.513468623161316\n",
      "Training loss:1.4692472219467163\n",
      "Training loss:1.5751307010650635\n",
      "Training loss:1.4923427104949951\n",
      "Training loss:1.5293244123458862\n",
      "Training loss:1.5301637649536133\n",
      "Training loss:1.5173312425613403\n",
      "Training loss:1.4923752546310425\n",
      "Training loss:1.538448691368103\n",
      "Training loss:1.527281403541565\n",
      "Training loss:1.4926193952560425\n",
      "Training loss:1.4197088479995728\n",
      "Training loss:1.5350161790847778\n",
      "Training loss:1.5238285064697266\n",
      "Training loss:1.472265601158142\n",
      "Training loss:1.5066579580307007\n",
      "Training loss:1.5341933965682983\n",
      "Training loss:1.5178282260894775\n",
      "Training loss:1.4537451267242432\n",
      "Training loss:1.538712501525879\n",
      "Training loss:1.4415366649627686\n",
      "Training loss:1.4912678003311157\n",
      "Training loss:1.5203838348388672\n",
      "Training loss:1.495748519897461\n",
      "Training loss:1.6089555025100708\n",
      "Training loss:1.4610811471939087\n",
      "Epoch: 0151 loss_train: 141.806316 acc_train: 0.451750 loss_val: 17.916452 acc_val: 0.455167 time: 390505.462013s\n",
      "Training loss:1.497585415840149\n",
      "Training loss:1.5610744953155518\n",
      "Training loss:1.5141890048980713\n",
      "Training loss:1.578265905380249\n",
      "Training loss:1.5346283912658691\n",
      "Training loss:1.4967600107192993\n",
      "Training loss:1.5283477306365967\n",
      "Training loss:1.5392142534255981\n",
      "Training loss:1.5265765190124512\n",
      "Training loss:1.4828858375549316\n",
      "Training loss:1.582513451576233\n",
      "Training loss:1.4713773727416992\n",
      "Training loss:1.4990531206130981\n",
      "Training loss:1.5030659437179565\n",
      "Training loss:1.5567677021026611\n",
      "Training loss:1.5457512140274048\n",
      "Training loss:1.5241414308547974\n",
      "Training loss:1.5003206729888916\n",
      "Training loss:1.5126556158065796\n",
      "Training loss:1.5878621339797974\n",
      "Training loss:1.4624974727630615\n",
      "Training loss:1.482764720916748\n",
      "Training loss:1.4500713348388672\n",
      "Training loss:1.498036503791809\n",
      "Training loss:1.430498480796814\n",
      "Training loss:1.5306875705718994\n",
      "Training loss:1.4893665313720703\n",
      "Training loss:1.541972041130066\n",
      "Training loss:1.5781879425048828\n",
      "Training loss:1.4710590839385986\n",
      "Training loss:1.4565101861953735\n",
      "Training loss:1.4729177951812744\n",
      "Training loss:1.5320253372192383\n",
      "Training loss:1.4657073020935059\n",
      "Training loss:1.625681757926941\n",
      "Training loss:1.514832615852356\n",
      "Training loss:1.4943097829818726\n",
      "Training loss:1.4133402109146118\n",
      "Training loss:1.5094633102416992\n",
      "Training loss:1.4421175718307495\n",
      "Training loss:1.4959818124771118\n",
      "Training loss:1.4306764602661133\n",
      "Training loss:1.4378005266189575\n",
      "Training loss:1.4078398942947388\n",
      "Training loss:1.4744224548339844\n",
      "Training loss:1.5064952373504639\n",
      "Training loss:1.5600547790527344\n",
      "Training loss:1.5288124084472656\n",
      "Training loss:1.4816046953201294\n",
      "Training loss:1.4782049655914307\n",
      "Training loss:1.530714511871338\n",
      "Training loss:1.4544974565505981\n",
      "Training loss:1.4997400045394897\n",
      "Training loss:1.468103051185608\n",
      "Training loss:1.57948899269104\n",
      "Training loss:1.4529958963394165\n",
      "Training loss:1.5831272602081299\n",
      "Training loss:1.5517284870147705\n",
      "Training loss:1.4910682439804077\n",
      "Training loss:1.4799492359161377\n",
      "Training loss:1.467776894569397\n",
      "Training loss:1.4797084331512451\n",
      "Training loss:1.4763388633728027\n",
      "Training loss:1.5453990697860718\n",
      "Training loss:1.5003290176391602\n",
      "Training loss:1.467551827430725\n",
      "Training loss:1.489661693572998\n",
      "Training loss:1.4998801946640015\n",
      "Training loss:1.4309501647949219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.4193392992019653\n",
      "Training loss:1.461560606956482\n",
      "Training loss:1.5259969234466553\n",
      "Training loss:1.5244446992874146\n",
      "Training loss:1.5240044593811035\n",
      "Training loss:1.5006647109985352\n",
      "Training loss:1.5521057844161987\n",
      "Training loss:1.4984196424484253\n",
      "Training loss:1.4560680389404297\n",
      "Training loss:1.5116267204284668\n",
      "Training loss:1.5037992000579834\n",
      "Training loss:1.5292192697525024\n",
      "Training loss:1.53153395652771\n",
      "Training loss:1.5651615858078003\n",
      "Training loss:1.4846311807632446\n",
      "Training loss:1.5793155431747437\n",
      "Training loss:1.5153851509094238\n",
      "Training loss:1.5545568466186523\n",
      "Training loss:1.5022869110107422\n",
      "Training loss:1.6112048625946045\n",
      "Training loss:1.455887794494629\n",
      "Training loss:1.5350342988967896\n",
      "Training loss:1.4962931871414185\n",
      "Training loss:1.5699418783187866\n",
      "Training loss:1.5693165063858032\n",
      "Epoch: 0152 loss_train: 141.633785 acc_train: 0.451813 loss_val: 18.026275 acc_val: 0.455333 time: 392936.853043s\n",
      "Training loss:1.42252516746521\n",
      "Training loss:1.5519523620605469\n",
      "Training loss:1.4993098974227905\n",
      "Training loss:1.5119621753692627\n",
      "Training loss:1.5360816717147827\n",
      "Training loss:1.564706563949585\n",
      "Training loss:1.4763857126235962\n",
      "Training loss:1.5351909399032593\n",
      "Training loss:1.553312063217163\n",
      "Training loss:1.4855046272277832\n",
      "Training loss:1.512473225593567\n",
      "Training loss:1.5575764179229736\n",
      "Training loss:1.4378360509872437\n",
      "Training loss:1.519327998161316\n",
      "Training loss:1.461613416671753\n",
      "Training loss:1.4783860445022583\n",
      "Training loss:1.4858962297439575\n",
      "Training loss:1.5329194068908691\n",
      "Training loss:1.5331865549087524\n",
      "Training loss:1.4560519456863403\n",
      "Training loss:1.4446865320205688\n",
      "Training loss:1.5404291152954102\n",
      "Training loss:1.6076185703277588\n",
      "Training loss:1.5464470386505127\n",
      "Training loss:1.5713651180267334\n",
      "Training loss:1.490453839302063\n",
      "Training loss:1.595595359802246\n",
      "Training loss:1.4818620681762695\n",
      "Training loss:1.4490612745285034\n",
      "Training loss:1.4671984910964966\n",
      "Training loss:1.6053041219711304\n",
      "Training loss:1.4781826734542847\n",
      "Training loss:1.5184537172317505\n",
      "Training loss:1.5247361660003662\n",
      "Training loss:1.4529528617858887\n",
      "Training loss:1.5236226320266724\n",
      "Training loss:1.5806159973144531\n",
      "Training loss:1.5506519079208374\n",
      "Training loss:1.4690589904785156\n",
      "Training loss:1.6300362348556519\n",
      "Training loss:1.484499454498291\n",
      "Training loss:1.4438010454177856\n",
      "Training loss:1.4977999925613403\n",
      "Training loss:1.5956478118896484\n",
      "Training loss:1.5031296014785767\n",
      "Training loss:1.5037002563476562\n",
      "Training loss:1.4905048608779907\n",
      "Training loss:1.5270577669143677\n",
      "Training loss:1.5357664823532104\n",
      "Training loss:1.4612418413162231\n",
      "Training loss:1.5021800994873047\n",
      "Training loss:1.4779987335205078\n",
      "Training loss:1.4640800952911377\n",
      "Training loss:1.5374188423156738\n",
      "Training loss:1.4488381147384644\n",
      "Training loss:1.5086361169815063\n",
      "Training loss:1.448978066444397\n",
      "Training loss:1.4036728143692017\n",
      "Training loss:1.4275615215301514\n",
      "Training loss:1.4368555545806885\n",
      "Training loss:1.5617412328720093\n",
      "Training loss:1.4538220167160034\n",
      "Training loss:1.6191117763519287\n",
      "Training loss:1.499380350112915\n",
      "Training loss:1.4716233015060425\n",
      "Training loss:1.4975042343139648\n",
      "Training loss:1.408154010772705\n",
      "Training loss:1.5048770904541016\n",
      "Training loss:1.4984852075576782\n",
      "Training loss:1.5073946714401245\n",
      "Training loss:1.5712140798568726\n",
      "Training loss:1.5606907606124878\n",
      "Training loss:1.5897657871246338\n",
      "Training loss:1.4978684186935425\n",
      "Training loss:1.4948409795761108\n",
      "Training loss:1.4598383903503418\n",
      "Training loss:1.4991461038589478\n",
      "Training loss:1.4688491821289062\n",
      "Training loss:1.439308524131775\n",
      "Training loss:1.4836302995681763\n",
      "Training loss:1.4732518196105957\n",
      "Training loss:1.5274169445037842\n",
      "Training loss:1.5080548524856567\n",
      "Training loss:1.5275554656982422\n",
      "Training loss:1.596396803855896\n",
      "Training loss:1.4617668390274048\n",
      "Training loss:1.514248251914978\n",
      "Training loss:1.5749166011810303\n",
      "Training loss:1.4176263809204102\n",
      "Training loss:1.538191795349121\n",
      "Training loss:1.5317045450210571\n",
      "Training loss:1.4945474863052368\n",
      "Training loss:1.5682575702667236\n",
      "Training loss:1.4952219724655151\n",
      "Epoch: 0153 loss_train: 141.656304 acc_train: 0.452562 loss_val: 17.923604 acc_val: 0.456333 time: 395364.902977s\n",
      "Training loss:1.4320330619812012\n",
      "Training loss:1.500137209892273\n",
      "Training loss:1.489599585533142\n",
      "Training loss:1.6058669090270996\n",
      "Training loss:1.5563218593597412\n",
      "Training loss:1.5088084936141968\n",
      "Training loss:1.4898202419281006\n",
      "Training loss:1.5019793510437012\n",
      "Training loss:1.4784777164459229\n",
      "Training loss:1.4645085334777832\n",
      "Training loss:1.4934313297271729\n",
      "Training loss:1.5084216594696045\n",
      "Training loss:1.5444276332855225\n",
      "Training loss:1.5275793075561523\n",
      "Training loss:1.4836950302124023\n",
      "Training loss:1.4979827404022217\n",
      "Training loss:1.4673701524734497\n",
      "Training loss:1.4842289686203003\n",
      "Training loss:1.490949034690857\n",
      "Training loss:1.5157058238983154\n",
      "Training loss:1.4806007146835327\n",
      "Training loss:1.4996951818466187\n",
      "Training loss:1.5317528247833252\n",
      "Training loss:1.475549578666687\n",
      "Training loss:1.5202248096466064\n",
      "Training loss:1.5122861862182617\n",
      "Training loss:1.4337345361709595\n",
      "Training loss:1.5327082872390747\n",
      "Training loss:1.531449317932129\n",
      "Training loss:1.5019193887710571\n",
      "Training loss:1.5441083908081055\n",
      "Training loss:1.4712231159210205\n",
      "Training loss:1.5141202211380005\n",
      "Training loss:1.446162462234497\n",
      "Training loss:1.571723461151123\n",
      "Training loss:1.4932817220687866\n",
      "Training loss:1.419784426689148\n",
      "Training loss:1.4411324262619019\n",
      "Training loss:1.539954662322998\n",
      "Training loss:1.5089266300201416\n",
      "Training loss:1.5304840803146362\n",
      "Training loss:1.5013054609298706\n",
      "Training loss:1.4893238544464111\n",
      "Training loss:1.5266263484954834\n",
      "Training loss:1.4456559419631958\n",
      "Training loss:1.6066583395004272\n",
      "Training loss:1.4783024787902832\n",
      "Training loss:1.48635733127594\n",
      "Training loss:1.4973324537277222\n",
      "Training loss:1.530402421951294\n",
      "Training loss:1.5391523838043213\n",
      "Training loss:1.5148719549179077\n",
      "Training loss:1.466326117515564\n",
      "Training loss:1.4557106494903564\n",
      "Training loss:1.436661720275879\n",
      "Training loss:1.5157173871994019\n",
      "Training loss:1.4723269939422607\n",
      "Training loss:1.6002873182296753\n",
      "Training loss:1.441550374031067\n",
      "Training loss:1.5166614055633545\n",
      "Training loss:1.5513285398483276\n",
      "Training loss:1.5483143329620361\n",
      "Training loss:1.4261360168457031\n",
      "Training loss:1.478340983390808\n",
      "Training loss:1.505881667137146\n",
      "Training loss:1.543938159942627\n",
      "Training loss:1.5574767589569092\n",
      "Training loss:1.5185339450836182\n",
      "Training loss:1.4916388988494873\n",
      "Training loss:1.4962266683578491\n",
      "Training loss:1.5437122583389282\n",
      "Training loss:1.4597532749176025\n",
      "Training loss:1.4761040210723877\n",
      "Training loss:1.5389341115951538\n",
      "Training loss:1.553392767906189\n",
      "Training loss:1.515420913696289\n",
      "Training loss:1.5633938312530518\n",
      "Training loss:1.4948668479919434\n",
      "Training loss:1.5857527256011963\n",
      "Training loss:1.5784637928009033\n",
      "Training loss:1.5107202529907227\n",
      "Training loss:1.4584894180297852\n",
      "Training loss:1.53093421459198\n",
      "Training loss:1.5311598777770996\n",
      "Training loss:1.6464091539382935\n",
      "Training loss:1.4691181182861328\n",
      "Training loss:1.4858592748641968\n",
      "Training loss:1.5622316598892212\n",
      "Training loss:1.4472391605377197\n",
      "Training loss:1.5500191450119019\n",
      "Training loss:1.535413384437561\n",
      "Training loss:1.477999210357666\n",
      "Training loss:1.480296015739441\n",
      "Training loss:1.4383374452590942\n",
      "Epoch: 0154 loss_train: 141.615235 acc_train: 0.451083 loss_val: 18.056212 acc_val: 0.452333 time: 397797.990600s\n",
      "Training loss:1.5553189516067505\n",
      "Training loss:1.4708545207977295\n",
      "Training loss:1.6390812397003174\n",
      "Training loss:1.5313386917114258\n",
      "Training loss:1.4674397706985474\n",
      "Training loss:1.5370219945907593\n",
      "Training loss:1.4328019618988037\n",
      "Training loss:1.475982666015625\n",
      "Training loss:1.5213724374771118\n",
      "Training loss:1.5511655807495117\n",
      "Training loss:1.5114952325820923\n",
      "Training loss:1.5224311351776123\n",
      "Training loss:1.5412116050720215\n",
      "Training loss:1.470268964767456\n",
      "Training loss:1.5491732358932495\n",
      "Training loss:1.534454345703125\n",
      "Training loss:1.5692116022109985\n",
      "Training loss:1.492324948310852\n",
      "Training loss:1.4677484035491943\n",
      "Training loss:1.4819700717926025\n",
      "Training loss:1.5201464891433716\n",
      "Training loss:1.545722246170044\n",
      "Training loss:1.548951506614685\n",
      "Training loss:1.5436487197875977\n",
      "Training loss:1.4371495246887207\n",
      "Training loss:1.518473744392395\n",
      "Training loss:1.5374072790145874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5357600450515747\n",
      "Training loss:1.6174273490905762\n",
      "Training loss:1.541007399559021\n",
      "Training loss:1.5331766605377197\n",
      "Training loss:1.575587272644043\n",
      "Training loss:1.5042150020599365\n",
      "Training loss:1.5602871179580688\n",
      "Training loss:1.4852790832519531\n",
      "Training loss:1.4084025621414185\n",
      "Training loss:1.5744222402572632\n",
      "Training loss:1.5200542211532593\n",
      "Training loss:1.5440903902053833\n",
      "Training loss:1.575113296508789\n",
      "Training loss:1.5397087335586548\n",
      "Training loss:1.498658537864685\n",
      "Training loss:1.4938751459121704\n",
      "Training loss:1.5226531028747559\n",
      "Training loss:1.4811787605285645\n",
      "Training loss:1.5269343852996826\n",
      "Training loss:1.4956209659576416\n",
      "Training loss:1.5097824335098267\n",
      "Training loss:1.455184817314148\n",
      "Training loss:1.5193642377853394\n",
      "Training loss:1.4330636262893677\n",
      "Training loss:1.3979530334472656\n",
      "Training loss:1.5305989980697632\n",
      "Training loss:1.5618892908096313\n",
      "Training loss:1.547426700592041\n",
      "Training loss:1.5207968950271606\n",
      "Training loss:1.4901206493377686\n",
      "Training loss:1.4889878034591675\n",
      "Training loss:1.5056753158569336\n",
      "Training loss:1.4508522748947144\n",
      "Training loss:1.4763925075531006\n",
      "Training loss:1.4319500923156738\n",
      "Training loss:1.5784469842910767\n",
      "Training loss:1.5320110321044922\n",
      "Training loss:1.5277591943740845\n",
      "Training loss:1.448146104812622\n",
      "Training loss:1.453521728515625\n",
      "Training loss:1.5458295345306396\n",
      "Training loss:1.6102946996688843\n",
      "Training loss:1.5122655630111694\n",
      "Training loss:1.542852520942688\n",
      "Training loss:1.4981313943862915\n",
      "Training loss:1.542364478111267\n",
      "Training loss:1.4914847612380981\n",
      "Training loss:1.4629184007644653\n",
      "Training loss:1.4727725982666016\n",
      "Training loss:1.5598825216293335\n",
      "Training loss:1.484346628189087\n",
      "Training loss:1.5070934295654297\n",
      "Training loss:1.558857798576355\n",
      "Training loss:1.4019420146942139\n",
      "Training loss:1.5539462566375732\n",
      "Training loss:1.5826408863067627\n",
      "Training loss:1.5113846063613892\n",
      "Training loss:1.4186872243881226\n",
      "Training loss:1.4640064239501953\n",
      "Training loss:1.5634652376174927\n",
      "Training loss:1.476243495941162\n",
      "Training loss:1.475387454032898\n",
      "Training loss:1.5208868980407715\n",
      "Training loss:1.4838111400604248\n",
      "Training loss:1.477559208869934\n",
      "Training loss:1.5289723873138428\n",
      "Training loss:1.5303460359573364\n",
      "Epoch: 0155 loss_train: 142.145916 acc_train: 0.449542 loss_val: 17.904250 acc_val: 0.457500 time: 400228.792457s\n",
      "Training loss:1.5208759307861328\n",
      "Training loss:1.503528356552124\n",
      "Training loss:1.5471278429031372\n",
      "Training loss:1.447222113609314\n",
      "Training loss:1.5445054769515991\n",
      "Training loss:1.4160689115524292\n",
      "Training loss:1.5674415826797485\n",
      "Training loss:1.5080739259719849\n",
      "Training loss:1.5169501304626465\n",
      "Training loss:1.4706579446792603\n",
      "Training loss:1.592015266418457\n",
      "Training loss:1.57600998878479\n",
      "Training loss:1.4191251993179321\n",
      "Training loss:1.469610571861267\n",
      "Training loss:1.477828025817871\n",
      "Training loss:1.4940706491470337\n",
      "Training loss:1.394702672958374\n",
      "Training loss:1.522239089012146\n",
      "Training loss:1.4563533067703247\n",
      "Training loss:1.5383459329605103\n",
      "Training loss:1.4526771306991577\n",
      "Training loss:1.4942518472671509\n",
      "Training loss:1.4297484159469604\n",
      "Training loss:1.5093894004821777\n",
      "Training loss:1.656921148300171\n",
      "Training loss:1.5878962278366089\n",
      "Training loss:1.476837158203125\n",
      "Training loss:1.4273486137390137\n",
      "Training loss:1.4344085454940796\n",
      "Training loss:1.516012191772461\n",
      "Training loss:1.49661123752594\n",
      "Training loss:1.5273879766464233\n",
      "Training loss:1.55948805809021\n",
      "Training loss:1.5187814235687256\n",
      "Training loss:1.5532170534133911\n",
      "Training loss:1.4999096393585205\n",
      "Training loss:1.5621767044067383\n",
      "Training loss:1.4807466268539429\n",
      "Training loss:1.4853285551071167\n",
      "Training loss:1.469103217124939\n",
      "Training loss:1.4385149478912354\n",
      "Training loss:1.4804658889770508\n",
      "Training loss:1.4768435955047607\n",
      "Training loss:1.4449862241744995\n",
      "Training loss:1.5085198879241943\n",
      "Training loss:1.4460879564285278\n",
      "Training loss:1.5144858360290527\n",
      "Training loss:1.5588480234146118\n",
      "Training loss:1.5228080749511719\n",
      "Training loss:1.4731932878494263\n",
      "Training loss:1.4988081455230713\n",
      "Training loss:1.5118157863616943\n",
      "Training loss:1.4970128536224365\n",
      "Training loss:1.5740962028503418\n",
      "Training loss:1.5540050268173218\n",
      "Training loss:1.4527020454406738\n",
      "Training loss:1.467902660369873\n",
      "Training loss:1.4095429182052612\n",
      "Training loss:1.533923864364624\n",
      "Training loss:1.538573145866394\n",
      "Training loss:1.4897781610488892\n",
      "Training loss:1.409943699836731\n",
      "Training loss:1.541261076927185\n",
      "Training loss:1.4979963302612305\n",
      "Training loss:1.5466703176498413\n",
      "Training loss:1.5472993850708008\n",
      "Training loss:1.57113516330719\n",
      "Training loss:1.4981083869934082\n",
      "Training loss:1.5716805458068848\n",
      "Training loss:1.496549367904663\n",
      "Training loss:1.5385236740112305\n",
      "Training loss:1.5476763248443604\n",
      "Training loss:1.4791114330291748\n",
      "Training loss:1.5397509336471558\n",
      "Training loss:1.485142707824707\n",
      "Training loss:1.4170620441436768\n",
      "Training loss:1.5697895288467407\n",
      "Training loss:1.4873626232147217\n",
      "Training loss:1.491759181022644\n",
      "Training loss:1.5015307664871216\n",
      "Training loss:1.6052896976470947\n",
      "Training loss:1.5036710500717163\n",
      "Training loss:1.53690505027771\n",
      "Training loss:1.5055475234985352\n",
      "Training loss:1.4827923774719238\n",
      "Training loss:1.4445645809173584\n",
      "Training loss:1.5001591444015503\n",
      "Training loss:1.4177987575531006\n",
      "Training loss:1.5760719776153564\n",
      "Training loss:1.50809645652771\n",
      "Training loss:1.5506333112716675\n",
      "Training loss:1.5757194757461548\n",
      "Training loss:1.4821858406066895\n",
      "Training loss:1.4142719507217407\n",
      "Epoch: 0156 loss_train: 141.355969 acc_train: 0.453917 loss_val: 18.161853 acc_val: 0.453333 time: 402656.741842s\n",
      "Training loss:1.5464222431182861\n",
      "Training loss:1.5307267904281616\n",
      "Training loss:1.5258175134658813\n",
      "Training loss:1.4483214616775513\n",
      "Training loss:1.4794845581054688\n",
      "Training loss:1.4550912380218506\n",
      "Training loss:1.4631867408752441\n",
      "Training loss:1.5471043586730957\n",
      "Training loss:1.5365489721298218\n",
      "Training loss:1.4490761756896973\n",
      "Training loss:1.4437170028686523\n",
      "Training loss:1.4717620611190796\n",
      "Training loss:1.4818600416183472\n",
      "Training loss:1.4294010400772095\n",
      "Training loss:1.429161548614502\n",
      "Training loss:1.40193772315979\n",
      "Training loss:1.45845365524292\n",
      "Training loss:1.4904981851577759\n",
      "Training loss:1.5352611541748047\n",
      "Training loss:1.47747802734375\n",
      "Training loss:1.5531151294708252\n",
      "Training loss:1.5837252140045166\n",
      "Training loss:1.5140758752822876\n",
      "Training loss:1.4330945014953613\n",
      "Training loss:1.5230427980422974\n",
      "Training loss:1.5848103761672974\n",
      "Training loss:1.4585866928100586\n",
      "Training loss:1.440375804901123\n",
      "Training loss:1.507691502571106\n",
      "Training loss:1.5478521585464478\n",
      "Training loss:1.4536081552505493\n",
      "Training loss:1.5663703680038452\n",
      "Training loss:1.579501748085022\n",
      "Training loss:1.510608434677124\n",
      "Training loss:1.4651124477386475\n",
      "Training loss:1.5534088611602783\n",
      "Training loss:1.4984952211380005\n",
      "Training loss:1.5966264009475708\n",
      "Training loss:1.5362861156463623\n",
      "Training loss:1.4693140983581543\n",
      "Training loss:1.4647853374481201\n",
      "Training loss:1.543414831161499\n",
      "Training loss:1.4658573865890503\n",
      "Training loss:1.6216634511947632\n",
      "Training loss:1.5489637851715088\n",
      "Training loss:1.4388574361801147\n",
      "Training loss:1.558594822883606\n",
      "Training loss:1.516732096672058\n",
      "Training loss:1.4508724212646484\n",
      "Training loss:1.5102581977844238\n",
      "Training loss:1.4802436828613281\n",
      "Training loss:1.5118318796157837\n",
      "Training loss:1.52852201461792\n",
      "Training loss:1.5598341226577759\n",
      "Training loss:1.4844605922698975\n",
      "Training loss:1.451640009880066\n",
      "Training loss:1.5329521894454956\n",
      "Training loss:1.509696125984192\n",
      "Training loss:1.592483401298523\n",
      "Training loss:1.535068154335022\n",
      "Training loss:1.450474500656128\n",
      "Training loss:1.521811842918396\n",
      "Training loss:1.5600991249084473\n",
      "Training loss:1.4722357988357544\n",
      "Training loss:1.5219767093658447\n",
      "Training loss:1.5612894296646118\n",
      "Training loss:1.5187102556228638\n",
      "Training loss:1.4658275842666626\n",
      "Training loss:1.5223095417022705\n",
      "Training loss:1.5084675550460815\n",
      "Training loss:1.5742686986923218\n",
      "Training loss:1.4955912828445435\n",
      "Training loss:1.4942172765731812\n",
      "Training loss:1.5601495504379272\n",
      "Training loss:1.5422707796096802\n",
      "Training loss:1.6020395755767822\n",
      "Training loss:1.4931827783584595\n",
      "Training loss:1.5213775634765625\n",
      "Training loss:1.4801486730575562\n",
      "Training loss:1.4584017992019653\n",
      "Training loss:1.532670497894287\n",
      "Training loss:1.554151177406311\n",
      "Training loss:1.5726282596588135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5457464456558228\n",
      "Training loss:1.508313775062561\n",
      "Training loss:1.566760778427124\n",
      "Training loss:1.4114283323287964\n",
      "Training loss:1.437007188796997\n",
      "Training loss:1.5252330303192139\n",
      "Training loss:1.4464893341064453\n",
      "Training loss:1.4886082410812378\n",
      "Training loss:1.5113394260406494\n",
      "Training loss:1.5310958623886108\n",
      "Training loss:1.5289279222488403\n",
      "Epoch: 0157 loss_train: 141.768995 acc_train: 0.451479 loss_val: 18.116929 acc_val: 0.449000 time: 405087.920820s\n",
      "Training loss:1.4879224300384521\n",
      "Training loss:1.4228034019470215\n",
      "Training loss:1.5625630617141724\n",
      "Training loss:1.6045079231262207\n",
      "Training loss:1.4752871990203857\n",
      "Training loss:1.5337268114089966\n",
      "Training loss:1.5119850635528564\n",
      "Training loss:1.528617024421692\n",
      "Training loss:1.4776082038879395\n",
      "Training loss:1.4851648807525635\n",
      "Training loss:1.4463609457015991\n",
      "Training loss:1.535050868988037\n",
      "Training loss:1.5047016143798828\n",
      "Training loss:1.5546499490737915\n",
      "Training loss:1.4495494365692139\n",
      "Training loss:1.4847126007080078\n",
      "Training loss:1.4874647855758667\n",
      "Training loss:1.4807826280593872\n",
      "Training loss:1.4987027645111084\n",
      "Training loss:1.4365296363830566\n",
      "Training loss:1.4976204633712769\n",
      "Training loss:1.5037095546722412\n",
      "Training loss:1.4664021730422974\n",
      "Training loss:1.526957631111145\n",
      "Training loss:1.508435606956482\n",
      "Training loss:1.4390568733215332\n",
      "Training loss:1.529144287109375\n",
      "Training loss:1.5346935987472534\n",
      "Training loss:1.4671026468276978\n",
      "Training loss:1.5095139741897583\n",
      "Training loss:1.4673221111297607\n",
      "Training loss:1.4332265853881836\n",
      "Training loss:1.5896000862121582\n",
      "Training loss:1.515841007232666\n",
      "Training loss:1.5039291381835938\n",
      "Training loss:1.5148738622665405\n",
      "Training loss:1.4825130701065063\n",
      "Training loss:1.5153241157531738\n",
      "Training loss:1.4468994140625\n",
      "Training loss:1.5644962787628174\n",
      "Training loss:1.514535903930664\n",
      "Training loss:1.5661003589630127\n",
      "Training loss:1.511660099029541\n",
      "Training loss:1.4895466566085815\n",
      "Training loss:1.547844409942627\n",
      "Training loss:1.4885011911392212\n",
      "Training loss:1.5768706798553467\n",
      "Training loss:1.4751347303390503\n",
      "Training loss:1.4671539068222046\n",
      "Training loss:1.5206258296966553\n",
      "Training loss:1.5297547578811646\n",
      "Training loss:1.556012749671936\n",
      "Training loss:1.5248632431030273\n",
      "Training loss:1.508169412612915\n",
      "Training loss:1.4309347867965698\n",
      "Training loss:1.5279347896575928\n",
      "Training loss:1.470970869064331\n",
      "Training loss:1.425189733505249\n",
      "Training loss:1.5392251014709473\n",
      "Training loss:1.5156291723251343\n",
      "Training loss:1.5469534397125244\n",
      "Training loss:1.5290478467941284\n",
      "Training loss:1.4691965579986572\n",
      "Training loss:1.4547154903411865\n",
      "Training loss:1.5045568943023682\n",
      "Training loss:1.481184959411621\n",
      "Training loss:1.4538700580596924\n",
      "Training loss:1.524453043937683\n",
      "Training loss:1.5213326215744019\n",
      "Training loss:1.4927109479904175\n",
      "Training loss:1.4411252737045288\n",
      "Training loss:1.5138095617294312\n",
      "Training loss:1.5576934814453125\n",
      "Training loss:1.5108981132507324\n",
      "Training loss:1.465264081954956\n",
      "Training loss:1.5019196271896362\n",
      "Training loss:1.499575138092041\n",
      "Training loss:1.5835261344909668\n",
      "Training loss:1.5114649534225464\n",
      "Training loss:1.461198329925537\n",
      "Training loss:1.45968759059906\n",
      "Training loss:1.468335747718811\n",
      "Training loss:1.4741125106811523\n",
      "Training loss:1.5175601243972778\n",
      "Training loss:1.5527517795562744\n",
      "Training loss:1.4955958127975464\n",
      "Training loss:1.4364532232284546\n",
      "Training loss:1.4453742504119873\n",
      "Training loss:1.5445494651794434\n",
      "Training loss:1.4653666019439697\n",
      "Training loss:1.5096580982208252\n",
      "Training loss:1.3935059309005737\n",
      "Training loss:1.4844506978988647\n",
      "Training loss:1.559584140777588\n",
      "Epoch: 0158 loss_train: 141.003495 acc_train: 0.456396 loss_val: 17.807953 acc_val: 0.457833 time: 407518.163473s\n",
      "Training loss:1.4910587072372437\n",
      "Training loss:1.5914630889892578\n",
      "Training loss:1.4539827108383179\n",
      "Training loss:1.4832563400268555\n",
      "Training loss:1.5068587064743042\n",
      "Training loss:1.4589970111846924\n",
      "Training loss:1.4221010208129883\n",
      "Training loss:1.3977752923965454\n",
      "Training loss:1.4281667470932007\n",
      "Training loss:1.4850293397903442\n",
      "Training loss:1.4986255168914795\n",
      "Training loss:1.4951914548873901\n",
      "Training loss:1.467859148979187\n",
      "Training loss:1.528530240058899\n",
      "Training loss:1.4984166622161865\n",
      "Training loss:1.5311177968978882\n",
      "Training loss:1.4547592401504517\n",
      "Training loss:1.5121060609817505\n",
      "Training loss:1.522042989730835\n",
      "Training loss:1.4769155979156494\n",
      "Training loss:1.4241918325424194\n",
      "Training loss:1.4544261693954468\n",
      "Training loss:1.4408107995986938\n",
      "Training loss:1.472400188446045\n",
      "Training loss:1.4974956512451172\n",
      "Training loss:1.489099383354187\n",
      "Training loss:1.4694911241531372\n",
      "Training loss:1.553826928138733\n",
      "Training loss:1.511154055595398\n",
      "Training loss:1.4817347526550293\n",
      "Training loss:1.4485493898391724\n",
      "Training loss:1.4070863723754883\n",
      "Training loss:1.5054630041122437\n",
      "Training loss:1.529289960861206\n",
      "Training loss:1.5061029195785522\n",
      "Training loss:1.4400832653045654\n",
      "Training loss:1.5421335697174072\n",
      "Training loss:1.4627058506011963\n",
      "Training loss:1.5518832206726074\n",
      "Training loss:1.5471299886703491\n",
      "Training loss:1.507502555847168\n",
      "Training loss:1.55641770362854\n",
      "Training loss:1.4544622898101807\n",
      "Training loss:1.4969693422317505\n",
      "Training loss:1.4956828355789185\n",
      "Training loss:1.43734872341156\n",
      "Training loss:1.4720878601074219\n",
      "Training loss:1.4822075366973877\n",
      "Training loss:1.5667718648910522\n",
      "Training loss:1.4515272378921509\n",
      "Training loss:1.4720430374145508\n",
      "Training loss:1.5057498216629028\n",
      "Training loss:1.5618083477020264\n",
      "Training loss:1.5518416166305542\n",
      "Training loss:1.447436809539795\n",
      "Training loss:1.4745365381240845\n",
      "Training loss:1.4824414253234863\n",
      "Training loss:1.5036736726760864\n",
      "Training loss:1.5352405309677124\n",
      "Training loss:1.5423821210861206\n",
      "Training loss:1.4922056198120117\n",
      "Training loss:1.5477977991104126\n",
      "Training loss:1.4527002573013306\n",
      "Training loss:1.6368448734283447\n",
      "Training loss:1.6114768981933594\n",
      "Training loss:1.5736777782440186\n",
      "Training loss:1.587913990020752\n",
      "Training loss:1.5208195447921753\n",
      "Training loss:1.525375485420227\n",
      "Training loss:1.4598839282989502\n",
      "Training loss:1.4740679264068604\n",
      "Training loss:1.5556957721710205\n",
      "Training loss:1.5646108388900757\n",
      "Training loss:1.538928508758545\n",
      "Training loss:1.6105858087539673\n",
      "Training loss:1.5297054052352905\n",
      "Training loss:1.5492985248565674\n",
      "Training loss:1.5869543552398682\n",
      "Training loss:1.5928629636764526\n",
      "Training loss:1.527449131011963\n",
      "Training loss:1.474439263343811\n",
      "Training loss:1.5003249645233154\n",
      "Training loss:1.5149539709091187\n",
      "Training loss:1.4848052263259888\n",
      "Training loss:1.5604093074798584\n",
      "Training loss:1.5411577224731445\n",
      "Training loss:1.5139347314834595\n",
      "Training loss:1.4479384422302246\n",
      "Training loss:1.604211449623108\n",
      "Training loss:1.455803632736206\n",
      "Training loss:1.5074331760406494\n",
      "Training loss:1.5114011764526367\n",
      "Training loss:1.4663944244384766\n",
      "Training loss:1.5602139234542847\n",
      "Epoch: 0159 loss_train: 141.519721 acc_train: 0.454188 loss_val: 18.182016 acc_val: 0.442667 time: 409947.904570s\n",
      "Training loss:1.590202808380127\n",
      "Training loss:1.5463709831237793\n",
      "Training loss:1.4836390018463135\n",
      "Training loss:1.4538317918777466\n",
      "Training loss:1.4474798440933228\n",
      "Training loss:1.4808299541473389\n",
      "Training loss:1.5272388458251953\n",
      "Training loss:1.5007187128067017\n",
      "Training loss:1.4536559581756592\n",
      "Training loss:1.449110746383667\n",
      "Training loss:1.4570462703704834\n",
      "Training loss:1.4482792615890503\n",
      "Training loss:1.4561818838119507\n",
      "Training loss:1.569556713104248\n",
      "Training loss:1.4791947603225708\n",
      "Training loss:1.4937386512756348\n",
      "Training loss:1.463798999786377\n",
      "Training loss:1.527146816253662\n",
      "Training loss:1.5700777769088745\n",
      "Training loss:1.4299089908599854\n",
      "Training loss:1.5048612356185913\n",
      "Training loss:1.480711817741394\n",
      "Training loss:1.5009040832519531\n",
      "Training loss:1.5346276760101318\n",
      "Training loss:1.4775052070617676\n",
      "Training loss:1.4974758625030518\n",
      "Training loss:1.5854127407073975\n",
      "Training loss:1.4729042053222656\n",
      "Training loss:1.6056852340698242\n",
      "Training loss:1.4630992412567139\n",
      "Training loss:1.473073959350586\n",
      "Training loss:1.509215235710144\n",
      "Training loss:1.5710235834121704\n",
      "Training loss:1.4686065912246704\n",
      "Training loss:1.5240741968154907\n",
      "Training loss:1.5151052474975586\n",
      "Training loss:1.5544465780258179\n",
      "Training loss:1.5229356288909912\n",
      "Training loss:1.48618483543396\n",
      "Training loss:1.479396104812622\n",
      "Training loss:1.4865648746490479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5820512771606445\n",
      "Training loss:1.4668296575546265\n",
      "Training loss:1.4907156229019165\n",
      "Training loss:1.5075873136520386\n",
      "Training loss:1.461441159248352\n",
      "Training loss:1.5788025856018066\n",
      "Training loss:1.4705091714859009\n",
      "Training loss:1.429720401763916\n",
      "Training loss:1.5005923509597778\n",
      "Training loss:1.4137178659439087\n",
      "Training loss:1.4525134563446045\n",
      "Training loss:1.5084620714187622\n",
      "Training loss:1.5260934829711914\n",
      "Training loss:1.4649007320404053\n",
      "Training loss:1.5003011226654053\n",
      "Training loss:1.5625048875808716\n",
      "Training loss:1.5604984760284424\n",
      "Training loss:1.509711742401123\n",
      "Training loss:1.4668774604797363\n",
      "Training loss:1.5881034135818481\n",
      "Training loss:1.479142189025879\n",
      "Training loss:1.5581117868423462\n",
      "Training loss:1.5503904819488525\n",
      "Training loss:1.4791158437728882\n",
      "Training loss:1.6136515140533447\n",
      "Training loss:1.3860496282577515\n",
      "Training loss:1.5122580528259277\n",
      "Training loss:1.478597640991211\n",
      "Training loss:1.4546499252319336\n",
      "Training loss:1.5283730030059814\n",
      "Training loss:1.5067822933197021\n",
      "Training loss:1.4803999662399292\n",
      "Training loss:1.4917651414871216\n",
      "Training loss:1.512248158454895\n",
      "Training loss:1.4824013710021973\n",
      "Training loss:1.5445704460144043\n",
      "Training loss:1.610809326171875\n",
      "Training loss:1.4834626913070679\n",
      "Training loss:1.57573401927948\n",
      "Training loss:1.4931260347366333\n",
      "Training loss:1.4542839527130127\n",
      "Training loss:1.500956416130066\n",
      "Training loss:1.432207703590393\n",
      "Training loss:1.5095291137695312\n",
      "Training loss:1.452970027923584\n",
      "Training loss:1.529420018196106\n",
      "Training loss:1.519525408744812\n",
      "Training loss:1.5379194021224976\n",
      "Training loss:1.4777699708938599\n",
      "Training loss:1.420073390007019\n",
      "Training loss:1.4524078369140625\n",
      "Training loss:1.4798924922943115\n",
      "Training loss:1.4983750581741333\n",
      "Epoch: 0160 loss_train: 141.070727 acc_train: 0.454667 loss_val: 18.176320 acc_val: 0.456500 time: 412419.707951s\n",
      "Training loss:1.617918610572815\n",
      "Training loss:1.5438401699066162\n",
      "Training loss:1.4859696626663208\n",
      "Training loss:1.5373042821884155\n",
      "Training loss:1.475160837173462\n",
      "Training loss:1.424315094947815\n",
      "Training loss:1.4666615724563599\n",
      "Training loss:1.4810799360275269\n",
      "Training loss:1.5645933151245117\n",
      "Training loss:1.5299721956253052\n",
      "Training loss:1.415345549583435\n",
      "Training loss:1.4815115928649902\n",
      "Training loss:1.5596493482589722\n",
      "Training loss:1.4780070781707764\n",
      "Training loss:1.4711984395980835\n",
      "Training loss:1.4124948978424072\n",
      "Training loss:1.5379611253738403\n",
      "Training loss:1.3730884790420532\n",
      "Training loss:1.4190242290496826\n",
      "Training loss:1.4895617961883545\n",
      "Training loss:1.4376088380813599\n",
      "Training loss:1.5296361446380615\n",
      "Training loss:1.525689721107483\n",
      "Training loss:1.5576094388961792\n",
      "Training loss:1.4774401187896729\n",
      "Training loss:1.617264747619629\n",
      "Training loss:1.501549243927002\n",
      "Training loss:1.5585993528366089\n",
      "Training loss:1.4950392246246338\n",
      "Training loss:1.4432646036148071\n",
      "Training loss:1.4903028011322021\n",
      "Training loss:1.4933031797409058\n",
      "Training loss:1.5096409320831299\n",
      "Training loss:1.4350045919418335\n",
      "Training loss:1.4659427404403687\n",
      "Training loss:1.4144011735916138\n",
      "Training loss:1.5067368745803833\n",
      "Training loss:1.4807006120681763\n",
      "Training loss:1.4345531463623047\n",
      "Training loss:1.4927399158477783\n",
      "Training loss:1.4582699537277222\n",
      "Training loss:1.495854377746582\n",
      "Training loss:1.4812337160110474\n",
      "Training loss:1.500573754310608\n",
      "Training loss:1.6150835752487183\n",
      "Training loss:1.4269328117370605\n",
      "Training loss:1.4872528314590454\n",
      "Training loss:1.405134916305542\n",
      "Training loss:1.4773001670837402\n",
      "Training loss:1.4927364587783813\n",
      "Training loss:1.513738989830017\n",
      "Training loss:1.483206033706665\n",
      "Training loss:1.5187243223190308\n",
      "Training loss:1.5240694284439087\n",
      "Training loss:1.5822087526321411\n",
      "Training loss:1.5593761205673218\n",
      "Training loss:1.5460411310195923\n",
      "Training loss:1.443225622177124\n",
      "Training loss:1.5201787948608398\n",
      "Training loss:1.4904056787490845\n",
      "Training loss:1.4976952075958252\n",
      "Training loss:1.4563546180725098\n",
      "Training loss:1.535865068435669\n",
      "Training loss:1.4242616891860962\n",
      "Training loss:1.537919521331787\n",
      "Training loss:1.4794106483459473\n",
      "Training loss:1.4319877624511719\n",
      "Training loss:1.5206090211868286\n",
      "Training loss:1.5586978197097778\n",
      "Training loss:1.5458046197891235\n",
      "Training loss:1.5122381448745728\n",
      "Training loss:1.4517289400100708\n",
      "Training loss:1.5580174922943115\n",
      "Training loss:1.4441627264022827\n",
      "Training loss:1.5450129508972168\n",
      "Training loss:1.5071921348571777\n",
      "Training loss:1.519387125968933\n",
      "Training loss:1.5041364431381226\n",
      "Training loss:1.5014067888259888\n",
      "Training loss:1.5083346366882324\n",
      "Training loss:1.4779855012893677\n",
      "Training loss:1.5298759937286377\n",
      "Training loss:1.4523898363113403\n",
      "Training loss:1.5014266967773438\n",
      "Training loss:1.4508391618728638\n",
      "Training loss:1.5400978326797485\n",
      "Training loss:1.4689174890518188\n",
      "Training loss:1.5039870738983154\n",
      "Training loss:1.4901312589645386\n",
      "Training loss:1.5039407014846802\n",
      "Training loss:1.4181163311004639\n",
      "Training loss:1.5108720064163208\n",
      "Training loss:1.4957220554351807\n",
      "Training loss:1.494039535522461\n",
      "Epoch: 0161 loss_train: 140.531798 acc_train: 0.456354 loss_val: 17.783673 acc_val: 0.456667 time: 414934.759729s\n",
      "Training loss:1.5538079738616943\n",
      "Training loss:1.5009963512420654\n",
      "Training loss:1.4327441453933716\n",
      "Training loss:1.5621492862701416\n",
      "Training loss:1.5559595823287964\n",
      "Training loss:1.4692319631576538\n",
      "Training loss:1.481248378753662\n",
      "Training loss:1.4841995239257812\n",
      "Training loss:1.536551833152771\n",
      "Training loss:1.4887313842773438\n",
      "Training loss:1.4640029668807983\n",
      "Training loss:1.4353584051132202\n",
      "Training loss:1.5295923948287964\n",
      "Training loss:1.5160757303237915\n",
      "Training loss:1.4378968477249146\n",
      "Training loss:1.546473503112793\n",
      "Training loss:1.4745728969573975\n",
      "Training loss:1.550067663192749\n",
      "Training loss:1.4520189762115479\n",
      "Training loss:1.5801998376846313\n",
      "Training loss:1.501051664352417\n",
      "Training loss:1.4734495878219604\n",
      "Training loss:1.5464098453521729\n",
      "Training loss:1.452616572380066\n",
      "Training loss:1.4515258073806763\n",
      "Training loss:1.5189135074615479\n",
      "Training loss:1.5720947980880737\n",
      "Training loss:1.5367019176483154\n",
      "Training loss:1.4932293891906738\n",
      "Training loss:1.471203088760376\n",
      "Training loss:1.520567536354065\n",
      "Training loss:1.4693756103515625\n",
      "Training loss:1.502105474472046\n",
      "Training loss:1.5853447914123535\n",
      "Training loss:1.5379201173782349\n",
      "Training loss:1.5103827714920044\n",
      "Training loss:1.4740760326385498\n",
      "Training loss:1.5277312994003296\n",
      "Training loss:1.457513689994812\n",
      "Training loss:1.5538678169250488\n",
      "Training loss:1.4959521293640137\n",
      "Training loss:1.5109338760375977\n",
      "Training loss:1.5443933010101318\n",
      "Training loss:1.4482320547103882\n",
      "Training loss:1.513092279434204\n",
      "Training loss:1.5703940391540527\n",
      "Training loss:1.5487825870513916\n",
      "Training loss:1.469820261001587\n",
      "Training loss:1.4896364212036133\n",
      "Training loss:1.4787043333053589\n",
      "Training loss:1.4395532608032227\n",
      "Training loss:1.5240192413330078\n",
      "Training loss:1.5080642700195312\n",
      "Training loss:1.5274696350097656\n",
      "Training loss:1.44296133518219\n",
      "Training loss:1.519258975982666\n",
      "Training loss:1.4886910915374756\n",
      "Training loss:1.4761818647384644\n",
      "Training loss:1.543915867805481\n",
      "Training loss:1.4803593158721924\n",
      "Training loss:1.4996023178100586\n",
      "Training loss:1.444088101387024\n",
      "Training loss:1.4178751707077026\n",
      "Training loss:1.5498404502868652\n",
      "Training loss:1.4601070880889893\n",
      "Training loss:1.4547041654586792\n",
      "Training loss:1.4385240077972412\n",
      "Training loss:1.451736330986023\n",
      "Training loss:1.5757298469543457\n",
      "Training loss:1.5171695947647095\n",
      "Training loss:1.4854495525360107\n",
      "Training loss:1.431053638458252\n",
      "Training loss:1.422824501991272\n",
      "Training loss:1.5298850536346436\n",
      "Training loss:1.5307058095932007\n",
      "Training loss:1.5066431760787964\n",
      "Training loss:1.5259298086166382\n",
      "Training loss:1.4837054014205933\n",
      "Training loss:1.5170750617980957\n",
      "Training loss:1.5726317167282104\n",
      "Training loss:1.5218442678451538\n",
      "Training loss:1.4674246311187744\n",
      "Training loss:1.5708708763122559\n",
      "Training loss:1.4479190111160278\n",
      "Training loss:1.4271587133407593\n",
      "Training loss:1.4261181354522705\n",
      "Training loss:1.4865431785583496\n",
      "Training loss:1.4872609376907349\n",
      "Training loss:1.4875142574310303\n",
      "Training loss:1.50348961353302\n",
      "Training loss:1.4158668518066406\n",
      "Training loss:1.4889687299728394\n",
      "Training loss:1.423677682876587\n",
      "Training loss:1.4275932312011719\n",
      "Epoch: 0162 loss_train: 140.625908 acc_train: 0.457042 loss_val: 18.122041 acc_val: 0.448833 time: 417506.041047s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.546175479888916\n",
      "Training loss:1.48263418674469\n",
      "Training loss:1.4586008787155151\n",
      "Training loss:1.4717625379562378\n",
      "Training loss:1.4584810733795166\n",
      "Training loss:1.4328444004058838\n",
      "Training loss:1.4733539819717407\n",
      "Training loss:1.532339334487915\n",
      "Training loss:1.5029934644699097\n",
      "Training loss:1.4922012090682983\n",
      "Training loss:1.503233551979065\n",
      "Training loss:1.4641026258468628\n",
      "Training loss:1.4733952283859253\n",
      "Training loss:1.5232691764831543\n",
      "Training loss:1.4411906003952026\n",
      "Training loss:1.4243991374969482\n",
      "Training loss:1.549107551574707\n",
      "Training loss:1.5085324048995972\n",
      "Training loss:1.4608898162841797\n",
      "Training loss:1.5242756605148315\n",
      "Training loss:1.550686001777649\n",
      "Training loss:1.496711015701294\n",
      "Training loss:1.4956272840499878\n",
      "Training loss:1.4335813522338867\n",
      "Training loss:1.5047399997711182\n",
      "Training loss:1.5556800365447998\n",
      "Training loss:1.524316430091858\n",
      "Training loss:1.528098702430725\n",
      "Training loss:1.4450981616973877\n",
      "Training loss:1.5442101955413818\n",
      "Training loss:1.4886047840118408\n",
      "Training loss:1.5737704038619995\n",
      "Training loss:1.500475287437439\n",
      "Training loss:1.4474109411239624\n",
      "Training loss:1.5034911632537842\n",
      "Training loss:1.4632844924926758\n",
      "Training loss:1.503278374671936\n",
      "Training loss:1.4749666452407837\n",
      "Training loss:1.5427535772323608\n",
      "Training loss:1.4583278894424438\n",
      "Training loss:1.5412324666976929\n",
      "Training loss:1.555662989616394\n",
      "Training loss:1.538757085800171\n",
      "Training loss:1.4971106052398682\n",
      "Training loss:1.536388874053955\n",
      "Training loss:1.4921544790267944\n",
      "Training loss:1.519423484802246\n",
      "Training loss:1.5646039247512817\n",
      "Training loss:1.5085089206695557\n",
      "Training loss:1.4678353071212769\n",
      "Training loss:1.4628709554672241\n",
      "Training loss:1.5728546380996704\n",
      "Training loss:1.4391083717346191\n",
      "Training loss:1.4825657606124878\n",
      "Training loss:1.493730068206787\n",
      "Training loss:1.4900835752487183\n",
      "Training loss:1.4860817193984985\n",
      "Training loss:1.397926688194275\n",
      "Training loss:1.5100194215774536\n",
      "Training loss:1.4753303527832031\n",
      "Training loss:1.4565565586090088\n",
      "Training loss:1.493993878364563\n",
      "Training loss:1.4509292840957642\n",
      "Training loss:1.5918374061584473\n",
      "Training loss:1.5042383670806885\n",
      "Training loss:1.3892297744750977\n",
      "Training loss:1.4173376560211182\n",
      "Training loss:1.5537829399108887\n",
      "Training loss:1.4925048351287842\n",
      "Training loss:1.4837473630905151\n",
      "Training loss:1.5246505737304688\n",
      "Training loss:1.6012300252914429\n",
      "Training loss:1.470345377922058\n",
      "Training loss:1.4907565116882324\n",
      "Training loss:1.532446265220642\n",
      "Training loss:1.4264861345291138\n",
      "Training loss:1.542218804359436\n",
      "Training loss:1.540606141090393\n",
      "Training loss:1.4727667570114136\n",
      "Training loss:1.4490450620651245\n",
      "Training loss:1.5372416973114014\n",
      "Training loss:1.5585163831710815\n",
      "Training loss:1.4750800132751465\n",
      "Training loss:1.4383238554000854\n",
      "Training loss:1.5032697916030884\n",
      "Training loss:1.5353360176086426\n",
      "Training loss:1.4192695617675781\n",
      "Training loss:1.4295042753219604\n",
      "Training loss:1.4808954000473022\n",
      "Training loss:1.4787100553512573\n",
      "Training loss:1.4818320274353027\n",
      "Training loss:1.525101661682129\n",
      "Training loss:1.5965615510940552\n",
      "Training loss:1.5707603693008423\n",
      "Epoch: 0163 loss_train: 140.706257 acc_train: 0.456604 loss_val: 17.751657 acc_val: 0.461167 time: 420006.614492s\n",
      "Training loss:1.4699597358703613\n",
      "Training loss:1.4920575618743896\n",
      "Training loss:1.4918690919876099\n",
      "Training loss:1.4831092357635498\n",
      "Training loss:1.4507033824920654\n",
      "Training loss:1.496361255645752\n",
      "Training loss:1.4779317378997803\n",
      "Training loss:1.451661467552185\n",
      "Training loss:1.5291078090667725\n",
      "Training loss:1.5305202007293701\n",
      "Training loss:1.5571081638336182\n",
      "Training loss:1.4298741817474365\n",
      "Training loss:1.440137505531311\n",
      "Training loss:1.4805643558502197\n",
      "Training loss:1.4285165071487427\n",
      "Training loss:1.4799330234527588\n",
      "Training loss:1.5901286602020264\n",
      "Training loss:1.6102160215377808\n",
      "Training loss:1.4519472122192383\n",
      "Training loss:1.4297267198562622\n",
      "Training loss:1.5188008546829224\n",
      "Training loss:1.4646375179290771\n",
      "Training loss:1.5046241283416748\n",
      "Training loss:1.3972797393798828\n",
      "Training loss:1.5340343713760376\n",
      "Training loss:1.4998178482055664\n",
      "Training loss:1.4800646305084229\n",
      "Training loss:1.4914942979812622\n",
      "Training loss:1.5024889707565308\n",
      "Training loss:1.5147833824157715\n",
      "Training loss:1.5124154090881348\n",
      "Training loss:1.548425555229187\n",
      "Training loss:1.5061943531036377\n",
      "Training loss:1.4773439168930054\n",
      "Training loss:1.5069355964660645\n",
      "Training loss:1.4780998229980469\n",
      "Training loss:1.4991036653518677\n",
      "Training loss:1.5937306880950928\n",
      "Training loss:1.53300940990448\n",
      "Training loss:1.5244885683059692\n",
      "Training loss:1.480794072151184\n",
      "Training loss:1.5201200246810913\n",
      "Training loss:1.4657517671585083\n",
      "Training loss:1.4340496063232422\n",
      "Training loss:1.4229567050933838\n",
      "Training loss:1.520990252494812\n",
      "Training loss:1.4684618711471558\n",
      "Training loss:1.5778827667236328\n",
      "Training loss:1.5195108652114868\n",
      "Training loss:1.5454511642456055\n",
      "Training loss:1.4587647914886475\n",
      "Training loss:1.5984032154083252\n",
      "Training loss:1.4129300117492676\n",
      "Training loss:1.5453130006790161\n",
      "Training loss:1.52335786819458\n",
      "Training loss:1.4602546691894531\n",
      "Training loss:1.5298928022384644\n",
      "Training loss:1.5065492391586304\n",
      "Training loss:1.5723150968551636\n",
      "Training loss:1.5322377681732178\n",
      "Training loss:1.4882864952087402\n",
      "Training loss:1.4787087440490723\n",
      "Training loss:1.5410574674606323\n",
      "Training loss:1.4806149005889893\n",
      "Training loss:1.4991306066513062\n",
      "Training loss:1.3866578340530396\n",
      "Training loss:1.541218876838684\n",
      "Training loss:1.5764912366867065\n",
      "Training loss:1.5150452852249146\n",
      "Training loss:1.4767016172409058\n",
      "Training loss:1.47800612449646\n",
      "Training loss:1.4386192560195923\n",
      "Training loss:1.4371620416641235\n",
      "Training loss:1.4734026193618774\n",
      "Training loss:1.528125286102295\n",
      "Training loss:1.4594862461090088\n",
      "Training loss:1.5140774250030518\n",
      "Training loss:1.4770516157150269\n",
      "Training loss:1.5758905410766602\n",
      "Training loss:1.4897799491882324\n",
      "Training loss:1.439321517944336\n",
      "Training loss:1.5198458433151245\n",
      "Training loss:1.516951560974121\n",
      "Training loss:1.4390628337860107\n",
      "Training loss:1.5776300430297852\n",
      "Training loss:1.4730792045593262\n",
      "Training loss:1.4439345598220825\n",
      "Training loss:1.5140273571014404\n",
      "Training loss:1.4519450664520264\n",
      "Training loss:1.5748355388641357\n",
      "Training loss:1.5325132608413696\n",
      "Training loss:1.5235378742218018\n",
      "Training loss:1.4607350826263428\n",
      "Training loss:1.5085090398788452\n",
      "Epoch: 0164 loss_train: 140.786637 acc_train: 0.456354 loss_val: 17.744858 acc_val: 0.463167 time: 422478.069988s\n",
      "Training loss:1.457991600036621\n",
      "Training loss:1.495099425315857\n",
      "Training loss:1.4939740896224976\n",
      "Training loss:1.5482853651046753\n",
      "Training loss:1.494398832321167\n",
      "Training loss:1.515367865562439\n",
      "Training loss:1.561734676361084\n",
      "Training loss:1.479227900505066\n",
      "Training loss:1.550790548324585\n",
      "Training loss:1.5382232666015625\n",
      "Training loss:1.505068302154541\n",
      "Training loss:1.4831463098526\n",
      "Training loss:1.4670195579528809\n",
      "Training loss:1.473178744316101\n",
      "Training loss:1.4757978916168213\n",
      "Training loss:1.4652138948440552\n",
      "Training loss:1.588483452796936\n",
      "Training loss:1.4935171604156494\n",
      "Training loss:1.5394158363342285\n",
      "Training loss:1.385591983795166\n",
      "Training loss:1.5284647941589355\n",
      "Training loss:1.4926087856292725\n",
      "Training loss:1.5177994966506958\n",
      "Training loss:1.5341211557388306\n",
      "Training loss:1.432096242904663\n",
      "Training loss:1.569335699081421\n",
      "Training loss:1.5164202451705933\n",
      "Training loss:1.5307064056396484\n",
      "Training loss:1.5250381231307983\n",
      "Training loss:1.4509522914886475\n",
      "Training loss:1.537537693977356\n",
      "Training loss:1.5406055450439453\n",
      "Training loss:1.4690159559249878\n",
      "Training loss:1.4745982885360718\n",
      "Training loss:1.4246779680252075\n",
      "Training loss:1.4759857654571533\n",
      "Training loss:1.4249294996261597\n",
      "Training loss:1.5389132499694824\n",
      "Training loss:1.5169402360916138\n",
      "Training loss:1.5716012716293335\n",
      "Training loss:1.431910514831543\n",
      "Training loss:1.4764753580093384\n",
      "Training loss:1.4750690460205078\n",
      "Training loss:1.4196301698684692\n",
      "Training loss:1.471024513244629\n",
      "Training loss:1.4361462593078613\n",
      "Training loss:1.4921042919158936\n",
      "Training loss:1.5967018604278564\n",
      "Training loss:1.4588932991027832\n",
      "Training loss:1.553205966949463\n",
      "Training loss:1.4933189153671265\n",
      "Training loss:1.4235633611679077\n",
      "Training loss:1.49825918674469\n",
      "Training loss:1.4981778860092163\n",
      "Training loss:1.462783694267273\n",
      "Training loss:1.4915205240249634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5566706657409668\n",
      "Training loss:1.5408601760864258\n",
      "Training loss:1.4950793981552124\n",
      "Training loss:1.451325535774231\n",
      "Training loss:1.471747636795044\n",
      "Training loss:1.5034003257751465\n",
      "Training loss:1.4119206666946411\n",
      "Training loss:1.4971532821655273\n",
      "Training loss:1.5558134317398071\n",
      "Training loss:1.4943495988845825\n",
      "Training loss:1.4912484884262085\n",
      "Training loss:1.539254903793335\n",
      "Training loss:1.533789873123169\n",
      "Training loss:1.5380213260650635\n",
      "Training loss:1.5005993843078613\n",
      "Training loss:1.5282056331634521\n",
      "Training loss:1.3988898992538452\n",
      "Training loss:1.5179575681686401\n",
      "Training loss:1.5269006490707397\n",
      "Training loss:1.4477646350860596\n",
      "Training loss:1.458730697631836\n",
      "Training loss:1.5005677938461304\n",
      "Training loss:1.487895131111145\n",
      "Training loss:1.4764997959136963\n",
      "Training loss:1.5161421298980713\n",
      "Training loss:1.4507856369018555\n",
      "Training loss:1.5353450775146484\n",
      "Training loss:1.3959417343139648\n",
      "Training loss:1.5163989067077637\n",
      "Training loss:1.473881721496582\n",
      "Training loss:1.4505466222763062\n",
      "Training loss:1.4801934957504272\n",
      "Training loss:1.5497515201568604\n",
      "Training loss:1.44667387008667\n",
      "Training loss:1.4864639043807983\n",
      "Training loss:1.4853512048721313\n",
      "Training loss:1.509298324584961\n",
      "Training loss:1.500542163848877\n",
      "Epoch: 0165 loss_train: 140.484623 acc_train: 0.456167 loss_val: 17.746902 acc_val: 0.464000 time: 424924.959883s\n",
      "Training loss:1.4544343948364258\n",
      "Training loss:1.5505799055099487\n",
      "Training loss:1.4777339696884155\n",
      "Training loss:1.6012953519821167\n",
      "Training loss:1.5651276111602783\n",
      "Training loss:1.4778907299041748\n",
      "Training loss:1.4715808629989624\n",
      "Training loss:1.462895393371582\n",
      "Training loss:1.4758723974227905\n",
      "Training loss:1.4522252082824707\n",
      "Training loss:1.5034083127975464\n",
      "Training loss:1.5286861658096313\n",
      "Training loss:1.3973311185836792\n",
      "Training loss:1.483128309249878\n",
      "Training loss:1.3698196411132812\n",
      "Training loss:1.503856897354126\n",
      "Training loss:1.419981598854065\n",
      "Training loss:1.463686466217041\n",
      "Training loss:1.4820151329040527\n",
      "Training loss:1.4434363842010498\n",
      "Training loss:1.4715784788131714\n",
      "Training loss:1.4885995388031006\n",
      "Training loss:1.5235751867294312\n",
      "Training loss:1.462441086769104\n",
      "Training loss:1.5268595218658447\n",
      "Training loss:1.4455221891403198\n",
      "Training loss:1.4883207082748413\n",
      "Training loss:1.5595502853393555\n",
      "Training loss:1.4593639373779297\n",
      "Training loss:1.5209307670593262\n",
      "Training loss:1.4385895729064941\n",
      "Training loss:1.473631739616394\n",
      "Training loss:1.5546104907989502\n",
      "Training loss:1.490540623664856\n",
      "Training loss:1.4274288415908813\n",
      "Training loss:1.488767147064209\n",
      "Training loss:1.4350152015686035\n",
      "Training loss:1.498842477798462\n",
      "Training loss:1.5144126415252686\n",
      "Training loss:1.503942608833313\n",
      "Training loss:1.4967751502990723\n",
      "Training loss:1.4498764276504517\n",
      "Training loss:1.498917579650879\n",
      "Training loss:1.501800775527954\n",
      "Training loss:1.4982532262802124\n",
      "Training loss:1.5248831510543823\n",
      "Training loss:1.5678329467773438\n",
      "Training loss:1.4736071825027466\n",
      "Training loss:1.4829820394515991\n",
      "Training loss:1.3461745977401733\n",
      "Training loss:1.5138177871704102\n",
      "Training loss:1.4652395248413086\n",
      "Training loss:1.5618712902069092\n",
      "Training loss:1.4755849838256836\n",
      "Training loss:1.4560277462005615\n",
      "Training loss:1.5005438327789307\n",
      "Training loss:1.536537766456604\n",
      "Training loss:1.541553020477295\n",
      "Training loss:1.4791476726531982\n",
      "Training loss:1.4922065734863281\n",
      "Training loss:1.5300801992416382\n",
      "Training loss:1.4730582237243652\n",
      "Training loss:1.555059790611267\n",
      "Training loss:1.4453089237213135\n",
      "Training loss:1.4407867193222046\n",
      "Training loss:1.4841773509979248\n",
      "Training loss:1.436526894569397\n",
      "Training loss:1.494964361190796\n",
      "Training loss:1.4099009037017822\n",
      "Training loss:1.5011509656906128\n",
      "Training loss:1.5082228183746338\n",
      "Training loss:1.5342826843261719\n",
      "Training loss:1.5371726751327515\n",
      "Training loss:1.4619086980819702\n",
      "Training loss:1.432455062866211\n",
      "Training loss:1.4878923892974854\n",
      "Training loss:1.5018320083618164\n",
      "Training loss:1.4846395254135132\n",
      "Training loss:1.5365257263183594\n",
      "Training loss:1.5305564403533936\n",
      "Training loss:1.428662896156311\n",
      "Training loss:1.5390429496765137\n",
      "Training loss:1.561201572418213\n",
      "Training loss:1.5332787036895752\n",
      "Training loss:1.5442829132080078\n",
      "Training loss:1.5259993076324463\n",
      "Training loss:1.4519755840301514\n",
      "Training loss:1.5339940786361694\n",
      "Training loss:1.548755407333374\n",
      "Training loss:1.5397125482559204\n",
      "Training loss:1.4927769899368286\n",
      "Training loss:1.5839073657989502\n",
      "Training loss:1.4788583517074585\n",
      "Training loss:1.4982261657714844\n",
      "Epoch: 0166 loss_train: 140.268319 acc_train: 0.459250 loss_val: 17.910287 acc_val: 0.456333 time: 427383.523231s\n",
      "Training loss:1.5199434757232666\n",
      "Training loss:1.4216352701187134\n",
      "Training loss:1.5801512002944946\n",
      "Training loss:1.4317179918289185\n",
      "Training loss:1.607746958732605\n",
      "Training loss:1.4807900190353394\n",
      "Training loss:1.4518482685089111\n",
      "Training loss:1.4299415349960327\n",
      "Training loss:1.4979079961776733\n",
      "Training loss:1.5567129850387573\n",
      "Training loss:1.5357486009597778\n",
      "Training loss:1.4787969589233398\n",
      "Training loss:1.510663390159607\n",
      "Training loss:1.44767165184021\n",
      "Training loss:1.5684741735458374\n",
      "Training loss:1.5570473670959473\n",
      "Training loss:1.4665285348892212\n",
      "Training loss:1.5829875469207764\n",
      "Training loss:1.4880824089050293\n",
      "Training loss:1.5458365678787231\n",
      "Training loss:1.4852641820907593\n",
      "Training loss:1.5207548141479492\n",
      "Training loss:1.5357270240783691\n",
      "Training loss:1.4879485368728638\n",
      "Training loss:1.4457294940948486\n",
      "Training loss:1.450250506401062\n",
      "Training loss:1.561325192451477\n",
      "Training loss:1.530470371246338\n",
      "Training loss:1.5045222043991089\n",
      "Training loss:1.5013023614883423\n",
      "Training loss:1.537366509437561\n",
      "Training loss:1.4771431684494019\n",
      "Training loss:1.4618773460388184\n",
      "Training loss:1.5150655508041382\n",
      "Training loss:1.445868730545044\n",
      "Training loss:1.5453007221221924\n",
      "Training loss:1.5010113716125488\n",
      "Training loss:1.494346022605896\n",
      "Training loss:1.4568229913711548\n",
      "Training loss:1.4885307550430298\n",
      "Training loss:1.5097546577453613\n",
      "Training loss:1.5114761590957642\n",
      "Training loss:1.478247046470642\n",
      "Training loss:1.5633742809295654\n",
      "Training loss:1.4982296228408813\n",
      "Training loss:1.4782376289367676\n",
      "Training loss:1.4840363264083862\n",
      "Training loss:1.5041996240615845\n",
      "Training loss:1.5742011070251465\n",
      "Training loss:1.5108823776245117\n",
      "Training loss:1.417921543121338\n",
      "Training loss:1.4504566192626953\n",
      "Training loss:1.5240874290466309\n",
      "Training loss:1.5421773195266724\n",
      "Training loss:1.4573376178741455\n",
      "Training loss:1.4377669095993042\n",
      "Training loss:1.4436039924621582\n",
      "Training loss:1.4928818941116333\n",
      "Training loss:1.4949687719345093\n",
      "Training loss:1.4790173768997192\n",
      "Training loss:1.5201523303985596\n",
      "Training loss:1.3811742067337036\n",
      "Training loss:1.4459699392318726\n",
      "Training loss:1.5249943733215332\n",
      "Training loss:1.4462051391601562\n",
      "Training loss:1.5085415840148926\n",
      "Training loss:1.5083963871002197\n",
      "Training loss:1.456839680671692\n",
      "Training loss:1.504631519317627\n",
      "Training loss:1.5298784971237183\n",
      "Training loss:1.4531266689300537\n",
      "Training loss:1.531146764755249\n",
      "Training loss:1.5229295492172241\n",
      "Training loss:1.5143259763717651\n",
      "Training loss:1.5253300666809082\n",
      "Training loss:1.4683672189712524\n",
      "Training loss:1.5815348625183105\n",
      "Training loss:1.4505107402801514\n",
      "Training loss:1.4062753915786743\n",
      "Training loss:1.5477471351623535\n",
      "Training loss:1.4989362955093384\n",
      "Training loss:1.4495961666107178\n",
      "Training loss:1.4885741472244263\n",
      "Training loss:1.4434254169464111\n",
      "Training loss:1.4892290830612183\n",
      "Training loss:1.461555004119873\n",
      "Training loss:1.509793996810913\n",
      "Training loss:1.5214290618896484\n",
      "Training loss:1.4839204549789429\n",
      "Training loss:1.4813199043273926\n",
      "Training loss:1.4446560144424438\n",
      "Training loss:1.5017611980438232\n",
      "Training loss:1.4910181760787964\n",
      "Training loss:1.5380815267562866\n",
      "Epoch: 0167 loss_train: 140.591094 acc_train: 0.455375 loss_val: 18.044879 acc_val: 0.455167 time: 429858.283571s\n",
      "Training loss:1.5047188997268677\n",
      "Training loss:1.521457552909851\n",
      "Training loss:1.5832982063293457\n",
      "Training loss:1.4679936170578003\n",
      "Training loss:1.5167311429977417\n",
      "Training loss:1.4764642715454102\n",
      "Training loss:1.4981985092163086\n",
      "Training loss:1.4338562488555908\n",
      "Training loss:1.5785751342773438\n",
      "Training loss:1.529753565788269\n",
      "Training loss:1.513096570968628\n",
      "Training loss:1.4576586484909058\n",
      "Training loss:1.5622496604919434\n",
      "Training loss:1.5009592771530151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.4699338674545288\n",
      "Training loss:1.4757272005081177\n",
      "Training loss:1.4744768142700195\n",
      "Training loss:1.5310776233673096\n",
      "Training loss:1.5386171340942383\n",
      "Training loss:1.5352075099945068\n",
      "Training loss:1.5058302879333496\n",
      "Training loss:1.4142712354660034\n",
      "Training loss:1.4951646327972412\n",
      "Training loss:1.5230735540390015\n",
      "Training loss:1.4968687295913696\n",
      "Training loss:1.540411353111267\n",
      "Training loss:1.5411373376846313\n",
      "Training loss:1.4328913688659668\n",
      "Training loss:1.530811071395874\n",
      "Training loss:1.5409340858459473\n",
      "Training loss:1.4603852033615112\n",
      "Training loss:1.4228254556655884\n",
      "Training loss:1.4901502132415771\n",
      "Training loss:1.5027968883514404\n",
      "Training loss:1.4744517803192139\n",
      "Training loss:1.6159483194351196\n",
      "Training loss:1.4765821695327759\n",
      "Training loss:1.4809147119522095\n",
      "Training loss:1.508360505104065\n",
      "Training loss:1.4783974885940552\n",
      "Training loss:1.4105740785598755\n",
      "Training loss:1.5141865015029907\n",
      "Training loss:1.443919062614441\n",
      "Training loss:1.4179050922393799\n",
      "Training loss:1.4233872890472412\n",
      "Training loss:1.5139195919036865\n",
      "Training loss:1.4435409307479858\n",
      "Training loss:1.4369758367538452\n",
      "Training loss:1.502478837966919\n",
      "Training loss:1.519264817237854\n",
      "Training loss:1.438859462738037\n",
      "Training loss:1.5167490243911743\n",
      "Training loss:1.4083449840545654\n",
      "Training loss:1.5490533113479614\n",
      "Training loss:1.4978219270706177\n",
      "Training loss:1.487324595451355\n",
      "Training loss:1.415329933166504\n",
      "Training loss:1.479754090309143\n",
      "Training loss:1.4868284463882446\n",
      "Training loss:1.5039790868759155\n",
      "Training loss:1.5025928020477295\n",
      "Training loss:1.4692461490631104\n",
      "Training loss:1.5130839347839355\n",
      "Training loss:1.433819055557251\n",
      "Training loss:1.4666985273361206\n",
      "Training loss:1.465372920036316\n",
      "Training loss:1.5091791152954102\n",
      "Training loss:1.515215277671814\n",
      "Training loss:1.5350309610366821\n",
      "Training loss:1.4866172075271606\n",
      "Training loss:1.5327024459838867\n",
      "Training loss:1.4931361675262451\n",
      "Training loss:1.585952877998352\n",
      "Training loss:1.4601500034332275\n",
      "Training loss:1.4503787755966187\n",
      "Training loss:1.5238595008850098\n",
      "Training loss:1.4697895050048828\n",
      "Training loss:1.4747499227523804\n",
      "Training loss:1.5398633480072021\n",
      "Training loss:1.4453233480453491\n",
      "Training loss:1.439285397529602\n",
      "Training loss:1.4903483390808105\n",
      "Training loss:1.4304801225662231\n",
      "Training loss:1.4544320106506348\n",
      "Training loss:1.4510948657989502\n",
      "Training loss:1.4756489992141724\n",
      "Training loss:1.4473674297332764\n",
      "Training loss:1.5020915269851685\n",
      "Training loss:1.487991452217102\n",
      "Training loss:1.440886378288269\n",
      "Training loss:1.5142203569412231\n",
      "Training loss:1.451216459274292\n",
      "Training loss:1.4802597761154175\n",
      "Training loss:1.525694727897644\n",
      "Epoch: 0168 loss_train: 139.976234 acc_train: 0.458917 loss_val: 17.790398 acc_val: 0.460000 time: 432367.447999s\n",
      "Training loss:1.5422133207321167\n",
      "Training loss:1.4722899198532104\n",
      "Training loss:1.5143725872039795\n",
      "Training loss:1.4667401313781738\n",
      "Training loss:1.4932249784469604\n",
      "Training loss:1.4806932210922241\n",
      "Training loss:1.5422390699386597\n",
      "Training loss:1.5751888751983643\n",
      "Training loss:1.3463935852050781\n",
      "Training loss:1.5078917741775513\n",
      "Training loss:1.509921908378601\n",
      "Training loss:1.4267029762268066\n",
      "Training loss:1.497101068496704\n",
      "Training loss:1.4520533084869385\n",
      "Training loss:1.5519323348999023\n",
      "Training loss:1.4653528928756714\n",
      "Training loss:1.423619031906128\n",
      "Training loss:1.4161961078643799\n",
      "Training loss:1.4381976127624512\n",
      "Training loss:1.4650871753692627\n",
      "Training loss:1.4895063638687134\n",
      "Training loss:1.5727519989013672\n",
      "Training loss:1.616923451423645\n",
      "Training loss:1.5405794382095337\n",
      "Training loss:1.4696844816207886\n",
      "Training loss:1.5195109844207764\n",
      "Training loss:1.4833664894104004\n",
      "Training loss:1.568051815032959\n",
      "Training loss:1.5504791736602783\n",
      "Training loss:1.5142292976379395\n",
      "Training loss:1.5382847785949707\n",
      "Training loss:1.4872206449508667\n",
      "Training loss:1.4422729015350342\n",
      "Training loss:1.513433575630188\n",
      "Training loss:1.5180827379226685\n",
      "Training loss:1.495152235031128\n",
      "Training loss:1.4699547290802002\n",
      "Training loss:1.5061733722686768\n",
      "Training loss:1.4363139867782593\n",
      "Training loss:1.5460437536239624\n",
      "Training loss:1.4096267223358154\n",
      "Training loss:1.5221837759017944\n",
      "Training loss:1.468906044960022\n",
      "Training loss:1.4679021835327148\n",
      "Training loss:1.508446455001831\n",
      "Training loss:1.5822278261184692\n",
      "Training loss:1.4215670824050903\n",
      "Training loss:1.5132471323013306\n",
      "Training loss:1.4326504468917847\n",
      "Training loss:1.5317962169647217\n",
      "Training loss:1.4670337438583374\n",
      "Training loss:1.4614485502243042\n",
      "Training loss:1.5233635902404785\n",
      "Training loss:1.502191424369812\n",
      "Training loss:1.479966402053833\n",
      "Training loss:1.5199573040008545\n",
      "Training loss:1.4261682033538818\n",
      "Training loss:1.5576069355010986\n",
      "Training loss:1.396877646446228\n",
      "Training loss:1.4777514934539795\n",
      "Training loss:1.5253077745437622\n",
      "Training loss:1.4998914003372192\n",
      "Training loss:1.4270784854888916\n",
      "Training loss:1.4924715757369995\n",
      "Training loss:1.5345975160598755\n",
      "Training loss:1.4599125385284424\n",
      "Training loss:1.5103881359100342\n",
      "Training loss:1.4992364645004272\n",
      "Training loss:1.48101007938385\n",
      "Training loss:1.4846997261047363\n",
      "Training loss:1.5639369487762451\n",
      "Training loss:1.4186785221099854\n",
      "Training loss:1.5181896686553955\n",
      "Training loss:1.4313730001449585\n",
      "Training loss:1.5037215948104858\n",
      "Training loss:1.4752048254013062\n",
      "Training loss:1.4732747077941895\n",
      "Training loss:1.5251610279083252\n",
      "Training loss:1.5215097665786743\n",
      "Training loss:1.4514427185058594\n",
      "Training loss:1.4700082540512085\n",
      "Training loss:1.4431227445602417\n",
      "Training loss:1.3920050859451294\n",
      "Training loss:1.490881085395813\n",
      "Training loss:1.4860979318618774\n",
      "Training loss:1.5515938997268677\n",
      "Training loss:1.4998836517333984\n",
      "Training loss:1.5719503164291382\n",
      "Training loss:1.4603036642074585\n",
      "Training loss:1.5066108703613281\n",
      "Training loss:1.521625280380249\n",
      "Training loss:1.4661517143249512\n",
      "Training loss:1.5893689393997192\n",
      "Training loss:1.5531601905822754\n",
      "Epoch: 0169 loss_train: 140.336197 acc_train: 0.457667 loss_val: 17.784076 acc_val: 0.464500 time: 434856.909820s\n",
      "Training loss:1.4920235872268677\n",
      "Training loss:1.5808311700820923\n",
      "Training loss:1.433737874031067\n",
      "Training loss:1.5357447862625122\n",
      "Training loss:1.5335595607757568\n",
      "Training loss:1.4666941165924072\n",
      "Training loss:1.4703859090805054\n",
      "Training loss:1.5308992862701416\n",
      "Training loss:1.4898803234100342\n",
      "Training loss:1.4466688632965088\n",
      "Training loss:1.4317249059677124\n",
      "Training loss:1.5126768350601196\n",
      "Training loss:1.5012881755828857\n",
      "Training loss:1.42534339427948\n",
      "Training loss:1.416845440864563\n",
      "Training loss:1.4511609077453613\n",
      "Training loss:1.4305890798568726\n",
      "Training loss:1.4841632843017578\n",
      "Training loss:1.4135138988494873\n",
      "Training loss:1.4533954858779907\n",
      "Training loss:1.5124472379684448\n",
      "Training loss:1.532631278038025\n",
      "Training loss:1.4566389322280884\n",
      "Training loss:1.5682293176651\n",
      "Training loss:1.4421817064285278\n",
      "Training loss:1.4339661598205566\n",
      "Training loss:1.5527526140213013\n",
      "Training loss:1.5767909288406372\n",
      "Training loss:1.513411045074463\n",
      "Training loss:1.4689205884933472\n",
      "Training loss:1.5031678676605225\n",
      "Training loss:1.4776089191436768\n",
      "Training loss:1.5005606412887573\n",
      "Training loss:1.4629229307174683\n",
      "Training loss:1.4507607221603394\n",
      "Training loss:1.50998854637146\n",
      "Training loss:1.5047982931137085\n",
      "Training loss:1.4554105997085571\n",
      "Training loss:1.437163233757019\n",
      "Training loss:1.4244608879089355\n",
      "Training loss:1.4994292259216309\n",
      "Training loss:1.5051932334899902\n",
      "Training loss:1.4558583498001099\n",
      "Training loss:1.445527195930481\n",
      "Training loss:1.5267260074615479\n",
      "Training loss:1.530400037765503\n",
      "Training loss:1.3752918243408203\n",
      "Training loss:1.5656659603118896\n",
      "Training loss:1.5287173986434937\n",
      "Training loss:1.5125010013580322\n",
      "Training loss:1.5957661867141724\n",
      "Training loss:1.4769110679626465\n",
      "Training loss:1.4234405755996704\n",
      "Training loss:1.5072383880615234\n",
      "Training loss:1.4840917587280273\n",
      "Training loss:1.4894533157348633\n",
      "Training loss:1.5355474948883057\n",
      "Training loss:1.4738986492156982\n",
      "Training loss:1.419623613357544\n",
      "Training loss:1.4308935403823853\n",
      "Training loss:1.434577465057373\n",
      "Training loss:1.4305437803268433\n",
      "Training loss:1.5061829090118408\n",
      "Training loss:1.5381629467010498\n",
      "Training loss:1.5679341554641724\n",
      "Training loss:1.5076086521148682\n",
      "Training loss:1.4919509887695312\n",
      "Training loss:1.4690231084823608\n",
      "Training loss:1.521097183227539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5248644351959229\n",
      "Training loss:1.5615475177764893\n",
      "Training loss:1.5119754076004028\n",
      "Training loss:1.4643619060516357\n",
      "Training loss:1.4548553228378296\n",
      "Training loss:1.5651437044143677\n",
      "Training loss:1.4506608247756958\n",
      "Training loss:1.5273644924163818\n",
      "Training loss:1.4607008695602417\n",
      "Training loss:1.5161776542663574\n",
      "Training loss:1.4522147178649902\n",
      "Training loss:1.4841597080230713\n",
      "Training loss:1.549641728401184\n",
      "Training loss:1.4548391103744507\n",
      "Training loss:1.4233206510543823\n",
      "Training loss:1.5318442583084106\n",
      "Training loss:1.5866786241531372\n",
      "Training loss:1.4381047487258911\n",
      "Training loss:1.4583120346069336\n",
      "Training loss:1.5627272129058838\n",
      "Training loss:1.4697821140289307\n",
      "Training loss:1.594197154045105\n",
      "Training loss:1.5744719505310059\n",
      "Training loss:1.521623134613037\n",
      "Training loss:1.433283805847168\n",
      "Epoch: 0170 loss_train: 140.108048 acc_train: 0.459229 loss_val: 17.995778 acc_val: 0.453667 time: 437347.146278s\n",
      "Training loss:1.472034215927124\n",
      "Training loss:1.5417132377624512\n",
      "Training loss:1.4818241596221924\n",
      "Training loss:1.5361379384994507\n",
      "Training loss:1.4459972381591797\n",
      "Training loss:1.4910756349563599\n",
      "Training loss:1.4788762331008911\n",
      "Training loss:1.5179985761642456\n",
      "Training loss:1.4779764413833618\n",
      "Training loss:1.4999120235443115\n",
      "Training loss:1.4814056158065796\n",
      "Training loss:1.544520378112793\n",
      "Training loss:1.5176094770431519\n",
      "Training loss:1.465339183807373\n",
      "Training loss:1.54535710811615\n",
      "Training loss:1.4792855978012085\n",
      "Training loss:1.5156067609786987\n",
      "Training loss:1.528630256652832\n",
      "Training loss:1.5176526308059692\n",
      "Training loss:1.5401448011398315\n",
      "Training loss:1.458940029144287\n",
      "Training loss:1.4795758724212646\n",
      "Training loss:1.4507864713668823\n",
      "Training loss:1.4581419229507446\n",
      "Training loss:1.4762452840805054\n",
      "Training loss:1.4688817262649536\n",
      "Training loss:1.4714099168777466\n",
      "Training loss:1.5502686500549316\n",
      "Training loss:1.5284308195114136\n",
      "Training loss:1.4937375783920288\n",
      "Training loss:1.4631352424621582\n",
      "Training loss:1.470535397529602\n",
      "Training loss:1.4627400636672974\n",
      "Training loss:1.5215427875518799\n",
      "Training loss:1.4356367588043213\n",
      "Training loss:1.5595985651016235\n",
      "Training loss:1.4675229787826538\n",
      "Training loss:1.4897063970565796\n",
      "Training loss:1.5113195180892944\n",
      "Training loss:1.4965366125106812\n",
      "Training loss:1.40670645236969\n",
      "Training loss:1.4899218082427979\n",
      "Training loss:1.455945372581482\n",
      "Training loss:1.4441757202148438\n",
      "Training loss:1.4834789037704468\n",
      "Training loss:1.4386394023895264\n",
      "Training loss:1.4750697612762451\n",
      "Training loss:1.481195092201233\n",
      "Training loss:1.44730544090271\n",
      "Training loss:1.4496995210647583\n",
      "Training loss:1.5104990005493164\n",
      "Training loss:1.4953653812408447\n",
      "Training loss:1.5326687097549438\n",
      "Training loss:1.4020812511444092\n",
      "Training loss:1.3995558023452759\n",
      "Training loss:1.5644868612289429\n",
      "Training loss:1.4594353437423706\n",
      "Training loss:1.52238130569458\n",
      "Training loss:1.4311705827713013\n",
      "Training loss:1.471635103225708\n",
      "Training loss:1.4482684135437012\n",
      "Training loss:1.4058139324188232\n",
      "Training loss:1.5272871255874634\n",
      "Training loss:1.4729114770889282\n",
      "Training loss:1.4923205375671387\n",
      "Training loss:1.5663602352142334\n",
      "Training loss:1.516121745109558\n",
      "Training loss:1.4477710723876953\n",
      "Training loss:1.4546504020690918\n",
      "Training loss:1.4298579692840576\n",
      "Training loss:1.5605792999267578\n",
      "Training loss:1.501335859298706\n",
      "Training loss:1.5297268629074097\n",
      "Training loss:1.483520269393921\n",
      "Training loss:1.5085461139678955\n",
      "Training loss:1.5211467742919922\n",
      "Training loss:1.5902345180511475\n",
      "Training loss:1.5689266920089722\n",
      "Training loss:1.5220590829849243\n",
      "Training loss:1.60713529586792\n",
      "Training loss:1.462391972541809\n",
      "Training loss:1.4538017511367798\n",
      "Training loss:1.4805922508239746\n",
      "Training loss:1.5315582752227783\n",
      "Training loss:1.5036187171936035\n",
      "Training loss:1.5664684772491455\n",
      "Training loss:1.4598406553268433\n",
      "Training loss:1.4173967838287354\n",
      "Training loss:1.4820237159729004\n",
      "Training loss:1.6005423069000244\n",
      "Training loss:1.487579345703125\n",
      "Training loss:1.4745862483978271\n",
      "Training loss:1.4566996097564697\n",
      "Training loss:1.4940285682678223\n",
      "Epoch: 0171 loss_train: 140.180909 acc_train: 0.456813 loss_val: 17.906858 acc_val: 0.464000 time: 440024.779289s\n",
      "Training loss:1.5924198627471924\n",
      "Training loss:1.4857882261276245\n",
      "Training loss:1.4080326557159424\n",
      "Training loss:1.4632034301757812\n",
      "Training loss:1.497902750968933\n",
      "Training loss:1.4960380792617798\n",
      "Training loss:1.4920194149017334\n",
      "Training loss:1.4947285652160645\n",
      "Training loss:1.434983491897583\n",
      "Training loss:1.3989554643630981\n",
      "Training loss:1.515535831451416\n",
      "Training loss:1.5912808179855347\n",
      "Training loss:1.5309264659881592\n",
      "Training loss:1.5457193851470947\n",
      "Training loss:1.4544695615768433\n",
      "Training loss:1.4280120134353638\n",
      "Training loss:1.4615994691848755\n",
      "Training loss:1.448441505432129\n",
      "Training loss:1.5028711557388306\n",
      "Training loss:1.496543288230896\n",
      "Training loss:1.4790186882019043\n",
      "Training loss:1.4850422143936157\n",
      "Training loss:1.410032033920288\n",
      "Training loss:1.5513556003570557\n",
      "Training loss:1.6003084182739258\n",
      "Training loss:1.4696779251098633\n",
      "Training loss:1.4967931509017944\n",
      "Training loss:1.5656075477600098\n",
      "Training loss:1.4620260000228882\n",
      "Training loss:1.479501724243164\n",
      "Training loss:1.4545327425003052\n",
      "Training loss:1.5530563592910767\n",
      "Training loss:1.4999966621398926\n",
      "Training loss:1.5068540573120117\n",
      "Training loss:1.519529104232788\n",
      "Training loss:1.5277471542358398\n",
      "Training loss:1.4706279039382935\n",
      "Training loss:1.5290158987045288\n",
      "Training loss:1.5257642269134521\n",
      "Training loss:1.579345464706421\n",
      "Training loss:1.4657055139541626\n",
      "Training loss:1.4097505807876587\n",
      "Training loss:1.4663214683532715\n",
      "Training loss:1.5381851196289062\n",
      "Training loss:1.4551951885223389\n",
      "Training loss:1.5165513753890991\n",
      "Training loss:1.497732400894165\n",
      "Training loss:1.411848783493042\n",
      "Training loss:1.4816561937332153\n",
      "Training loss:1.4518269300460815\n",
      "Training loss:1.4209364652633667\n",
      "Training loss:1.4987280368804932\n",
      "Training loss:1.4113647937774658\n",
      "Training loss:1.4269757270812988\n",
      "Training loss:1.5299521684646606\n",
      "Training loss:1.528464913368225\n",
      "Training loss:1.5284459590911865\n",
      "Training loss:1.4712903499603271\n",
      "Training loss:1.5216342210769653\n",
      "Training loss:1.4589033126831055\n",
      "Training loss:1.4816222190856934\n",
      "Training loss:1.4835305213928223\n",
      "Training loss:1.4994697570800781\n",
      "Training loss:1.4443457126617432\n",
      "Training loss:1.4517015218734741\n",
      "Training loss:1.5214534997940063\n",
      "Training loss:1.5376302003860474\n",
      "Training loss:1.380489468574524\n",
      "Training loss:1.515668272972107\n",
      "Training loss:1.521152138710022\n",
      "Training loss:1.4386833906173706\n",
      "Training loss:1.433188796043396\n",
      "Training loss:1.4276643991470337\n",
      "Training loss:1.4760531187057495\n",
      "Training loss:1.3866124153137207\n",
      "Training loss:1.492627739906311\n",
      "Training loss:1.4860326051712036\n",
      "Training loss:1.436338186264038\n",
      "Training loss:1.503360390663147\n",
      "Training loss:1.4779788255691528\n",
      "Training loss:1.4090560674667358\n",
      "Training loss:1.5215567350387573\n",
      "Training loss:1.509420394897461\n",
      "Training loss:1.471539855003357\n",
      "Training loss:1.5947656631469727\n",
      "Training loss:1.5522645711898804\n",
      "Training loss:1.5399043560028076\n",
      "Training loss:1.4518245458602905\n",
      "Training loss:1.5027413368225098\n",
      "Training loss:1.428810954093933\n",
      "Training loss:1.4593766927719116\n",
      "Training loss:1.5164058208465576\n",
      "Training loss:1.482858657836914\n",
      "Training loss:1.4485090970993042\n",
      "Epoch: 0172 loss_train: 139.681412 acc_train: 0.461042 loss_val: 17.770896 acc_val: 0.463833 time: 442655.374418s\n",
      "Training loss:1.4591574668884277\n",
      "Training loss:1.5132064819335938\n",
      "Training loss:1.44338059425354\n",
      "Training loss:1.4556970596313477\n",
      "Training loss:1.4725087881088257\n",
      "Training loss:1.503625512123108\n",
      "Training loss:1.473252534866333\n",
      "Training loss:1.4535319805145264\n",
      "Training loss:1.5134552717208862\n",
      "Training loss:1.4578651189804077\n",
      "Training loss:1.5608845949172974\n",
      "Training loss:1.459289312362671\n",
      "Training loss:1.5099586248397827\n",
      "Training loss:1.5792951583862305\n",
      "Training loss:1.4206033945083618\n",
      "Training loss:1.4674110412597656\n",
      "Training loss:1.4598475694656372\n",
      "Training loss:1.6203452348709106\n",
      "Training loss:1.4904165267944336\n",
      "Training loss:1.4813423156738281\n",
      "Training loss:1.530348300933838\n",
      "Training loss:1.4420883655548096\n",
      "Training loss:1.4536234140396118\n",
      "Training loss:1.4544464349746704\n",
      "Training loss:1.5512970685958862\n",
      "Training loss:1.5284029245376587\n",
      "Training loss:1.424670934677124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.5241585969924927\n",
      "Training loss:1.452797293663025\n",
      "Training loss:1.4730212688446045\n",
      "Training loss:1.4775300025939941\n",
      "Training loss:1.4888982772827148\n",
      "Training loss:1.5254864692687988\n",
      "Training loss:1.3988316059112549\n",
      "Training loss:1.422582745552063\n",
      "Training loss:1.4711230993270874\n",
      "Training loss:1.479896068572998\n",
      "Training loss:1.4461593627929688\n",
      "Training loss:1.5662225484848022\n",
      "Training loss:1.5432376861572266\n",
      "Training loss:1.4999154806137085\n",
      "Training loss:1.5064587593078613\n",
      "Training loss:1.4864940643310547\n",
      "Training loss:1.5302072763442993\n",
      "Training loss:1.5327409505844116\n",
      "Training loss:1.5520504713058472\n",
      "Training loss:1.507457971572876\n",
      "Training loss:1.545182466506958\n",
      "Training loss:1.438206672668457\n",
      "Training loss:1.4843260049819946\n",
      "Training loss:1.424643874168396\n",
      "Training loss:1.4616787433624268\n",
      "Training loss:1.509918451309204\n",
      "Training loss:1.5063837766647339\n",
      "Training loss:1.4528075456619263\n",
      "Training loss:1.5520597696304321\n",
      "Training loss:1.479952096939087\n",
      "Training loss:1.4402095079421997\n",
      "Training loss:1.4789683818817139\n",
      "Training loss:1.4983998537063599\n",
      "Training loss:1.4451416730880737\n",
      "Training loss:1.4411981105804443\n",
      "Training loss:1.5661498308181763\n",
      "Training loss:1.5403094291687012\n",
      "Training loss:1.5222195386886597\n",
      "Training loss:1.4456841945648193\n",
      "Training loss:1.4995051622390747\n",
      "Training loss:1.4333621263504028\n",
      "Training loss:1.4519217014312744\n",
      "Training loss:1.5130400657653809\n",
      "Training loss:1.4536657333374023\n",
      "Training loss:1.5002365112304688\n",
      "Training loss:1.509926676750183\n",
      "Training loss:1.427466630935669\n",
      "Training loss:1.5021414756774902\n",
      "Training loss:1.4363563060760498\n",
      "Training loss:1.5278680324554443\n",
      "Training loss:1.531508207321167\n",
      "Training loss:1.4353086948394775\n",
      "Training loss:1.5763041973114014\n",
      "Training loss:1.3742133378982544\n",
      "Training loss:1.4513100385665894\n",
      "Training loss:1.5266242027282715\n",
      "Training loss:1.445282220840454\n",
      "Training loss:1.515533208847046\n",
      "Training loss:1.5324532985687256\n",
      "Training loss:1.5662232637405396\n",
      "Training loss:1.4996838569641113\n",
      "Training loss:1.4827662706375122\n",
      "Training loss:1.5438790321350098\n",
      "Training loss:1.5388567447662354\n",
      "Training loss:1.5760501623153687\n",
      "Training loss:1.5320179462432861\n",
      "Training loss:1.4701709747314453\n",
      "Epoch: 0173 loss_train: 140.152338 acc_train: 0.458208 loss_val: 17.726122 acc_val: 0.463667 time: 445139.322182s\n",
      "Training loss:1.5168848037719727\n",
      "Training loss:1.4919936656951904\n",
      "Training loss:1.5071016550064087\n",
      "Training loss:1.410351276397705\n",
      "Training loss:1.4663958549499512\n",
      "Training loss:1.5145224332809448\n",
      "Training loss:1.4600023031234741\n",
      "Training loss:1.4233250617980957\n",
      "Training loss:1.5037205219268799\n",
      "Training loss:1.4082609415054321\n",
      "Training loss:1.487715721130371\n",
      "Training loss:1.5122795104980469\n",
      "Training loss:1.466243028640747\n",
      "Training loss:1.4497838020324707\n",
      "Training loss:1.4763449430465698\n",
      "Training loss:1.5068399906158447\n",
      "Training loss:1.5014562606811523\n",
      "Training loss:1.4817148447036743\n",
      "Training loss:1.4562500715255737\n",
      "Training loss:1.4483304023742676\n",
      "Training loss:1.4990154504776\n",
      "Training loss:1.5337971448898315\n",
      "Training loss:1.4373294115066528\n",
      "Training loss:1.4872969388961792\n",
      "Training loss:1.5649381875991821\n",
      "Training loss:1.5695408582687378\n",
      "Training loss:1.5456775426864624\n",
      "Training loss:1.4669395685195923\n",
      "Training loss:1.5141609907150269\n",
      "Training loss:1.5146539211273193\n",
      "Training loss:1.4644120931625366\n",
      "Training loss:1.4596400260925293\n",
      "Training loss:1.4943705797195435\n",
      "Training loss:1.4663550853729248\n",
      "Training loss:1.4316037893295288\n",
      "Training loss:1.472538948059082\n",
      "Training loss:1.469510793685913\n",
      "Training loss:1.5089545249938965\n",
      "Training loss:1.4902855157852173\n",
      "Training loss:1.4325779676437378\n",
      "Training loss:1.5497853755950928\n",
      "Training loss:1.5091567039489746\n",
      "Training loss:1.457565188407898\n",
      "Training loss:1.5676045417785645\n",
      "Training loss:1.4143383502960205\n",
      "Training loss:1.491122841835022\n",
      "Training loss:1.4904508590698242\n",
      "Training loss:1.557253122329712\n",
      "Training loss:1.4932070970535278\n",
      "Training loss:1.4708011150360107\n",
      "Training loss:1.527021050453186\n",
      "Training loss:1.4678387641906738\n",
      "Training loss:1.394346833229065\n",
      "Training loss:1.5108673572540283\n",
      "Training loss:1.4907420873641968\n",
      "Training loss:1.4796770811080933\n",
      "Training loss:1.437137246131897\n",
      "Training loss:1.525491714477539\n",
      "Training loss:1.489515781402588\n",
      "Training loss:1.5140973329544067\n",
      "Training loss:1.47873854637146\n",
      "Training loss:1.4436239004135132\n",
      "Training loss:1.5347490310668945\n",
      "Training loss:1.5333398580551147\n",
      "Training loss:1.4851154088974\n",
      "Training loss:1.5551600456237793\n",
      "Training loss:1.4694520235061646\n",
      "Training loss:1.4400298595428467\n",
      "Training loss:1.544979214668274\n",
      "Training loss:1.5202997922897339\n",
      "Training loss:1.5707423686981201\n",
      "Training loss:1.470112919807434\n",
      "Training loss:1.4787219762802124\n",
      "Training loss:1.4642415046691895\n",
      "Training loss:1.4648544788360596\n",
      "Training loss:1.4066269397735596\n",
      "Training loss:1.5139938592910767\n",
      "Training loss:1.5106861591339111\n",
      "Training loss:1.4696661233901978\n",
      "Training loss:1.4837477207183838\n",
      "Training loss:1.5335873365402222\n",
      "Training loss:1.4485478401184082\n",
      "Training loss:1.4544297456741333\n",
      "Training loss:1.4053374528884888\n",
      "Training loss:1.5000944137573242\n",
      "Training loss:1.4324275255203247\n",
      "Training loss:1.378065824508667\n",
      "Training loss:1.4490597248077393\n",
      "Training loss:1.528289556503296\n",
      "Training loss:1.49630606174469\n",
      "Training loss:1.4706758260726929\n",
      "Training loss:1.5555751323699951\n",
      "Training loss:1.4622491598129272\n",
      "Training loss:1.5450462102890015\n",
      "Epoch: 0174 loss_train: 139.649710 acc_train: 0.459667 loss_val: 17.867125 acc_val: 0.464500 time: 447601.727107s\n",
      "Training loss:1.3780783414840698\n",
      "Training loss:1.4812934398651123\n",
      "Training loss:1.4324580430984497\n",
      "Training loss:1.4564825296401978\n",
      "Training loss:1.4851926565170288\n",
      "Training loss:1.475024700164795\n",
      "Training loss:1.4216527938842773\n",
      "Training loss:1.490264892578125\n",
      "Training loss:1.489220380783081\n",
      "Training loss:1.4844183921813965\n",
      "Training loss:1.4441057443618774\n",
      "Training loss:1.5518646240234375\n",
      "Training loss:1.5375357866287231\n",
      "Training loss:1.4827603101730347\n",
      "Training loss:1.5178128480911255\n",
      "Training loss:1.5399376153945923\n",
      "Training loss:1.4544978141784668\n",
      "Training loss:1.5802193880081177\n",
      "Training loss:1.5308719873428345\n",
      "Training loss:1.5222251415252686\n",
      "Training loss:1.5762481689453125\n",
      "Training loss:1.4941350221633911\n",
      "Training loss:1.543099045753479\n",
      "Training loss:1.4339613914489746\n",
      "Training loss:1.5610549449920654\n",
      "Training loss:1.5014878511428833\n",
      "Training loss:1.5121067762374878\n",
      "Training loss:1.604191541671753\n",
      "Training loss:1.5012762546539307\n",
      "Training loss:1.4734408855438232\n",
      "Training loss:1.4416836500167847\n",
      "Training loss:1.4071019887924194\n",
      "Training loss:1.5135118961334229\n",
      "Training loss:1.4878050088882446\n",
      "Training loss:1.4762818813323975\n",
      "Training loss:1.419689655303955\n",
      "Training loss:1.5206708908081055\n",
      "Training loss:1.5275212526321411\n",
      "Training loss:1.3786669969558716\n",
      "Training loss:1.5346006155014038\n",
      "Training loss:1.5782817602157593\n",
      "Training loss:1.4406275749206543\n",
      "Training loss:1.5441927909851074\n",
      "Training loss:1.4508293867111206\n",
      "Training loss:1.5018048286437988\n",
      "Training loss:1.5083460807800293\n",
      "Training loss:1.5027788877487183\n",
      "Training loss:1.5157902240753174\n",
      "Training loss:1.501097559928894\n",
      "Training loss:1.5544053316116333\n",
      "Training loss:1.540839672088623\n",
      "Training loss:1.5320079326629639\n",
      "Training loss:1.5169446468353271\n",
      "Training loss:1.532241702079773\n",
      "Training loss:1.4552192687988281\n",
      "Training loss:1.5168648958206177\n",
      "Training loss:1.4677757024765015\n",
      "Training loss:1.4279842376708984\n",
      "Training loss:1.4884699583053589\n",
      "Training loss:1.5319474935531616\n",
      "Training loss:1.4447013139724731\n",
      "Training loss:1.4700413942337036\n",
      "Training loss:1.558538556098938\n",
      "Training loss:1.4669054746627808\n",
      "Training loss:1.42586088180542\n",
      "Training loss:1.4765859842300415\n",
      "Training loss:1.4376579523086548\n",
      "Training loss:1.5079545974731445\n",
      "Training loss:1.4368747472763062\n",
      "Training loss:1.4987765550613403\n",
      "Training loss:1.4176673889160156\n",
      "Training loss:1.4907737970352173\n",
      "Training loss:1.429109811782837\n",
      "Training loss:1.5463600158691406\n",
      "Training loss:1.4393706321716309\n",
      "Training loss:1.4423291683197021\n",
      "Training loss:1.4974963665008545\n",
      "Training loss:1.4597729444503784\n",
      "Training loss:1.4996730089187622\n",
      "Training loss:1.545876145362854\n",
      "Training loss:1.5234650373458862\n",
      "Training loss:1.5295599699020386\n",
      "Training loss:1.5046751499176025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.498903512954712\n",
      "Training loss:1.4814844131469727\n",
      "Training loss:1.5360394716262817\n",
      "Training loss:1.479516625404358\n",
      "Training loss:1.4432088136672974\n",
      "Training loss:1.4338047504425049\n",
      "Training loss:1.4579713344573975\n",
      "Training loss:1.4135550260543823\n",
      "Training loss:1.3820208311080933\n",
      "Training loss:1.4827693700790405\n",
      "Training loss:1.4507802724838257\n",
      "Epoch: 0175 loss_train: 139.882984 acc_train: 0.459542 loss_val: 17.670286 acc_val: 0.466000 time: 450066.536731s\n",
      "Training loss:1.5149883031845093\n",
      "Training loss:1.4659006595611572\n",
      "Training loss:1.520695447921753\n",
      "Training loss:1.536974310874939\n",
      "Training loss:1.4683455228805542\n",
      "Training loss:1.519027829170227\n",
      "Training loss:1.4877270460128784\n",
      "Training loss:1.4772473573684692\n",
      "Training loss:1.4161230325698853\n",
      "Training loss:1.5099328756332397\n",
      "Training loss:1.4811383485794067\n",
      "Training loss:1.4270662069320679\n",
      "Training loss:1.518821120262146\n",
      "Training loss:1.5002623796463013\n",
      "Training loss:1.4633445739746094\n",
      "Training loss:1.5468459129333496\n",
      "Training loss:1.445149302482605\n",
      "Training loss:1.451441764831543\n",
      "Training loss:1.5007495880126953\n",
      "Training loss:1.391506314277649\n",
      "Training loss:1.451475977897644\n",
      "Training loss:1.4522532224655151\n",
      "Training loss:1.4621402025222778\n",
      "Training loss:1.4957387447357178\n",
      "Training loss:1.4217355251312256\n",
      "Training loss:1.5123701095581055\n",
      "Training loss:1.4326950311660767\n",
      "Training loss:1.510603427886963\n",
      "Training loss:1.4722442626953125\n",
      "Training loss:1.4785321950912476\n",
      "Training loss:1.4515137672424316\n",
      "Training loss:1.4300802946090698\n",
      "Training loss:1.439170241355896\n",
      "Training loss:1.5054395198822021\n",
      "Training loss:1.4284318685531616\n",
      "Training loss:1.4569432735443115\n",
      "Training loss:1.475635290145874\n",
      "Training loss:1.4983354806900024\n",
      "Training loss:1.5379396677017212\n",
      "Training loss:1.4695484638214111\n",
      "Training loss:1.4987784624099731\n",
      "Training loss:1.4844890832901\n",
      "Training loss:1.5086750984191895\n",
      "Training loss:1.4610663652420044\n",
      "Training loss:1.5132721662521362\n",
      "Training loss:1.5341500043869019\n",
      "Training loss:1.4670552015304565\n",
      "Training loss:1.4726094007492065\n",
      "Training loss:1.4505040645599365\n",
      "Training loss:1.575271725654602\n",
      "Training loss:1.504378318786621\n",
      "Training loss:1.4569615125656128\n",
      "Training loss:1.5198638439178467\n",
      "Training loss:1.366591453552246\n",
      "Training loss:1.4935938119888306\n",
      "Training loss:1.4414303302764893\n",
      "Training loss:1.5238449573516846\n",
      "Training loss:1.5380382537841797\n",
      "Training loss:1.5198838710784912\n",
      "Training loss:1.4817218780517578\n",
      "Training loss:1.439768671989441\n",
      "Training loss:1.476953387260437\n",
      "Training loss:1.4777876138687134\n",
      "Training loss:1.4257973432540894\n",
      "Training loss:1.4969841241836548\n",
      "Training loss:1.4924191236495972\n",
      "Training loss:1.4421099424362183\n",
      "Training loss:1.5284122228622437\n",
      "Training loss:1.4628329277038574\n",
      "Training loss:1.4603439569473267\n",
      "Training loss:1.4335604906082153\n",
      "Training loss:1.4394551515579224\n",
      "Training loss:1.3941524028778076\n",
      "Training loss:1.5096032619476318\n",
      "Training loss:1.4370421171188354\n",
      "Training loss:1.455207109451294\n",
      "Training loss:1.443150281906128\n",
      "Training loss:1.5198613405227661\n",
      "Training loss:1.4130054712295532\n",
      "Training loss:1.4897533655166626\n",
      "Training loss:1.5606439113616943\n",
      "Training loss:1.4795141220092773\n",
      "Training loss:1.4574949741363525\n",
      "Training loss:1.4148646593093872\n",
      "Training loss:1.4802802801132202\n",
      "Training loss:1.5303516387939453\n",
      "Training loss:1.5323052406311035\n",
      "Training loss:1.5055454969406128\n",
      "Training loss:1.5375709533691406\n",
      "Training loss:1.4566015005111694\n",
      "Training loss:1.4667527675628662\n",
      "Training loss:1.495302677154541\n",
      "Training loss:1.445604681968689\n",
      "Training loss:1.4554389715194702\n",
      "Epoch: 0176 loss_train: 138.894766 acc_train: 0.462479 loss_val: 17.719300 acc_val: 0.464333 time: 452685.486076s\n",
      "Training loss:1.449046015739441\n",
      "Training loss:1.5353716611862183\n",
      "Training loss:1.4594624042510986\n",
      "Training loss:1.4617277383804321\n",
      "Training loss:1.4636236429214478\n",
      "Training loss:1.4583156108856201\n",
      "Training loss:1.5490362644195557\n",
      "Training loss:1.4237781763076782\n",
      "Training loss:1.4850696325302124\n",
      "Training loss:1.4167258739471436\n",
      "Training loss:1.4987585544586182\n",
      "Training loss:1.502349615097046\n",
      "Training loss:1.504069209098816\n",
      "Training loss:1.4569995403289795\n",
      "Training loss:1.4863277673721313\n",
      "Training loss:1.4254661798477173\n",
      "Training loss:1.4124895334243774\n",
      "Training loss:1.4519885778427124\n",
      "Training loss:1.4584041833877563\n",
      "Training loss:1.4108858108520508\n",
      "Training loss:1.5019371509552002\n",
      "Training loss:1.5126793384552002\n",
      "Training loss:1.4460448026657104\n",
      "Training loss:1.4403443336486816\n",
      "Training loss:1.5146236419677734\n",
      "Training loss:1.496457815170288\n",
      "Training loss:1.5647891759872437\n",
      "Training loss:1.4036613702774048\n",
      "Training loss:1.406087875366211\n",
      "Training loss:1.5161854028701782\n",
      "Training loss:1.4349114894866943\n",
      "Training loss:1.5037946701049805\n",
      "Training loss:1.425349473953247\n",
      "Training loss:1.4704442024230957\n",
      "Training loss:1.4668307304382324\n",
      "Training loss:1.5050948858261108\n",
      "Training loss:1.502106785774231\n",
      "Training loss:1.4705480337142944\n",
      "Training loss:1.4716910123825073\n",
      "Training loss:1.5658503770828247\n",
      "Training loss:1.549167275428772\n",
      "Training loss:1.4371401071548462\n",
      "Training loss:1.5373804569244385\n",
      "Training loss:1.5378175973892212\n",
      "Training loss:1.4748704433441162\n",
      "Training loss:1.4789761304855347\n",
      "Training loss:1.4869717359542847\n",
      "Training loss:1.4472616910934448\n",
      "Training loss:1.511476755142212\n",
      "Training loss:1.5163824558258057\n",
      "Training loss:1.4053797721862793\n",
      "Training loss:1.5017000436782837\n",
      "Training loss:1.4984735250473022\n",
      "Training loss:1.4910489320755005\n",
      "Training loss:1.462079405784607\n",
      "Training loss:1.3904106616973877\n",
      "Training loss:1.4723076820373535\n",
      "Training loss:1.4309910535812378\n",
      "Training loss:1.4693892002105713\n",
      "Training loss:1.5409843921661377\n",
      "Training loss:1.4257436990737915\n",
      "Training loss:1.474648356437683\n",
      "Training loss:1.4726725816726685\n",
      "Training loss:1.5074365139007568\n",
      "Training loss:1.484230637550354\n",
      "Training loss:1.4566442966461182\n",
      "Training loss:1.5024508237838745\n",
      "Training loss:1.45525324344635\n",
      "Training loss:1.4642189741134644\n",
      "Training loss:1.4887973070144653\n",
      "Training loss:1.514468789100647\n",
      "Training loss:1.4520570039749146\n",
      "Training loss:1.468260407447815\n",
      "Training loss:1.375511646270752\n",
      "Training loss:1.480974555015564\n",
      "Training loss:1.5532687902450562\n",
      "Training loss:1.463242530822754\n",
      "Training loss:1.484951138496399\n",
      "Training loss:1.4844732284545898\n",
      "Training loss:1.485497236251831\n",
      "Training loss:1.4505048990249634\n",
      "Training loss:1.4964150190353394\n",
      "Training loss:1.406535267829895\n",
      "Training loss:1.5745289325714111\n",
      "Training loss:1.499504804611206\n",
      "Training loss:1.4666526317596436\n",
      "Training loss:1.4729214906692505\n",
      "Training loss:1.4782285690307617\n",
      "Training loss:1.4318464994430542\n",
      "Training loss:1.4804364442825317\n",
      "Training loss:1.4319825172424316\n",
      "Training loss:1.4875233173370361\n",
      "Training loss:1.5108009576797485\n",
      "Training loss:1.5052040815353394\n",
      "Epoch: 0177 loss_train: 138.757425 acc_train: 0.463229 loss_val: 17.772330 acc_val: 0.459667 time: 455663.613953s\n",
      "Training loss:1.5577900409698486\n",
      "Training loss:1.605648159980774\n",
      "Training loss:1.5073031187057495\n",
      "Training loss:1.4514042139053345\n",
      "Training loss:1.4342929124832153\n",
      "Training loss:1.5034621953964233\n",
      "Training loss:1.5196539163589478\n",
      "Training loss:1.4741684198379517\n",
      "Training loss:1.527258276939392\n",
      "Training loss:1.460782527923584\n",
      "Training loss:1.412178874015808\n",
      "Training loss:1.4344526529312134\n",
      "Training loss:1.3382889032363892\n",
      "Training loss:1.5260486602783203\n",
      "Training loss:1.4961410760879517\n",
      "Training loss:1.5946624279022217\n",
      "Training loss:1.41194486618042\n",
      "Training loss:1.4369879961013794\n",
      "Training loss:1.4457801580429077\n",
      "Training loss:1.4702327251434326\n",
      "Training loss:1.5319677591323853\n",
      "Training loss:1.465455174446106\n",
      "Training loss:1.4280394315719604\n",
      "Training loss:1.5345304012298584\n",
      "Training loss:1.4929187297821045\n"
     ]
    }
   ],
   "source": [
    "# from SGAPoolORI import SAGLearnerori\n",
    "\n",
    "# testori=SAGLearnerori(SAG)\n",
    "# out,pred,loss,correct=testori()\n",
    "# print(len(SAGftlist[0][1]))\n",
    "# for i in range (len(SAGftlist[0][0])):\n",
    "#     print(SAGftlist[0][1][i].y)\n",
    "\n",
    "\n",
    "\n",
    "from HGPORILearner import HGPORILearner\n",
    "\n",
    "testoriHGP=HGPORILearner(SAG)\n",
    "acc,loss,correct=testoriHGP()\n",
    "print(acc)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.226805Z",
     "start_time": "2020-06-06T00:36:05.386Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# from SAGLearner import SAGLearner\n",
    "\n",
    "# test=SAGLearner(4)\n",
    "\n",
    "\n",
    "\n",
    "# print(type(test.dataset[0]))\n",
    "# print(type(test.dataset))\n",
    "# print('first',test.dataset[0].y)\n",
    "# print('second',test.dataset[1].y)\n",
    "# print(test.dataset.num_classes)\n",
    "# print(test.dataset.num_features)\n",
    "\n",
    "\n",
    "\n",
    "# pred,out,loss,correct=test(SAGtrainlist[0],'test')\n",
    "# print('pe',pred)\n",
    "# print('out',correct)\n",
    "# # print()\n",
    "# # test('train')\n",
    "# for g in range(len(test.modelbn)):\n",
    "#     print('bn',g,test.modelbn[g])\n",
    "#     print('para',g,list(test.model.parameters())[g])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T04:26:26.812264Z",
     "start_time": "2020-05-25T04:26:26.730239Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.228340Z",
     "start_time": "2020-06-06T00:36:05.398Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from SGAmeta import SGAMeta\n",
    "\n",
    "\n",
    "    \n",
    "metalist=[0.0001,0.0002,0.0003,0.0005,0.0007,0.001,0.002,0.003,0.005,0.007,0.01,0.02,0.03,0.05]\n",
    "\n",
    "result=[]\n",
    "# m=metalist[3]\n",
    "metalist=[5]\n",
    "m=0.0005\n",
    "acclist=[]\n",
    "for s in metalist:\n",
    "        \n",
    "    sgtest=SGAMeta(s,m,way)\n",
    "    acc,loss=sgtest(SAGtrainlist,SAGtrainlist)\n",
    "    acctestlist=[]\n",
    "    acclist.append([s,m,acc,loss])\n",
    "    for i in range (task):\n",
    "    \n",
    "        acctest=sgtest.finetunning(SAGftlist[i],SAGftlist[i])\n",
    "        acctestlist.append(acctest)\n",
    "    maccs = np.array(acctestlist).mean(axis=0).astype(np.float16)\n",
    "    result.append([s,m,maccs])\n",
    "    \n",
    "for d in range (len(result)):\n",
    "    \n",
    "    print(' way: ',way,' shot: ',shot,' train set step: ',acclist[d][0],' meta: ',acclist[d][1],' acc: ',acclist[d][2],' loss ',acclist[d][3])\n",
    "\n",
    "for r in range (len(result)):\n",
    "    \n",
    "    print(' way: ',way,' shot: ',shot,' step: ',result[r][0],' meta: ',result[r][1],' acc: ',result[r][2])\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.230365Z",
     "start_time": "2020-06-06T00:36:05.401Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from meta import Meta\n",
    "support=[]\n",
    "query=[]\n",
    "supporty=[]\n",
    "queryy=[]\n",
    "\n",
    "for it in range (20):\n",
    "    support.append(datasetlist[it])\n",
    "    query.append(datasetlist[it])\n",
    "\n",
    "    supporty.append(datasetlist[it].train.graph_labels)\n",
    "\n",
    "    queryy.append(datasetlist[it].test.graph_labels)\n",
    "\n",
    "    \n",
    "metalist=[0.0001,0.0002,0.0005,0.0007,0.001,0.002,0.005,0.007,0.01,0.0003,0.003,0.006]\n",
    "# metalist=[0.0001,0.0005,0.0007,0.001,0.005,0.007,0.01]\n",
    "# metalist=[0.0007]\n",
    "result=[]\n",
    "# m=metalist[11]\n",
    "m=0.0007\n",
    "acclist=[]\n",
    "for s in metalist:\n",
    "        \n",
    "    test=Meta(s,m,way)\n",
    "    acc,loss=test(support,supporty,query,queryy)\n",
    "    acctestlist=[]\n",
    "    acclist.append([s,m,acc,loss])\n",
    "    for i in range (task):\n",
    "    \n",
    "        acctest=test.finetunning(ftlist[i], ftlist[i].train.graph_labels, ftlist[i], ftlist[i].test.graph_labels)\n",
    "        acctestlist.append(acctest)\n",
    "    maccs = np.array(acctestlist).mean(axis=0).astype(np.float16)\n",
    "    result.append([s,m,maccs])\n",
    "    \n",
    "for d in range (len(result)):\n",
    "    \n",
    "    print(' way: ',way,' shot: ',shot,' train set step: ',acclist[d][0],' meta: ',acclist[d][1],' acc: ',acclist[d][2])\n",
    "\n",
    "for r in range (len(result)):\n",
    "    \n",
    "    print(' way: ',way,' shot: ',shot,' step: ',result[r][0],' meta: ',result[r][1],' acc: ',result[r][2])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.232026Z",
     "start_time": "2020-06-06T00:36:05.403Z"
    }
   },
   "outputs": [],
   "source": [
    "#### import tensorflow as tf\n",
    "x_train_tensor=torch.FloatTensor([0.01])\n",
    "# testensor=torch.nn.Parameter(testensor)\n",
    "# # testensor=testensor+testensor\n",
    "# print(type(testensor))\n",
    "a = torch.nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "a.retain_grad()\n",
    "b = torch.nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "c = a + 1\n",
    "# c.data = c.data + 1\n",
    "# print(c)\n",
    "# print('aa',a)\n",
    "c.retain_grad()\n",
    "d = torch.nn.Parameter(c, requires_grad=True,)\n",
    "for epoch in range(2):\n",
    "    yhat = d + b * x_train_tensor\n",
    "    error = x_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    loss.backward()\n",
    "    print('a',a.grad)\n",
    "    print('b',b.grad)\n",
    "    print('c',c.grad)\n",
    "    print('d',d.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.233722Z",
     "start_time": "2020-06-06T00:36:05.408Z"
    }
   },
   "outputs": [],
   "source": [
    "grads = {}\n",
    "def save_grad(name):\n",
    "    def hook(grad):\n",
    "        grads[name] = grad\n",
    "    return hook\n",
    "a = torch.nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "b = torch.nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "c = a + 1\n",
    "# c.register_hook(save_grad(0))\n",
    "# a.register_hook(save_grad(1))\n",
    "print('cg1',grads)\n",
    "# c.retain_grad()\n",
    "# grad = torch.autograd.grad(loss, c,allow_unused=True)\n",
    "d = c\n",
    "# d = torch.nn.Parameter(c, requires_grad=True,)\n",
    "for epoch in range(2):\n",
    "    yhat = d + b * x_train_tensor\n",
    "    error = x_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    grad = torch.autograd.grad(loss, c,allow_unused=True)\n",
    "#     loss.backward()\n",
    "    print('grad',grad)\n",
    "    print('a',a.grad)\n",
    "    print('b',b.grad)\n",
    "    print('c',c.grad)\n",
    "    print('d',d.grad)\n",
    "    print('cg2',grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.236382Z",
     "start_time": "2020-06-06T00:36:05.410Z"
    }
   },
   "outputs": [],
   "source": [
    "from    torch.nn import functional as F\n",
    "from    torch import nn\n",
    "w = torch.randn(8,4,3,3)\n",
    "x= tf.constant([1,2,0,1,2])\n",
    "py= tf.constant([[0.1,0.4,0.5],[0.1,0.4,0.5],[0.1,0.4,0.5],[0.1,0.4,0.5],[0.1,0.4,0.5]])\n",
    "print(x)\n",
    "# vars = nn.ParameterList()\n",
    "# vars.append(w)\n",
    "\n",
    "# b = torch.randn(8,4,3,3)\n",
    "lm= nn.Linear(4, 6)\n",
    "y=lm(x)\n",
    "# y.backward()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(y,py)\n",
    "loss = criterion(py,y)\n",
    "\n",
    "\n",
    "# print(y)\n",
    "# print(x.grad)\n",
    "# F.conv2d(inputs, filters, padding=1)\n",
    "# y=torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)                \n",
    "# y = F.conv2d(x, w,padding=1)\n",
    "# y.backward()\n",
    "# loss_q = F.cross_entropy(y, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.238157Z",
     "start_time": "2020-06-06T00:36:05.414Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import Adam\n",
    "\n",
    "# class NN_Network(nn.Module):\n",
    "#     def __init__(self,in_dim,hid,out_dim):\n",
    "#         super(NN_Network, self).__init__()\n",
    "#         self.linear1 = nn.Linear(in_dim,hid)\n",
    "#         self.linear2 = nn.Linear(hid,out_dim)\n",
    "#         self.linear1.weight = torch.nn.Parameter(torch.zeros(in_dim,hid))\n",
    "#         self.linear1.bias = torch.nn.Parameter(torch.zeros(hid))\n",
    "#         self.linear2.weight = torch.nn.Parameter(torch.zeros(in_dim,hid))\n",
    "#         self.linear2.bias = torch.nn.Parameter(torch.ones(hid))\n",
    "#         print('p1',self.linear1.weight)\n",
    "#         print('p1',self.linear1.bias)\n",
    "#         print('p3',self.linear2.weight)\n",
    "#         print('p1',self.linear2.bias)\n",
    "#     def forward(self, input_array):\n",
    "#         h = self.linear1(input_array)\n",
    "#         y_pred = self.linear2(h)\n",
    "#         return y_pred\n",
    "\n",
    "# in_d = 5\n",
    "# hidn = 2\n",
    "# out_d = 3\n",
    "# net = NN_Network(in_d, hidn, out_d)\n",
    "# print('check',list(net.parameters()))\n",
    "# for param in net.parameters():\n",
    "#     print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.239897Z",
     "start_time": "2020-06-06T00:36:05.417Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from    learner import Learner\n",
    "test=Learner(4)\n",
    "ori=test.parameters()\n",
    "# print('onigi',test.vars[1].weight)\n",
    "loss1,pred1,acc1=test(datasetlist[0],'train',init=True)\n",
    "loss2,pred2,acc2=test(datasetlist[0],'test',init=True)\n",
    "\n",
    "\n",
    "# test(datasetlist[1],'train')\n",
    "# test(datasetlist[2],'train')\n",
    "# print('aft',aft.weight)\n",
    "# print('onigi',test.vars[1].weight)\n",
    "print('acc1',acc1)\n",
    "print('testloss',loss1)\n",
    "print('labelout1',datasetlist[0].train.graph_labels)\n",
    "print('pred',pred1)\n",
    "print('acc2',acc2)\n",
    "# print('score',escore)\n",
    "# print('label',elabel)\n",
    "\n",
    "print('testloss2',loss2)\n",
    "# print('score',escore)\n",
    "# print('label',elabel)\n",
    "print('labelout2',datasetlist[0].test.graph_labels)\n",
    "print('pred2',pred2)\n",
    "# print(test.parameters())\n",
    "# print(type(elabel))\n",
    "# print(type(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.241987Z",
     "start_time": "2020-06-06T00:36:05.420Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = 0.01*x + 2\n",
    "print(y)\n",
    "# z = y * y * 3\n",
    "# out = z.mean()\n",
    "# f=out-2\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.243624Z",
     "start_time": "2020-06-06T00:36:05.422Z"
    }
   },
   "outputs": [],
   "source": [
    "from    torch import nn\n",
    "n = 70 # num of points\n",
    "\n",
    "# x is a tensor\n",
    "x = torch.linspace(0, 10, steps=n)\n",
    "k = torch.tensor(2.5)\n",
    "\n",
    "# y is a tensor\n",
    "y = k*x + 5*torch.rand(n)\n",
    "\n",
    "# loss function\n",
    "def mse(y_hat, y): return ((y_hat-y)**2).mean()\n",
    "\n",
    "a = torch.tensor(-1., requires_grad=True)\n",
    "a = nn.Parameter(a)\n",
    "print('ori',a)\n",
    "y_hat = a*x\n",
    "\n",
    "loss = mse(y_hat, y) \n",
    "# print(loss) \n",
    "loss.backward(a)\n",
    "# print(a.grad)\n",
    "a.grad.zero_()\n",
    "print('new',a)\n",
    "lr = 0.1\n",
    "a = a - lr * a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.245664Z",
     "start_time": "2020-06-06T00:36:05.425Z"
    }
   },
   "outputs": [],
   "source": [
    "# def plot_histo_graphs(dataset, title):\n",
    "#     # histogram of graph sizes\n",
    "#     graph_sizes = []\n",
    "#     for graph in dataset:\n",
    "#         graph_sizes.append(graph[0].number_of_nodes())\n",
    "#         #graph_sizes.append(graph[0].number_of_edges())\n",
    "#     plt.figure(1)\n",
    "#     plt.hist(graph_sizes, bins=20)\n",
    "#     plt.title(title)\n",
    "#     plt.show()\n",
    "#     graph_sizes = torch.Tensor(graph_sizes)\n",
    "#     print('nb/min/max :',len(graph_sizes),graph_sizes.min().long().item(),graph_sizes.max().long().item())\n",
    "    \n",
    "# plot_histo_graphs(dataset.train,'trainset')\n",
    "# plot_histo_graphs(dataset.val,'valset')\n",
    "# plot_histo_graphs(dataset.test,'testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.251744Z",
     "start_time": "2020-06-06T00:36:05.428Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(len(dataset.train))\n",
    "# print(len(dataset.val))\n",
    "# print(len(dataset.test))\n",
    "\n",
    "# print(dataset.train[0])\n",
    "# print(dataset.val[0])\n",
    "# print(dataset.test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.254227Z",
     "start_time": "2020-06-06T00:36:05.431Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "with open('data/superpixels/MNIST.pkl','wb') as f:\n",
    "        pickle.dump([dataset.train,dataset.val,dataset.test],f)\n",
    "        \n",
    "print('Time (sec):',time.time() - start) # 38s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.255857Z",
     "start_time": "2020-06-06T00:36:05.433Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'MNIST'\n",
    "dataset = LoadData(DATASET_NAME) # 54s\n",
    "trainset, valset, testset = dataset.train, dataset.val, dataset.test\n",
    "print(dataset.train.graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.258541Z",
     "start_time": "2020-06-06T00:36:05.436Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from    learner import Learner\n",
    "test=Learner()\n",
    "\n",
    "loss=test(dataset)\n",
    "print('loss',loss)\n",
    "# from sklearn import preprocessing \n",
    "\n",
    "# label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "# check = [4,7,7,6,4,4,7,6,7,4,4,4]\n",
    "# trans=torch.tensor(label_encoder.fit(range(10)))\n",
    "# trans=label_encoder.fit([4,6,7])\n",
    "# change=trans.transform(check)\n",
    "# print(check)\n",
    "# print(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.263320Z",
     "start_time": "2020-06-06T00:36:05.439Z"
    }
   },
   "outputs": [],
   "source": [
    "# # print(test_acc)\n",
    "# # print(train_acc)\n",
    "from    torch.nn import functional as F\n",
    "input = torch.randn(15, 4, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 3,1,1,3,1,3,1,1,3,1,3,1,3])\n",
    "output = F.nll_loss(F.log_softmax(input), target)\n",
    "output.backward()\n",
    "print(input)\n",
    "print(target)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T00:36:09.264605Z",
     "start_time": "2020-06-06T00:36:05.442Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "input = torch.randn(20, 4, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([4,4,0,4,1,4,0,1,4,0,4,1,4,0,4,1,4,0,4,1])\n",
    "batch_size = 10\n",
    "collate = dataset.collate\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "# for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(train_loader):\n",
    "#     print(batch_labels)\n",
    "print('Time (sec):',time.time() - start) # 0.0003s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:23:51.729167Z",
     "start_time": "2020-05-06T20:23:51.629503Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
